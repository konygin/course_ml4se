{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. ENCODER\n",
    "\n",
    "1. Seq2seq\n",
    "2. Transformer\n",
    "3. Encoder\n",
    "4. Attention\n",
    "5. FFN\n",
    "6. Embeddings and Softmax\n",
    "7. Positional Encodings\n",
    "8. Residual Stream\n",
    "9. Exercise\n",
    "10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq is a family of machine learning approaches designed for sequence transformation tasks, such as machine translation, automatic speech recognition (ASR), and code generation.\n",
    "\n",
    "A standard _encoder-decoder_ model consists of two main components:\n",
    "- The _encoder_ processes an input sequence $(x_1, ..., x_n)$ and converts it into a sequence of continuous representations $z = (z_1, ..., z_n)$.\n",
    "- The _decoder_ then generates the output sequence $(y_1, ..., y_m)$ from this representation $z$, one element at a time.\n",
    "\n",
    "The generation process is _autoregressive_, meaning that at each step, the decoder produces the next output element conditioned on both the encoded representation $z$ and the previously generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f36bdfb99f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early encoder-decoder models were primarily based on Recurrent Neural Networks (RNNs). This paradigm shifted in 2017 with the introduction of the Transformer architecture in the seminal paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. from Google Brain.\n",
    "\n",
    "\n",
    "![paper](res/04_attention_is_all_you_need.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer consists of an encoder and a decoder, each of which consists of layers [lena-voita.github.io]:\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Jay Allamar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Brandon Rohrer - Transformers from Scratch](https://e2eml.school/transformers.html)\n",
    "- [Peter Bloem - Transformers from Scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of $N$ (e.g.,  $N = 6$) identical consecutive layers.\n",
    "Each layer has two sublayers:\n",
    "- multi-head self-attention mechanism, and\n",
    "- position-wise fully connected feed-forward network.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/Transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequence is tokenized. Each token corresponds to a vector (embedding).\n",
    "\n",
    "![](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the architecture uses a *residual connection* for each of the two sublayers, followed by [layer normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "More formally, the output of each sublayer is `LayerNorm(x + Sublayer(x))`, where `Sublayer(x)` is a function implemented by the sublayer itself.\n",
    "\n",
    "![](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure_W640.jpg)\n",
    "\n",
    "All sublayers in the model and output embeddings have the same dimension $d_{model}$ (e.g. $d_{model} = 512$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Ulf Mertens - The Transformer encoder](https://github.com/mertensu/transformer-tutorial/blob/master/transformer_encoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention can be described as a function mapping a *query* and a set of *key-value* pairs to some output.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
    "\n",
    "The query, keys, values, and output are vectors.\n",
    "The output vector is computed as a weighted sum of values, where the weight assigned to each value is computed over the query-key pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the calculations are done in matrix form (simultaneously for a set of queries, packed together into a matrix $Q$). The keys and values ​​are also packed together into matrices $K$ and $V$.\n",
    "\n",
    "Thus,\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ allows us to reduce the value of large scalar products.\n",
    "(If they are large, then the softmax function has very small gradients. This is bad.)\n",
    "\n",
    "In matrix notation:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention allows you to run several attention mechanisms, each of which can have its own useful features.\n",
    "This way, information is collected from different views.\n",
    "\n",
    "Therefore, using linear transformations (projections), we obtain several ($h$) query-key-value triplets (dimensions, respectively, $d_k$, $d_k$, $d_v$).\n",
    "\n",
    "Then, for each of these triplets, an attention mechanism is launched in parallel, which yields output values ​​of dimension $d_v$.\n",
    "Then the obtained output values ​​are concatenated and projected again.\n",
    "\n",
    "![](https://repository-images.githubusercontent.com/283979760/0f00ed80-d368-11ea-979d-78033d0a1cee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally:\n",
    "\n",
    "$$MultiHead(Q, K, V ) = concat(head_1, ..., head_h)W^O$$\n",
    "\n",
    "where $head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)$ and the following projection matrices are used\n",
    "\n",
    "$W^Q_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^K_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^V_i \\in R^{d_{model} \\times d_v}$,\n",
    "\n",
    "$W^O_i \\in R^{hd_v \\times d_{model}}$.\n",
    "\n",
    "The dimensions can be, for example, $h = 8$, $d_k = d_v = \\frac{d_{model}}{h} = 64$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer uses multi-head attention in three different ways:\n",
    "\n",
    "1. (cross) In the encoder-decoder attention layers, the *queries* come from the _previous_ decoder layer, and the *keys* and *values* come from the encoder output. This allows each position in the decoder to visit all positions in the input sequence.\n",
    "\n",
    "2. (self) The encoder contains self-attention layers.\n",
    "Here, all *keys*, *values*, and *queries* come from the same place --- from the output of the previous encoder layer. Each position in the encoder can visit all positions in the previous encoder layer.\n",
    "\n",
    "3. (masked) The decoder contains self-attention layers.\n",
    "Here, each position in the decoder is allowed to visit all previous positions (from the left) up to and including the current position.\n",
    "This restriction arises because it is necessary to preserve the autoregressive property of the decoder (predict the next token based on the previous tokens).\n",
    "The constraint is implemented inside scaled dot-product attention by maximizing (assuming $-\\infty$) all values ​​at the softmax input that correspond to invalid visits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the encoder and decoder layers contain a _Feed-Forward Network (FFN)_.\n",
    "This is a sublayer that is applied identically and independently to each position.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
    "\n",
    "The FFN consists of two linear transformations with a [ReLU](https://arxiv.org/abs/1803.08375) activation function in between:\n",
    "\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "While the same linear transformation is applied to every position, the parameters $W_1, b_1, W_2, b_2$ are different for each layer.\n",
    "The inner layer has a higher dimensionality; for instance, with an input and output dimension of \\(d_{model} = 512\\), the inner layer dimension is often \\(d_{ff} = 2048\\).\n",
    "\n",
    "A key characteristic of the Transformer is that the token at each position follows its own path through the encoder. While the self-attention layers create dependencies between these positions, the FFN layers operate independently. This allows the FFN computations for all positions to be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Embeddings and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform input/output tokens into vectors (of dimension $d_{model}$), learnable embeddings are used.\n",
    "\n",
    "To transform outputs into predicted probabilities of the next token, learnable linear transformation and softmax are used.\n",
    "\n",
    "In a bit more detail, the output vector is passed through a linear layer (a fully connected neural network), followed by softmax.\n",
    "\n",
    "The linear layer projects the output into a much larger vector (a logits vector).\n",
    "The size of this vector is the size of the token dictionary.\n",
    "\n",
    "Then the softmax layer turns the coordinates into probabilities.\n",
    "The most probable token is selected.\n",
    "\n",
    "![](./res/04_transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable embeddings are used to convert input and output tokens into vectors of dimension $d_{model}$.\n",
    "\n",
    "To generate predictions, the decoder's output is converted into probabilities for the next token using a learnable linear transformation followed by a softmax function.\n",
    "\n",
    "In detail, the output vector is passed through a linear layer that projects it into a logits vector with a size equal to the vocabulary.\n",
    "The softmax function then converts these logits into probabilities. The token with the highest probability is selected as the prediction.\n",
    "\n",
    "![](./res/04_transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the information about the order of tokens in the sequence, the Transformer uses positional encoding. Positional encoding is a vector that encodes the token's ordinal number. It has the same length as the embeddings ($d_{model}$), and is added to the embedding at the input.\n",
    "\n",
    "For example, you can use the following functions:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "where $pos$ is the position index, and $i$ is the coordinate index.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options:\n",
    "- [learnable](https://aclanthology.org/2022.findings-aacl.42.pdf)\n",
    "- [relative](https://arxiv.org/abs/1803.02155)\n",
    "- [RoPE](https://arxiv.org/abs/2104.09864)\n",
    "\n",
    "#### More\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [LongRoPE](https://arxiv.org/abs/2402.13753)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Batches and Paddings\n",
    "\n",
    "![](./res/04_padding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Residual stream\n",
    "\n",
    "[Source: [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)]\n",
    "\n",
    "Simplification: \"attention-only\" transformers, which don't have MLP layers. \n",
    "\n",
    "![](./res/04_residual.png)\n",
    "\n",
    "\n",
    "The residual stream can be considered as a communication channel, since it doesn't do any processing itself and all layers communicate through it.\n",
    "\n",
    "The residual stream has a deeply linear structure.\n",
    "Every layer performs an arbitrary linear transformation to \"read in\" information from the residual stream at the start, and performs another arbitrary linear transformation before adding to \"write\" its output back into the residual stream.\n",
    "This linear, additive structure of the residual stream has a lot of important implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./res/04_attention_heads.png)\n",
    "\n",
    "The fundamental action of attention heads is _moving information_.\n",
    "\n",
    "They read information from the residual stream of one token, and write it to the residual stream of another token. The main observation to take away from this section is that which tokens to move information from is completely separable from what information is \"read\" to be moved and how it is \"written\" to the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLM implement Encoder from scratch in PyTorch. Be prepared to answer questions about the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. References\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [nn.labml.ai - Paper Implementations](https://nn.labml.ai/)\n",
    "- [Post LayerNorm - Pre LayerNorm](https://arxiv.org/abs/2002.04745)\n",
    "- [LayerNorm - RMSNorm](https://arxiv.org/abs/1910.07467)\n",
    "- [Attention modifications](https://arxiv.org/abs/2305.13245)\n",
    "- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)\n",
    "- [pytorch - NLP from Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
