{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.  TRANSFORMER\n",
    "\n",
    "1. Seq2seq\n",
    "2. Transformer\n",
    "3. Encoder\n",
    "4. Attention\n",
    "5. FFN\n",
    "6. Embeddings and Softmax\n",
    "7. Positional Encodings\n",
    "8. Exercise\n",
    "9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq --- это семейство подходов для задач преобразования последовательностей (например, NLP, ASR, ML4SE).\n",
    "\n",
    "На текущий момент, самые распространённые подходы --- это подходы на основе Трансформера (энкодер-декодер, декодер или энкодер).\n",
    "\n",
    "Энкодер-декодерная модель состоит из:\n",
    "- энкодера (encoder) и\n",
    "- декодера (decoder).\n",
    "\n",
    "*Энкодер* отображает входную последовательность представлений символов $(x_1, ..., x_n)$ в последовательность непрерывных представлений $z = (z_1, ..., z_n)$.\n",
    "\n",
    "*Декодер* отображает $z$ в выходную последовательность $(y_1, ..., y_m)$ символов по одному элементу за раз.\n",
    "\n",
    "На каждом шаге модель является авторегрессивной, получая ранее сгенерированные символы в качестве дополнительных входных данных при создании следующего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff8343f3ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее использовались энкодер-декодерные архитектуры на основе [RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html). В 2017 в работе [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762) был предложена архитектура Трансформер (Google Brain).\n",
    "\n",
    "![paper](res/04_attention_is_all_you_need.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер состоит из энкодера и декодера, каждый из которых состоит из слоёв.\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Jay Allamar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Brandon Rohrer - Transformers from Scratch](https://e2eml.school/transformers.html)\n",
    "- [Peter Bloem - Transformers from Scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер состоит из $N$ (например,  $N = 6$) одинаковых последовательных слоёв.\n",
    "Каждый слой имеет два подслоя:\n",
    "- multi-head self-attention mechanism, и\n",
    "- position-wise fully connected feed-forward network.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/Transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная последовательность токенизируется. Каждому токену соответствует вектор (эмбеддинг).\n",
    "![](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого используется *residual connection* вокруг каждого из двух подслоев,\n",
    "за которым следует [layer normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "Более формально, выход каждого подслоя --- это `LayerNorm(x + Sublayer(x))`, где `Sublayer(x)` --- это функция, реализованная самим подслоём.\n",
    "\n",
    "![](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure_W640.jpg)\n",
    "\n",
    "Все подслои в модели и выходные эмбеддинги имеют одинаковую размерность $d_{model}$ (например, $d_{model} = 512$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Ulf Mertens - The Transformer encoder](https://github.com/mertensu/transformer-tutorial/blob/master/transformer_encoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention можно описать как функцию, отображающую *запрос (query)* и множество пар *ключ-значение (key-value)* в некоторый выход.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
    "\n",
    "Запрос, ключи, значения и выход являются векторами.\n",
    "Вектор вычисляется как взвешенная сумма значений, где вес, присвоенный каждому значению, вычисляется по паре \"запрос-ключ\".\n",
    "\n",
    "![](https://i.stack.imgur.com/SG66z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "На входе мы имеем:\n",
    "- запросы и ключи ключей размерности $d_k$ и\n",
    "- значений размерности $d_v$.\n",
    "\n",
    "Далее вычисляем скалярные произведения запроса со всеми ключами, делим каждое такое произведение на $\\sqrt{d_k}$.\n",
    "\n",
    "Затем применяем softmax для получения весов, с которыми будем суммировать значения.\n",
    "\n",
    "![](res/04_scaled_dot-product_attention.png)\n",
    "\n",
    "На практике вычисления происходят в матричной форме (одновременно для набора запросов, упакованных вместе в матрицу $Q$). Ключи и значения также упаковываются вместе в матрицы $K$ и $V$.\n",
    "\n",
    "Таким образом,\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Деление на $\\sqrt{d_k}$ позволяет уменьшить значение больших скалярных произведений.\n",
    "(Если они большие, то функция softmax в таком случае имеет очень маленькие градиенты. Это плохо.)\n",
    "\n",
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричной записи:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention позволяет запустить несколько attention-механизмов, каждый из которых может обладать своими полезными особенностями.\n",
    "Таким образом собирается информация из различных представлений.\n",
    "\n",
    "![](https://i.ytimg.com/vi/mMa2PmYJlCo/mqdefault.jpg)\n",
    "\n",
    "Поэтому с помощью линейных преобразований (projections) получаем несколько ($h$) троек запрос-ключ-значение (размерности, соответственно, $d_k$, $d_k$, $d_v$).\n",
    "\n",
    "Затем для каждой из этих троек параллельно запускается attention-механизм, что даёт выходные значения размерности $d_v$.\n",
    "Далее полученные выходные значения конкатенируются и снова проецируются.\n",
    "\n",
    "![](https://repository-images.githubusercontent.com/283979760/0f00ed80-d368-11ea-979d-78033d0a1cee)\n",
    "\n",
    "Более формально:\n",
    "\n",
    "$$MultiHead(Q, K, V ) = concat(head_1, ..., head_h)W^O$$\n",
    "\n",
    "где $head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)$ и используются следующие матрицы проецирования\n",
    "\n",
    "$W^Q_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^K_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^V_i \\in R^{d_{model} \\times d_v}$,\n",
    "\n",
    "$W^O_i \\in R^{hd_v \\times d_{model}}$.\n",
    "\n",
    "Размерности могут быть, например, следующими $h = 8$, $d_k = d_v = \\frac{d_{model}}{h} = 64$.\n",
    "\n",
    "Трансформер использует multi-head attention тремя различными способами:\n",
    "\n",
    "1. В слоях \"энкодер-декодер attention\" запросы поступают с предыдущего уровня декодера, а ключи и значения поступают с выходных данных энкодера. Это позволяет каждой позиции в декодере посетить все позиции входной последовательности.\n",
    "\n",
    "2. Энкодер содержит self-attention слои.\n",
    "Здесь все ключи, значения и запросы поступают из одного и того же места --- из вывода предыдущего слоя энкодера. Каждая позиция в энкодере может посетить все позиции предыдущего слоя энкодера.\n",
    "\n",
    "3. Декодер содержит self-attention слои.\n",
    "Здесь допускается для каждой позиции декодера посетить все предыдущие позиции (слева) до текущей позии включительно.\n",
    "Такое ограничение возникает, поскольку необходимо сохранить авторегрессионное свойство декодера (по предыдущим токенам предсказывать следующий).\n",
    "Ограничение реализуются внутри scaled dot-product attention максированием (полагая $-\\infty$) всех значений на входе softmax-а, которые соответствуют недопустимым посещениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый из слоёв энкодера и декодера содержит FFN. Этот подслой одинаково применяется к каждой позиции отдельно.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
    "Он состоит из двух линейных преобразований с активацией [ReLU](https://arxiv.org/abs/1803.08375) между ними.\n",
    "\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Хотя линейные преобразования одинаковы для разных позиций,\n",
    "  они используют разные параметры от слоя к слою.\n",
    "\n",
    "Внутренний слой имеет большую размерностью.\n",
    "Например, размерность входа и выхода $d_{model} = 512$, размерность внутреннего слоя $d_{ff} = 2048$.\n",
    "\n",
    "В частности мы имеем, что в Трансформере токен в каждой позиции проходит свой собственный путь в энкодере.\n",
    "Есть зависимости на уровне self-attention слоёв, но нет зависимостей на уровне FFN.\n",
    "Это позволяет проходить FFN-подслои параллельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Embeddings and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для преобразования входных/выходных токенов в вектора (размерности $d_{model}$) используются обучаемые эмбеддинги.\n",
    "Для преобразования выходов декодеров в предсказанные вероятности следующего токена используется обучаемое линейное преобразование и softmax.\n",
    "\n",
    "Чуть более подробно.  На выходе декодера мы получаем вектор.\n",
    "Этот вектор проходит через линейный слой (fully connected neural network), за которым следует softmax.\n",
    "\n",
    "Линейный слой проецирует выход декодера в гораздо больший вектор (logits-вектор).\n",
    "Размер этого вектор --- это размер словаря токенов.\n",
    "\n",
    "Затем слой softmax превращает координаты в вероятности.\n",
    "Выбирается наиболее вероятный токен.\n",
    "\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы использовать информацию о порядке токенов в последовательности, в Трансформере применяются positional encoding. Positional encoding --- это вектор, в котором закодирован номер.\n",
    "Он имеет ту же длину, что и эмбеддинги ($d_{model}$). И  прибавляется к эмбеддингу на входе.\n",
    "\n",
    "Например, можно использовать такие функции:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "где $pos$ --- это индекс позиции, а $i$  --- индекс координаты.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие варианты:\n",
    "- обучаемые\n",
    "- [относительные](https://arxiv.org/abs/1803.02155)\n",
    "- [RoPE](https://arxiv.org/abs/2104.09864)\n",
    "  \n",
    "#### More\n",
    "- [О методах позиционного кодирования в Transformer](https://habr.com/ru/articles/780116/)\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [LongRoPE](https://arxiv.org/abs/2402.13753)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Batches and Paddings\n",
    "\n",
    "![](https://i.stack.imgur.com/UOo7qm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать Encoder с нуля на PyTorch. Быть готовыми ответить на вопросы по коду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. References\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [nn.labml.ai - Paper Implementations](https://nn.labml.ai/)\n",
    "- [Функция активации ReLU - SwiGLU](https://arxiv.org/abs/2002.05202)\n",
    "- [Post LayerNorm - Pre LayerNorm](https://arxiv.org/abs/2002.04745)\n",
    "- [LayerNorm - RMSNorm](https://arxiv.org/abs/1910.07467)\n",
    "- [Модификации Attention](https://arxiv.org/abs/2305.13245)\n",
    "- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)\n",
    "- [pytorch - NLP from Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
