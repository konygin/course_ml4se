{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE STATE OF THE ART OF NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержание\n",
    "\n",
    "1. NLP\n",
    "2. Задачи\n",
    "3. Модели\n",
    "  - 3.1. Трансформер\n",
    "  - 3.2. Энкодер\n",
    "  - 3.3. Декодер\n",
    "  - 3.4. Энкодер-декодер\n",
    "  - 3.5. Сравнение\n",
    "  - 3.6. Hugging Face\n",
    "4. Обучение\n",
    "  - 4.1. Pretraining\n",
    "  - 4.2. Finetuning\n",
    "5. LLMs\n",
    "  - 5.1. Эмерджентность\n",
    "  - 5.2. PEFT\n",
    "  - 5.3. Alignment\n",
    "6. Проблемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP\n",
    "\n",
    "Natural language processing (обработка естественного языка) --- различные задачи, связанные с обработкой текста.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/NLP_-_isometric.svg/800px-NLP_-_isometric.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sentiment analysis\n",
    "\n",
    "Анализ тональности текста, выявление эмоциональной окраски текста.\n",
    "\n",
    "![](https://cdn-gkenl.nitrocdn.com/uMXnJIMvZJzBKtDYnZeeVwfFXPbPMFBY/assets/images/optimized/rev-93feb00/wp-content/uploads/2021/06/sentimentanalysishotelgeneric-2048x803-1-1536x602.jpg)\n",
    "\n",
    "Например:\n",
    "- [Pascual - Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
    " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text classification\n",
    "\n",
    "Классификация текстов или документов.\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Named entity recognition (NER)\n",
    "\n",
    "Идентификация и классификации сущностей (таких как люди, места и организации...) в тексте.\n",
    "\n",
    "![](https://www.shaip.com/wp-content/uploads/2022/02/Blog_Named-Entity-Recognition-%E2%80%93-The-Concept-Types-Applications.jpg)\n",
    "\n",
    "Например:\n",
    "- [bert-base-NER ](https://huggingface.co/dslim/bert-base-NER)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Part-of-speech tagging (POS tagging, POST)\n",
    "\n",
    "Определение, какой частью речи является каждое слово в предложении.\n",
    "\n",
    "![](https://slideplayer.com/slide/6426089/22/images/2/Part-of-Speech+Tagging.jpg)\n",
    "\n",
    "Например:\n",
    "- [English Part-of-Speech Tagging in Flair](https://huggingface.co/flair/pos-english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Question answering (QA)\n",
    "\n",
    "Генерация ответов на вопросы. Вариации:\n",
    "- запрос содержит ответ на вопрос (например, документ + вопрос к документу)\n",
    "- запрос не содержит ответа на вопрос (необходимо использовать информацию, извлечённую на этапе обучения)\n",
    "\n",
    "![](https://assets-160c6.kxcdn.com/wp-content/uploads/2022/01/2022-01-18-en-content-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Text summarization\n",
    "\n",
    "Генерация резюме текста (абстрактивная суммаризация и экстрактивная суммаризация).\n",
    "\n",
    "![](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/180981i9EA877DDFF97D50D)\n",
    "\n",
    "Подробнее:\n",
    "- [Summarization](https://huggingface.co/docs/transformers/tasks/summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Translation\n",
    "\n",
    "Перевод с одного языка на другой.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 (Mathematical) reasoning\n",
    "\n",
    "Построение рассуждений, решение математических задач\n",
    "\n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpHov9BD2yiBEDrAWZUQxDYWRIuofmpbWVJaJPDPrE-2BbT3B_15-R4n22yNnDVs_8Vkea-Y-ykOHaB6mCKwkLYkBDBoS1r8NX2u4KsCpNC53GAM_8seK6L_90CJCmhC4ML9SSVY03lErXDQd6Pp-ysGsANdvNcqur7lMARO7h4RtDtf6Y7UlNYuEjjQ/s1999/image5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 NLU vs NLG\n",
    "\n",
    "Обычно выделяют:\n",
    "- задачи генерации текста (NLG) --- происходит генерация ответа на естественном языке\n",
    "- задачи понимания текста (NLU) --- происходит генерация (или дополнение) ответа в рамках некоторого формата\n",
    "\n",
    "<img src=\"https://datasolut.com/wp-content/uploads/2021/05/NLP-100.jpg.webp\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Трансформер\n",
    "\n",
    "Архитектура для *sequence-to-sequence* задач. Пусть $X_{1:n} = \\{ x_1, \\ldots, x_n\\}$.\n",
    "Необходимо найти $f$: $X_{1:n} \\mapsto Y_{1:m}$.\n",
    "\n",
    "\n",
    "\n",
    "### Общий вид\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слои\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распределение знаний по слоям\n",
    "\n",
    "![https://arxiv.org/abs/2304.05216](res_nlp/telly_layer-wise_contributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Platen - Transformers-based Encoder-Decoder Models](https://huggingface.co/blog/encoder-decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Энкодер\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/Encoder_block.png)\n",
    "\n",
    "Примеры:\n",
    "- BERT\n",
    "- RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Декодер\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_decoder_detail.png)\n",
    "\n",
    "![](res_nlp/enc-dec_formula.png)\n",
    "\n",
    "Примеры:\n",
    "- GPT\n",
    "- PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Энкодер-декодер\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder_step_by_step.png)\n",
    "\n",
    "![](res_nlp/enc-dec_formula.png)\n",
    "\n",
    "Примеры:\n",
    "- BART\n",
    "- T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Сравнение\n",
    "\n",
    "|                |   NLU      | NLG        |\n",
    "|----------------|------------|------------|\n",
    "| encoder-only   | optimal    | poor       |\n",
    "| decoder-only   | poor       | optimal    |\n",
    "| encoder-decoder| suboptimal | suboptimal |\n",
    "\n",
    "Например, в ML4SE:\n",
    "\n",
    "![](https://github.com/microsoft/CodeXGLUE/raw/main/baselines.jpg)\n",
    "\n",
    "[comment]: <> (https://github.com/microsoft/CodeXGLUE/raw/main/tasks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если необходимо отличное качество одновременно в NLU и NLG?\n",
    "![CodeT5+](res_nlp/codet5+.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Wang et al - CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Hugging Face\n",
    "\n",
    "![](res_nlp/hf-logo-with-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP:\n",
    "\n",
    "![](res_nlp/hf_nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV:\n",
    "\n",
    "![](res_nlp/hf_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio:\n",
    "\n",
    "![](res_nlp/hf_a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular, RL:\n",
    "    \n",
    "![](res_nlp/hf_t_rl.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal:\n",
    "\n",
    "![](res_nlp/hf_mm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "\n",
    "```\n",
    "pip install -q transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[{'label': 'POSITIVE', 'score': 0.9998},\n",
    " {'label': 'NEGATIVE', 'score': 0.9991}]\n",
    "```\n",
    "\n",
    "Подробнее:\n",
    "- [Quick Tour](https://huggingface.co/docs/transformers/quicktour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Обучение\n",
    "\n",
    "- обучать с нуля (*from scratch*)\n",
    "- использовать модели, которые были обучены на других (но связанных с текущей) задачах (*transfer learning*)\n",
    "\n",
    "Если обучать модель полностью с нуля, то потребуется много рамеченных данных. Где взять?\n",
    "\n",
    "Идея:\n",
    "- давайте найдём подходящую промежуточную задачу со свойствами:\n",
    "    - связана с исходной задачей\n",
    "    - легко найти для неё много размеченных данных\n",
    "- обучимся на ней (*pretraining*)\n",
    "- дообучим модель (*finetuning*) на нашей задачи (*downstream*-задача), используя меньший датасет\n",
    "\n",
    "| обучение | тип задачи        | назначение          | таргеты        | разметка        |\n",
    "|----------|-------------------|---------------------|----------------|-----------------|\n",
    "| pretrain | pretrain-задача   | моделирование языка | синтетические  | self-supervised |\n",
    "| finetune | downstream-задача | полезная задача     | реальные       | supervised      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Pretraining\n",
    "\n",
    "Обучение модели с нуля промежуточной задаче. Особенности:\n",
    "- требуется много данных\n",
    "- дорого\n",
    "\n",
    "Выбор промежуточной задачи:\n",
    "- связь с конечной задачей, например, если NLP, то в качестве такой задачи подойдёт *задача моделирования языка*\n",
    "- наличие большого количества размеченных данных.\n",
    "\n",
    "Разметка с помощью экспертов:\n",
    "- долго,\n",
    "- дорого.\n",
    "\n",
    "Поэтому на этом этапе используется автоматическая разметка (self-supervised), т.е. разметка, которую можно получить без участия человека:\n",
    "- быстро,\n",
    "- дёшево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal language modeling (CLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg)\n",
    "\n",
    "- слева направо\n",
    "- авторегрессионно\n",
    "- GPT (decoder)\n",
    "- Prefix language modeling (PrefixLM)\n",
    "\n",
    "\n",
    "### Masked language modeling (MLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg)\n",
    "\n",
    "- BERT (encoder)\n",
    "\n",
    "### Span corruption\n",
    "\n",
    "![](https://user-images.githubusercontent.com/6536835/116129345-4b306700-a6ca-11eb-9acd-a14aa2b8d115.png)\n",
    "\n",
    "- T5 (encoder-decoder)\n",
    "\n",
    "\n",
    "### UL2\n",
    "\n",
    "- R-denoising (regular denoising): standard span corruption objective --- short spans & low corruption)\n",
    "- S-denoising (sequential denoising): strictly follows sequence order --- sequential denoising / prefix language modeling\n",
    "- X-denoising (extreme denoising): extreme span lengths and corruption rates\n",
    "\n",
    "![](res_nlp/denoising.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Tay et al - UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Finetuning (SFT)\n",
    "\n",
    "\n",
    "### Датасеты / бенчмарки\n",
    "\n",
    "- SQuAD ---  Stanford Question Answering Dataset\n",
    "- CoQA --- Conversational Question Answering\n",
    "- (Super) GLUE --- General Language Understanding Evaluation benchmark\n",
    "    - CoLA --- Corpus of Linguistic Acceptability\n",
    "    - SST --- Stanford Sentiment Treebank\n",
    "    - MRPC --- Microsoft Research Paraphrase Corpus\n",
    "    - QQP --- Quora Question Pairs\n",
    "    - MultiNLI --- Multi-Genre Natural Language Inference Corpus\n",
    "    - RTE --- Recognizing Textual Entailment\n",
    "    - etc\n",
    "\n",
    "### Метрики\n",
    "\n",
    "- Exact match (EM)\n",
    "- F1 score\n",
    "- Perplexity\n",
    "- BLEU\n",
    "\n",
    "Подробнее:\n",
    "- [Resources and Benchmarks for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Эмерджентность\n",
    "\n",
    "### Scaling laws\n",
    "\n",
    "![](res_nlp/scaling_laws.png)\n",
    "\n",
    "### Large models\n",
    "\n",
    "![](https://cdn.builder.io/api/v1/image/assets%2Fe0438815ba51486bbb6a202747122d4b%2F894dade70d724952bfad3956c599865a)\n",
    "\n",
    "\n",
    "Claude --- 100K токенов контекста\n",
    "- примерно 75K  слов\n",
    "- сотни страниц\n",
    "- книга (\"Великий Гэтсби около 72K токенов)\n",
    "- текст, который чтобы прочитать потребуется около 5 часов\n",
    "\n",
    "### Эмерджентные свойства\n",
    "\n",
    "> An ability is emergent if it is not present in smaller models but is present in larger models.\n",
    "\n",
    "\n",
    "#### In-context learning\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)\n",
    "\n",
    "\n",
    "#### Chain-of-thought (CoT)\n",
    "\n",
    "![](https://learnprompting.org/assets/images/chain_of_thought_example-37c925a2720c9c4bb4c823d237bc72c8.png)\n",
    "\n",
    "\n",
    "Подробнее:\n",
    "\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. PEFT\n",
    "    \n",
    "### Подходы\n",
    "\n",
    "![](https://media.arxiv-vanity.com/render-output/7514651/img/peft_taxonomy_v3.2.jpg)\n",
    "\n",
    "\n",
    "#### LoRA\n",
    "\n",
    "![](res_nlp/lora.png)\n",
    "\n",
    "### Hugging Face\n",
    "\n",
    "- [AdapterHub](https://adapterhub.ml/)\n",
    "- AdapterFusion\n",
    "\n",
    "Подробнее:\n",
    "- [Lialin et al - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "- [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Alignment\n",
    "\n",
    "![](res_nlp/alignment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF\n",
    "\n",
    "![](res_nlp/rlhf_overview.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Stiennon et al - Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### не RL\n",
    "\n",
    "- LIMA: предлагают использовать SFT на тщательно отобранных примерах (1000), без использования RL\n",
    "- DPO: предлагают новый алгоритм оптимизации, который не требует RL, но результаты DPO лучше, чем у RLHF\n",
    "\n",
    "Подробнее:\n",
    "- [Zhout et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)\n",
    "- [Rafailov et al - Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Проблемы\n",
    "\n",
    "Есть примеры сложных типов задач, которые SOTA модели решают.\n",
    "\n",
    "![](res_nlp/gpt-4.png)\n",
    "\n",
    "Есть примеры типов задач, которые SOTA модели решают плохо, хотя люди решают хорошо.\n",
    "\n",
    "Например, применяя простые правила арифметики люди легко перемножают трёхзначные числа.\n",
    "В то время, как ChatGPT и GPT4 решают такие задачи только в 55% и 59% случаев, соответственно.\n",
    "\n",
    "Подробнее:\n",
    "- [Dziri et al - Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/abs/2305.18654)\n",
    "- [Bubeck et al - Sparks of Artificial General Intelligence:\n",
    "Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
