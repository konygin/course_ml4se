{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intro\n",
    "2. Prompting\n",
    "3. PEFT\n",
    "4. Alignment\n",
    "5. LLMs for Code\n",
    "6. Exercise\n",
    "7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers ([lena-voita.github.io](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)):\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCALING LAWS\n",
    "\n",
    "![](res/05_scaling_laws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LARGE MODELS\n",
    "\n",
    "![](https://cdn.builder.io/api/v1/image/assets%2Fe0438815ba51486bbb6a202747122d4b%2F894dade70d724952bfad3956c599865a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONTEXT SIZE\n",
    "\n",
    "![](https://i.redd.it/b3h0mebrssna1.png)\n",
    "\n",
    "Claude 2.1 --- 200K токенов контекста\n",
    "- примерно 150K слов\n",
    "- сотни страниц текста\n",
    "- пара книг (\"Великий Гэтсби около 72K токенов)\n",
    "- текст, который чтобы прочитать потребуется около 10 часов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HARD TASKS FOR LLMS\n",
    "\n",
    "Есть примеры сложных задач, которые некоторые LLMs решают хорошо.\n",
    "\n",
    "![](res/07_hard_task_gpt4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть примеры простых задач, которые LLMs решают плохо.\n",
    "\n",
    "![](res/07_letters_1_false.png)\n",
    "\n",
    "![](res/07_letters_2.png)\n",
    "\n",
    "![](res/07_letters_3_false.png)\n",
    "\n",
    "![](res/07_math_1_false.png)\n",
    "\n",
    "![](res/07_math_1_true.png)\n",
    "\n",
    "![](res/07_math_2.png)\n",
    "\n",
    "![](res/07_math_3.png)\n",
    "\n",
    "#### MORE\n",
    "- [Dziri et al - Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/abs/2305.18654)\n",
    "- [Bubeck et al - Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IN-CONTEXT LEARNING\n",
    "\n",
    "In-context learning (ICL) is a technique where task demonstrations are integrated into the prompt in a natural language format.\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)\n",
    "\n",
    "- 0-shot\n",
    "- 1-shot\n",
    "- few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSTRUCTIONS\n",
    "\n",
    "- LM just predicts the next token given the previous tokens\n",
    "- One core capability of Large Language Models (LLMs) is to follow natural language instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAIN-OF-THOUGHT\n",
    "\n",
    "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MORE\n",
    "\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)\n",
    "- [Microsoft: The power of prompting](https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/)\n",
    "- [Yandex: Тетрадь с чит-промптами](https://ya.ru/project/cheat-prompts/index)\n",
    "- [Antropic: Prompt engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TAXONOMY\n",
    "\n",
    "![](res/07_peft_taxonomy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "\n",
    "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MORE\n",
    "\n",
    "- [Lialin et al - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "- [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [Practical tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Alignment\n",
    "\n",
    "![](res/07_alignment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLHF\n",
    "\n",
    "![](2022-2023/res_nlp/rlhf_overview.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Stiennon et al - Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### не RL\n",
    "\n",
    "- LIMA: предлагают использовать SFT на тщательно отобранных примерах (1000), без использования RL\n",
    "- DPO: предлагают новый алгоритм оптимизации, который не требует RL, но результаты DPO лучше, чем у RLHF\n",
    "\n",
    "#### MORE\n",
    "- [Zhout et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)\n",
    "- [Rafailov et al - Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)\n",
    "- https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx\n",
    "- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "- https://github.com/opendilab/awesome-RLHF\n",
    "- https://rail.eecs.berkeley.edu/deeprlcourse/\n",
    "- [лекция](https://www.youtube.com/watch?v=f5JFXvX7FLE&list=PL6Wui14DvQPzMqtOOnfL00ZQ61JcN8V87&index=7) с gpt week\n",
    "- [+ семинар](https://www.youtube.com/watch?v=dcN0BFa0OAI&list=PL6Wui14DvQPzMqtOOnfL00ZQ61JcN8V87&index=8&pp=iAQB) по алайменту\n",
    "- [блог hf по алайменту](https://huggingface.co/blog/pref-tuning): DPO, IPO, KTO\n",
    "- [свежий блог](https://huggingface.co/blog/constitutional_ai) от hf по Constitutional AI, тоже алаймент\n",
    "- [Новые методы алаймента языковых моделей — Борис Шапошников, Тинькофф](https://www.youtube.com/watch?v=IwA1ZgM5RFA)\n",
    "- [Статья от Anthropic, в которой вводится терминология Harmless, Helpful, Honest агента, и в целом описан процесс обучения модели предпочтений](https://arxiv.org/abs/2112.00861)\n",
    "- [C-RLFT, алаймент для Openchat](https://arxiv.org/pdf/2309.11235.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMs for Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**model**     | **team** |**year** | **context** | **terms**   | **reference**|\n",
    "|--------------|----------|---------|-------------|-------------|--------------|\n",
    "| GPT-4        | OpenAI   | 2023    | 32K         | [pricing](https://openai.com/pricing)|[gpt-4](https://openai.com/gpt-4) |\n",
    "| codellama-7b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf) |\n",
    "| codellama-13b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf) |\n",
    "| codellama-34b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |\n",
    "| codellama-70b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-70b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf) |\n",
    "| deepseek-coder-33b-instruct | deepseek | 2024 | 16K | [terms](https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL) | [deepseek-ai/deepseek-coder-33b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct)|\n",
    "| codegemma-2b | google   | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-2b](https://huggingface.co/google/codegemma-2b)|\n",
    "| codegemma-7b | google   | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-7b](https://huggingface.co/google/codegemma-7b)|\n",
    "| codegemma-7b-it| google | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-7b-it](https://huggingface.co/google/codegemma-7b-it)|\n",
    "|              |          |         |                          |                    |             |              |\n",
    "|              |          |         |                          |                    |             |              |\n",
    "\n",
    "#### MORE\n",
    "\n",
    "- https://github.com/huybery/Awesome-Code-LLM\n",
    "- [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)\n",
    "- [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)\n",
    "- [CodeGemma: Open Code Models Based on Gemma](https://goo.gle/codegemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercise\n",
    "\n",
    "Необходимо провести мини-исследование на тему применимости больших языковых исследований для задачи детекции клонов.\n",
    "\n",
    "Возможный план:\n",
    "1. выбираете любую открытую модель (codellama, codegemma etc.)\n",
    "2. выбираете любой датасет для клонов, или его часть, или сами придумываете небольшое кол-во примеров\n",
    "3. подбираете промпт\n",
    "4. получаете ответы модели\n",
    "5. превращаете ответы модели в метки (любым способом: классификация, регулярки, вручную...)\n",
    "6. считаете метрики, делаете выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Self-Play fIne-tuNing](https://arxiv.org/abs/2401.01335)\n",
    "- [Comprehensive Language Model Fine Tuning](https://www.ntentional.com/nlp/datasets/tokenization/processing/2020/10/09/comprehensive-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
