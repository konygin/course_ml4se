{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intro\n",
    "- Prompting\n",
    "- PEFT\n",
    "- Alignment\n",
    "- LLMs for Code\n",
    "- Exercise\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "[Source: [lena-voita.github.io](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)]\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture-of-Experts (MoE)\n",
    "\n",
    "[Source: [github.com/huggingface/MoE](https://github.com/huggingface/blog/blob/main/moe.md)]\n",
    "\n",
    "In the context of transformer models, a MoE consists of two main elements:\n",
    "- Sparse MoE layers are used instead of dense feed-forward network (FFN) layers.\n",
    "    - MoE layers have a certain number of “experts” (e.g. 8), where each expert is a neural network.\n",
    "    - In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs!\n",
    "- A gate network or router, that determines which tokens are sent to which expert.\n",
    "    - For example, in the image below, the token “More” is sent to the second expert, and the token \"Parameters” is sent to the first network.\n",
    "    - As we’ll explore later, we can send a token to more than one expert.\n",
    "    - How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network\n",
    "\n",
    "![](./res/07_moe_switch.png)\n",
    "\n",
    "\n",
    "The operation of a router in a standard MoE layer typically follows this sequence:\n",
    "\n",
    "1.  _Input Reception:_ The router receives the representation (embedding) of an input token after it has passed through the self-attention mechanism in a transformer block.\n",
    "2.  _Score Calculation:_ A trainable network within the router (often a simple linear layer) calculates a score (logit) for each expert, indicating how well-suited that expert is to process the given token.\n",
    "3.  _Expert Selection (Routing):_ Based on the calculated scores, a subset of experts is selected. The most common strategy is _Top-$K$ Gating_, where the $K$ experts with the highest scores are chosen. For example, in models like Mixtral 8x7B and Grok-1, $K=2$ is used with a total of 8 experts.\n",
    "4.  _Output Aggregation:_ The selected experts independently process the token. Their outputs are then combined into a weighted sum. The weights are determined by the router's scores, normalized (typically using a softmax function over the top-K experts).\n",
    "\n",
    "It's crucial to understand that this process happens _independently_ in every MoE layer of the model. A token can be routed to different experts at different stages (layers) of its processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Laws\n",
    "\n",
    "[Source: [Kaplan et al - Scaling Laws for Neural Language Models 2020](https://arxiv.org/abs/2001.08361)]\n",
    "\n",
    "Model performance depends most strongly on scale, which consists of three factors:\n",
    "- the amount of compute used for training,\n",
    "- the size of the dataset, and\n",
    "- the number of model parameters.\n",
    "\n",
    " Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width.\n",
    "\n",
    "![](res/05_scaling_laws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: [LLMs](https://explodingtopics.com/blog/list-of-llms)]\n",
    "\n",
    "| LLM Name | Developer | Release Date | Access | Parameters |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| GPT-5 | OpenAI | August 7, 2025 | API | Unknown |\n",
    "| Claude 4.1 | Anthropic | August 5, 2025 | API | Unknown |\n",
    "| Grok 5 | xAI | July 9, 2025 | API | Unknown |\n",
    "| Qwen 3 | Alibaba | April 29, 2025 | API, Open Source | 235B |\n",
    "| GPT-o4-mini | OpenAI | April 16, 2025 | API | Unknown |\n",
    "| GPT-o3 | OpenAI | April 16, 2025 | API | Unknown |\n",
    "| GPT-4.1 | OpenAI | April 14, 2025 | API | Unknown |\n",
    "| Llama 4 Scout | Meta AI | April 5, 2025 | API | 17B |\n",
    "| Gemini 2.5 Pro | Google DeepMind | Mar 25, 2025 | API | Unknown |\n",
    "| GPT-4.5 | OpenAI | Feb 27, 2025 | API | Unknown |\n",
    "| Claude 3.7 Sonnet | Anthropic | Feb 24, 2025 | API | Unknown (est. 200B+) |\n",
    "| Grok-3 | xAI | Feb 17, 2025 | API | Unknown |\n",
    "| Gemini 2.0 Flash-Lite | Google DeepMind | Feb 5, 2025 | API | Unknown |\n",
    "| Gemini 2.0 Pro | Google DeepMind | Feb 5, 2025 | API | Unknown |\n",
    "| GPT-o3-mini | OpenAI | Jan 31, 2025 | API | Unknown |\n",
    "| Qwen 2.5-Max | Alibaba | Jan 29, 2025 | API | Unknown |\n",
    "| DeepSeek R1 | DeepSeek | Jan 20, 2025 | API, Open Source | 671B (37B active) |\n",
    "| DeepSeek-V3 | DeepSeek | Dec 26, 2024 | API, Open Source | 671B (37B active) |\n",
    "| Gemini 2.0 Flash | Google DeepMind | Dec 11, 2024 | API | Unknown |\n",
    "| Sora | OpenAI | Dec 9, 2024 | API | Unknown |\n",
    "| Nova | Amazon | Dec 3, 2024 | API | Unknown |\n",
    "| Claude 3.5 Sonnet (New) | Anthropic | Oct 22, 2024 | API | Unknown |\n",
    "| GPT-o1 | OpenAI | Sept 12, 2024 | API | Unknown (o1-mini est. ~100B) |\n",
    "| DeepSeek-V2.5 | DeepSeek | Sept 5, 2024 | API, Open Source | Unknown |\n",
    "| Grok-2 | xAI | Aug 13, 2024 | API | Unknown |\n",
    "| Mistral Large 2 | Mistral AI | July 24, 2024 | API | 123B |\n",
    "| Llama 3.1 | Meta AI | July 23, 2024 | Open Source | 405B |\n",
    "| GPT-4o mini | OpenAI | July 18, 2024 | API | ~8B (est.) |\n",
    "| Nemotron-4 | Nvidia | July 14, 2024 | Open Source | 340B |\n",
    "| Claude 3.5 Sonnet | Anthropic | June 20, 2024 | API | ~175-200B (est.) |\n",
    "| GPT-4o | OpenAI | May 13, 2024 | API | ~1.8T (est.) |\n",
    "| DeepSeek-V2 | DeepSeek | May 6, 2024 | API, Open Source | Unknown |\n",
    "| Phi-3 | Microsoft | April 23, 2024 | API, Open Source | Mini 3B, Small 7B, Medium 14B |\n",
    "| Mixtral 8x22B | Mistral AI | April 10, 2024 | Open Source | 141B (39B active) |\n",
    "| Jamba | AI21 Labs | Mar 29, 2024 | Open Source | 52B (12B active) |\n",
    "| DBRX | Databricks' Mosaic ML | Mar 27, 2024 | Open Source | 132B |\n",
    "| Command R | Cohere | Mar 11, 2024 | API, Open Source | 35B |\n",
    "| Inflection-2.5 | Inflection AI | Mar 7, 2024 | Proprietary | Unknown (predecessor ~400B) |\n",
    "| Gemma | Google DeepMind | Feb 21, 2024 | API, Open Source | 2B, 7B |\n",
    "| Gemini 1.5 | Google DeepMind | Feb 15, 2024 | API | ~1.5T Pro, ~8B Flash (est.) |\n",
    "| Stable LM 2 | Stability AI | Jan 19, 2024 | Open Source | 1.6B, 12B |\n",
    "| Grok-1 | xAI | Nov 4, 2023 | API, Open Source | 314 billion |\n",
    "| Mistral 7B | Mistral AI | Sept 27, 2023 | Open Source | 7.3 billion |\n",
    "| Falcon 180B | Technology Innovation Institute | Sept 6, 2023 | Open Source | 180 billion |\n",
    "| XGen-7B | Salesforce | July 3, 2023 | Open Source | 7 billion |\n",
    "| PaLM 2 | May 10, 2023 | API | 340 billion |\n",
    "| Alpaca 7B | Stanford CRFM | Mar 13, 2023 | Open Source | 7 billion |\n",
    "| Pythia | EleutherAI | Mar 13, 2023 | Open Source | 70 million to 12 billion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: [Timeline of major LLM releases](https://www.researchgate.net/publication/393983430_How_Well_Do_LLMs_Predict_Prerequisite_Skills_Zero-Shot_Comparison_to_Expert-Defined_Concepts)]\n",
    "\n",
    "![](./res/07_llm_timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: [menlovc.com](https://menlovc.com/perspective/2025-mid-year-llm-market-update/)]\n",
    "\n",
    "> Open-source models offer clear enterprise advantages: greater customization, potential cost savings, and the ability to deploy within private cloud or on-premises environments. But despite these benefits and recent improvements, open-source has continued to trail frontier, closed-source models in performance by nine to 12 months.\n",
    "\n",
    "![](./res/07_closed_vs_open.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The size of datasets used to train language models doubles approximately every six months\n",
    "\n",
    "[Source: [epoch.ai/data-insights/dataset-size-trend](https://epoch.ai/data-insights/dataset-size-trend)]\n",
    "\n",
    "> Across all domains of ML, models are using more and more training data. In language modeling, datasets are growing at a rate of 3.7x per year. The largest models currently use datasets with tens of trillions of words. The largest public datasets are about ten times larger than this, for example Common Crawl contains hundreds of trillions of words before.\n",
    "\n",
    "![](./res/07_data_trend_llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training open-weight models is becoming more data intensive\n",
    "\n",
    "[Source: [epoch.ai/data-insights/training-tokens-per-parameter](https://epoch.ai/data-insights/training-tokens-per-parameter)]\n",
    "\n",
    "> The ratio of training data to active parameters in open-weight LLMs has grown 3.1x per year since 2022. Recent models have been trained with 20 times more data per parameter than the optimal ratio suggested by the 2022 Chinchilla scaling laws. Our analysis focuses on open-weights models, where information on training tokens and parameters is more available.\n",
    "\n",
    "![](./res/07_training_tokens_per_parameter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Size\n",
    "\n",
    "[Source: [epoch.ai](https://epoch.ai/data-insights/context-windows)]\n",
    "\n",
    "> Since mid-2023, the longest LLM context windows have grown by about 30x per year. Their ability to use that input effectively is improving even faster: on two long-context benchmarks, the input length where top models reach 80% accuracy has risen by over 250x in the past 9 months.\n",
    "\n",
    "![](./res/07_context_windows.png)\n",
    "\n",
    "\n",
    "[Source: [lifearchitect.ai/gpt-4](https://lifearchitect.ai/gpt-4/)]\n",
    "\n",
    "200K context tokens:\n",
    "- about 150K words\n",
    "- hundreds of pages of text\n",
    "- a couple of books (The Great Gatsby about 72K tokens)\n",
    "- text that would take about 10 hours to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Tasks for LLMs\n",
    "\n",
    "There are examples of complex problems that some LLMs solve well.\n",
    "\n",
    "![](res/07_hard_task_gpt4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are examples of simple problems that LLMs do poorly.\n",
    "\n",
    "![](./res/07_llm_hard_gpt4o_calculator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE 2025.11.05\n",
    "\n",
    "```deepseek\n",
    "The product of 124454 and 54776 is 6817092304.\n",
    "\n",
    "This is calculated as follows:  \n",
    "124454 × 54776 = 124454 × (54000 + 776) = 124454 × 54000 + 124454 × 776  \n",
    "124454 × 54000 = 6720516000  \n",
    "124454 × 776 = 96576304  \n",
    "Adding these gives 6720516000 + 96576304 = 6817092304.\n",
    "\n",
    "Alternatively, using direct multiplication:  \n",
    "124454 × 54776 = 6817092304.\n",
    "\n",
    "\\boxed{6817092304}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./res/07_llm_hard_gpt4o_alice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE 2025.11.05\n",
    "\n",
    "```\n",
    "Alice has N brothers and M sisters. This means that, including Alice, there are M + 1 girls in the family. When considering Alice's brother, all the girls in the family are his sisters. Therefore, Alice's brother has M +  sisters.\n",
    "\n",
    "\\boxed{M+1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:\n",
    "```\n",
    "How many \"r\"s in \"strarrtrabbbery\"?\n",
    "\n",
    "```\n",
    "\n",
    "A:\n",
    "```\n",
    "The string \"strarrtrabbbery\" contains **5** instances of the letter \"r\". Here's a breakdown for clarity:\n",
    "\n",
    "- The string is: s, t, **r**, a, **r**, **r**, t, **r**, a, b, b, b, e, **r**, y\n",
    "- Positions of \"r\": 3rd, 5th, 6th, 8th, and 14th characters.\n",
    "\n",
    "Thus, the total count is 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More:\n",
    "\n",
    "- [Dziri et al - Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/abs/2305.18654)\n",
    "- [Bubeck et al - Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)\n",
    "- [Nezhurina et al - Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models](https://arxiv.org/abs/2406.02061)\n",
    "- [Williams Huckle - Easy Problems That LLMs Get Wrong](https://arxiv.org/abs/2405.19616v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning\n",
    "\n",
    "In-context learning (ICL) is a technique where task demonstrations are integrated into the prompt in a natural language format.\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)\n",
    "\n",
    "- 0-shot\n",
    "- 1-shot\n",
    "- few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- LM just predicts the next token given the previous tokens\n",
    "- One core capability of Large Language Models (LLMs) is to follow natural language instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thoughts\n",
    "\n",
    "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "Tool calling, also known as function calling, is a technique that allows LLMs to interact with external tools, APIs, or functions.\n",
    "This capability transforms LLMs from conversational chatbots into active agents that can perform actions like fetching real-time data or performing calculations.\n",
    "\n",
    "LLMs are specifically taught to use tools. The process looks like this:\n",
    "\n",
    "1.  Creating a Specialized Dataset: Researchers create thousands or millions of example dialogues that demonstrate _when_ and _how_ to call tools.\n",
    "2.  Training Data Structure:\n",
    "    - User Query: \"What's the weather in London?\"\n",
    "    - Correct Model Response (Internal Representation): Not just \"It's sunny, 20°C\", but a structured output:\n",
    "        ```json\n",
    "        {\n",
    "          \"tool_calls\": [{\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"arguments\": {\"location\": \"London\"}\n",
    "          }]\n",
    "        }\n",
    "        ```\n",
    "    *   The dataset also includes multi-turn examples where the model calls a tool, receives a result from the \"system,\" and then generates a final, natural language answer for the user.\n",
    "4.  The Learning Process: The model (already knowledgeable from pretraining) is fine-tuned on these specific examples. It learns to recognize the pattern: `If the user asks about X, and answering requires tool Y, then I must generate a structured JSON object instead of plain text.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Reasoning\" refers to the model's ability to process information in a multi-step, structured way to arrive at an answer or conclusion that is not directly present in its training data or the immediate prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Taxonomy\n",
    "\n",
    "![](res/07_peft_taxonomy.png)\n",
    "\n",
    "[Source: [Lialin et al 2023](https://arxiv.org/abs/2303.15647)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "![](./res/07_lora.png)\n",
    "\n",
    "[Source: [sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Alignment\n",
    "\n",
    "![](res/07_alignment.png)\n",
    "\n",
    "_AI Alignment_ (or LLM Alignment) is the field of research and techniques aimed at ensuring that an Artificial Intelligence system's goals and behaviors are in accordance with human intentions.\n",
    "\n",
    "For LLMs, this means shaping the model to be:\n",
    "1.  Helpful: It proactively tries to understand and fulfill the user's request.\n",
    "2.  Harmless: It refuses to generate dangerous, unethical, or illegal content.\n",
    "3.  Honest: It strives to provide accurate information and indicates when it is uncertain, rather than \"hallucinating\" confidently.\n",
    "\n",
    "## RLHF\n",
    "\n",
    "![](2022-2023/res_nlp/rlhf_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMs for Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Name | Release Date | Opensource or Closed | Number of Parameters (Billion) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **GPT-5** | August 2025 | Closed | Undisclosed |\n",
    "| **GPT-5-mini / nano** | 2025 (specific date not given) | Closed | Undisclosed |\n",
    "| **GPT-5-Codex** | September 2024 | Closed | Undisclosed |\n",
    "| **Claude Opus 4.1** | August 2025 | Closed | Undisclosed |\n",
    "| **Claude Sonnet 4.5** | September 2025 | Closed | Undisclosed |\n",
    "| **Claude Haiku 4.5** | October 2025 | Closed | Undisclosed |\n",
    "| **Gemini 2.5 Pro** | March 2025 | Closed | Undisclosed |\n",
    "| **Gemini 2.5 Flash / Flash-Lite** | 2025 (specific date not given) | Closed | Undisclosed |\n",
    "| **Mistral Large 2** | July 2024 | Closed | 123B |\n",
    "| **Codestral** | May 2024 | Closed | 22B |\n",
    "| **DeepSeek R1** | January 2025 | Open Source | 671B (37B active) |\n",
    "| **Qwen 3** | April 2025 | Open Source | 235B |\n",
    "| **Falcon 3** | December 2024 | Open Source | Up to 10B |\n",
    "| **Granite 3.2** | February 2025 | Open Source | 2B and 8B |\n",
    "| **Llama 4 Scout** | April 2025 | Information Missing | 17B |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "\n",
    "It is necessary to conduct a mini-research to determine whether LLMs are good for clone detection.  \n",
    "\n",
    "> The clone detection task: two snippets (which can be considered as two functions) are given as input.\n",
    "> It is necessary to determine whether they implement the same functionality or not. In other words, whether they will generate the same output given the same input.  \n",
    "\n",
    "To conduct the research, a dataset will be required. The [BigCloneBench](https://github.com/clonebench/BigCloneBench) dataset can be used.  \n",
    "\n",
    "The task will be evaluated based on how well-justified the conclusions are. If assumptions are made, they must be documented. In this task, it is necessary to make maximum use of LLMs.\n",
    "\n",
    "Possible plan:\n",
    "1. choose any open model (codellama, codegemma etc.)\n",
    "2. choose any dataset for clones, or part of it, or come up with a small number of examples yourself\n",
    "3. select a prompt\n",
    "4. get the model's responses\n",
    "5. transform the model's responses into labels (in any way: structured output, classification, regular expressions, manually...)\n",
    "6. calculate metrics, make conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Machine Learning Trends](https://epoch.ai/trends)\n",
    "- [Lenovo LLM Sizing Guide](https://lenovopress.lenovo.com/lp2130-lenovo-llm-sizing-guide)\n",
    "- [Mixture of Experts Explained](https://huggingface.co/blog/moe)\n",
    "- [What is MoE 2.0? Update Your Knowledge about Mixture-of-experts](https://huggingface.co/blog/Kseniase/moe2)\n",
    "- [How do mixture-of-experts models compare to dense models in inference?](https://epoch.ai/gradient-updates/moe-vs-dense-models-inference)\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)\n",
    "- [Microsoft: The power of prompting](https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/)\n",
    "- [Yandex: Тетрадь с чит-промптами](https://ya.ru/project/cheat-prompts/index)\n",
    "- [Antropic: Prompt engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Lialin et al - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "- [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [Practical tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)\n",
    "- [Zhou et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206): SFT on carefully selected examples (1000), without using RL\n",
    "- [Rafailov et al - Direct Preference Optimization](https://arxiv.org/abs/2305.18290)\n",
    "- [Constitutional AI](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)\n",
    "- [Zhout et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)\n",
    "- [Rafailov et al - Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)\n",
    "- https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx\n",
    "- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "- https://github.com/opendilab/awesome-RLHF\n",
    "- https://rail.eecs.berkeley.edu/deeprlcourse/\n",
    "- [Stiennon et al - Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)\n",
    "- https://github.com/huybery/Awesome-Code-LLM\n",
    "- [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)\n",
    "- [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)\n",
    "- [CodeGemma: Open Code Models Based on Gemma](https://goo.gle/codegemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
