{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "2. HumanEval\n",
    "3. Generation with LLMs\n",
    "4. Generation with Agents\n",
    "5. Context-aware generation\n",
    "6. Exercise\n",
    "7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROBLEM\n",
    "\n",
    "- Input: documentation (+ unit tests)\n",
    "- Output: code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASETS\n",
    "\n",
    "- [HumanEval](https://arxiv.org/abs/2107.03374): [164 programming problems](https://huggingface.co/datasets/openai_humaneval)\n",
    "- [MBPP](https://arxiv.org/abs/2108.07732): The Mostly Basic Programming Problem, [974 programming problems](https://huggingface.co/datasets/mbpp)\n",
    "- [WikiSQL](https://github.com/salesforce/WikiSQL): 87726 hand-annotated SQL query and natural language question pairs\n",
    "- [more datasets](https://paperswithcode.com/datasets?task=code-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRICS\n",
    "\n",
    "![](res/12_metrics.png)\n",
    "\n",
    "Source: [Li et al - The Metric for Automatic Code Generation 2020](https://www.sciencedirect.com/science/article/pii/S1877050920302210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRIC `pass@k`\n",
    "\n",
    "1. Generate $k$ code samples per problem.\n",
    "2. A problem is considered solved if any sample passes the unit tests, and the total fraction of problems solved is reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HumanEval\n",
    "\n",
    "- paper: [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)\n",
    "- github: [openai/human-eval](https://github.com/openai/human-eval)\n",
    "- dataset: [openai_humaneval](https://huggingface.co/datasets/openai_humaneval)\n",
    "- leaderboard: [Code Generation on HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval)\n",
    "- [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HumanEval:\n",
    "- 164 tasks\n",
    "- task information: function signature, docstring, body, and several tests (on average, 7.7 tests per task)\n",
    "- tasks are rewritten to avoid overlap with GitHub: for example, there are more than ten public repositories containing solutions to [Codeforces](https://codeforces.com/) tasks\n",
    "- programming tasks in the HumanEval dataset assess language understanding, reasoning, algorithms, and simple mathematics\n",
    "\n",
    "Quality is evaluated using the `pass@k` metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMPLES\n",
    "\n",
    "- white background: send to model\n",
    "- yellow background: generation result\n",
    "\n",
    "![Prompt](https://production-media.paperswithcode.com/datasets/a2af6cf1-212b-4a05-8d5c-55170a21ce05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generation with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEMO\n",
    "\n",
    "![](./res/12_qwen_demo.png)\n",
    "\n",
    "[Qwen2.5-Coder](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EASY WITH HUGGINGFACE\n",
    "\n",
    "```\n",
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model='lvwerra/codeparrot')\n",
    "\n",
    "text = 'def add_numbers(a, b):\\n    \"\"\"add two numbers\"\"\"'\n",
    "code = pipe(text)[0]['generated_text']\n",
    "\n",
    "print(code)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sure! Below is a simple implementation of the Quick Sort algorithm in Python:\n",
    "\n",
    "```python\n",
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "\n",
    "    return quick_sort(left) + middle + quick_sort(right)\n",
    "\n",
    "# Example usage:\n",
    "arr = [3, 6, 8, 10, 1, 2, 5, 7]\n",
    "sorted_arr = quick_sort(arr)\n",
    "print(sorted_arr)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- The function `quick_sort` takes an array `arr` as input.\n",
    "- It first checks if the array has one or zero elements. If so, it returns the array as is.\n",
    "- It selects the middle element of the array as the pivot.\n",
    "- It then creates three lists: `left`, `middle`, and `right`. These lists contain elements less than, equal to, and greater than the pivot, respectively.\n",
    "- The function recursively applies the `quick_sort` function to these three lists and concatenates the results with the pivot.\n",
    "- Finally, the function returns the sorted array.\n",
    "\n",
    "You can test the `quick_sort` function with different arrays to see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE MODELS: HUMANEVAL LEADERBOARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/12_humaneval_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/12_humaneval_2.png)\n",
    "\n",
    "Source: [[Zheng et al. - A Survey of Large Language Models for Code 2023]](https://arxiv.org/abs/2311.10372)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE MODELS: MBPP LEADERBOARD\n",
    "\n",
    "![](res/12_mbpp_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/12_mbpp_2.png)\n",
    "\n",
    "Source: [[Zheng et al. - A Survey of Large Language Models for Code 2023]](https://arxiv.org/abs/2311.10372)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE MODELS: BIG CODE LEADERBOARD\n",
    "\n",
    "![](./res/12_big_code_leaderboard.png)\n",
    "\n",
    "Source: [[bigcode-models-leaderboard]](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generation with Agents\n",
    "\n",
    "![](res/12_humaneval_agents_1.png)\n",
    "![](res/12_humaneval_agents_2.png)\n",
    "\n",
    "Source: [Code Generation on HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Rank | Model | Pass@1  | Source | Year| \n",
    "|---:|-------------------------------------------------------|-------|--------------------------------------------------------------------------------------------------------|-----:|\n",
    "|  1 | AgentCoder  (GPT-4)                                   | 96.3  | AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation                  | 2023 |\n",
    "|  2 | LDB  (GPT-3.5, based on seed programs from Reflexion) | 95.1  | LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step                      | 2024 |\n",
    "|  3 | Language Agent Tree Search  (GPT-4)                   | 94.4  | Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models                    | 2023 |\n",
    "|  4 | L2MAC  (GPT-4)                                        | 90.2  | L2MAC: Large Language Model Automatic Computer for Extensive Code Generation                           | 2023 |\n",
    "|  5 | OctorCoder  (GPT-4)                                   | 86.6  | OctoPack: Instruction Tuning Code Large Language Models                                                | 2023 |\n",
    "|  6 | ANPL  (GPT-4)                                         | 86.6  | ANPL: Towards Natural Programming with Interactive Decomposition                                       | 2023 |\n",
    "|  7 | MetaGPT  (GPT-4)                                      | 85.9  | MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework                                    | 2023 |\n",
    "|  8 | Parsel  (GPT-4 + CodeT)                               | 85.1  | Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions                         | 2022 |\n",
    "|  9 | Claude 3 Opus  (0-shot)                               | 84.9  | The Claude 3 Model Family: Opus, Sonnet, Haiku                                                         | 2024 |\n",
    "| 10 | Language Agent Tree Search  (GPT-3.5)                 | 83.8  | Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models                    | 2023 |\n",
    "| 11 | AgentCoder  (ChatGPT)                                 | 79.9  | AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation                  | 2023 |\n",
    "| 12 | LLMCodeGen Scrum  (GPT-3.5 + zero-shot)               | 78.5  | When LLM-based Code Generation Meets the Software Development Process                                  | 2024 |\n",
    "| 13 | GPT-4  (0-shot)                                       | 76.5  | DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence       | 2024 |\n",
    "| 14 | ANPL  (GPT-3.5)                                       | 76.2  | ANPL: Towards Natural Programming with Interactive Decomposition                                       | 2023 |\n",
    "| 15 | Claude 3 Haiku  (0-shot)                              | 75.9  | The Claude 3 Model Family: Opus, Sonnet, Haiku                                                         | 2024 |\n",
    "| 16 | INTERVENOR  (GPT-3.5)                                 | 75.6  | INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair | 2023 |\n",
    "| 17 | SRank  (WizardCoder)                                  | 75.31 | Neural Code Generation Enhancement via Functional Overlap Reranking                                    | 2023 |\n",
    "| 18 | Gemini Ultra  (0-shot)                                | 74.4  | Gemini: A Family of Highly Capable Multimodal Models                                                   | 2023 |\n",
    "| 19 | Claude 3 Sonnet  (0-shot)                             |    73 | The Claude 3 Model Family: Opus, Sonnet, Haiku                                                         | 2024 |\n",
    "| 20 | Gemini 1.5 Pro  (0-shot)                              | 71.9  | Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context                    | 2024 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Agent-based approaches outperform agent-less approaches.*\n",
    "\n",
    "More:\n",
    "- [Collaborative Agents](09_collaborative_agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Context-aware generation\n",
    "\n",
    "- fine-tuning\n",
    "- in-context learning\n",
    "- RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "![](res/12_naive_rag.png)\n",
    "\n",
    "Source: [[Gao et al. - Retrieval-Augmented Generation for Large Language Models 2024]](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "1. cleaning and extraction of raw data in diverse formats\n",
    "2. converting into a uniform plain text format\n",
    "3. text is segmented into smaller chunks\n",
    "4. chunks are then encoded into vector representations using an embedding model\n",
    "5. embeddings are stored in vector database\n",
    "\n",
    "This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "\n",
    "1. transform the query into a vector representation (the same encoding model utilized during the indexing phase)\n",
    "2. compute the similarity scores between the query vector and the vector of chunks within the indexed corpus.\n",
    "3. prioritize and retrieve the top-$K$ chunks that demonstrate the greatest similarity to the query\n",
    "\n",
    "The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation\n",
    "\n",
    "- chunks are subsequently used as the expanded context in prompt\n",
    "\n",
    "In generating responses, the model may face the issue of **hallucination**, where it produces content not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced RAG\n",
    "\n",
    "![](res/12_naive_advanced_modular_rag.png)\n",
    "\n",
    "Source: [[Gao et al. - Retrieval-Augmented Generation for Large Language Models 2024]](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG or Fine-Tuning\n",
    "\n",
    "![](res/12_rag_vs_all.png) \n",
    "\n",
    "Source: [[Gao et al. - Retrieval-Augmented Generation for Large Language Models 2024]](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG for Code Generation\n",
    "\n",
    "The diagram below shows a high-level RAG pattern for code generation with Codey APIs.\n",
    "\n",
    "![](res/12_context-aware_code_generation_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify source information: API definition, code repositories, documentation, or similar.\n",
    "2. Determine the chunking scheme: methods that respect natural code boundaries, such as function, class, or module borders; techniques like random splits or mid-sentence/clauses could break the context and degrade the output.\n",
    "3. Generate embeddings and index them in a vector database: when a query is received, another embedding is generated for the query and used to help retrieve relevant information chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More:\n",
    "- [Context-aware code generation: Retrieval augmentation and Vertex AI Codey APIs](https://cloud.google.com/blog/products/ai-machine-learning/context-aware-code-generation-rag-and-vertex-ai-codey-apis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercise\n",
    "\n",
    "Implement code generation using the RAG approach:\n",
    "- you have a small repository (several files), the code is written in a certain style (generate)\n",
    "- upon request, it is necessary to generate code that will be written in the same style\n",
    "\n",
    "Maybe useful: [Code a simple RAG from scratch](https://huggingface.co/blog/ngxson/make-your-own-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References\n",
    "\n",
    "- https://arxiv.org/abs/2107.03374\n",
    "- https://arxiv.org/abs/2208.03133\n",
    "- https://arxiv.org/abs/2311.10372\n",
    "- https://arxiv.org/abs/2401.15940\n",
    "- https://arxiv.org/abs/2403.08299\n",
    "- https://arxiv.org/abs/2407.21059v1\n",
    "- [Gao et al - Retrieval-Augmented Generation for Large Language Models A Survey 2024](https://arxiv.org/abs/2312.10997)\n",
    "- [Zhao et al - Retrieval-Augmented Generation for AI-Generated Content A Survey 2024](https://arxiv.org/abs/2402.19473)\n",
    "- [Yu et al - Evaluation of Retrieval-Augmented Generation A Survey 2024](https://arxiv.org/abs/2405.07437)\n",
    "- https://github.com/saltudelft/ml4se?tab=readme-ov-file#code-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
