{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. CODE SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] введение\n",
    "2. [x] данные\n",
    "3. [x] BLEU\n",
    "4. [x] упражение\n",
    "5. [x] ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение\n",
    "\n",
    "[Обратная задача](https://proceedings.neurips.cc/paper/2019/file/e52ad5c9f751f599492b4f087ed7ecfc-Paper.pdf) к задаче генерации кода:\n",
    "- синтез кода --- по описанию создать код;\n",
    "- суммаризация --- по коду создать описание.\n",
    "\n",
    "В целом, задача суммаризации (генерация docstring-а в случае Python) похожа на задачу генерации кода:\n",
    "- можно использовать аналогичную архитектуру\n",
    "- можно использовать аналогичные данные (вместо пары текст-код нужна пара код-текст)\n",
    "- НО нужна другая функция потерь, не можем использовать тесты для фильтрации или проверки\n",
    "\n",
    "\n",
    "Например, результаты использования модели генерации кода [InCode](https://arxiv.org/abs/2204.05999) для задачи суммаризации:\n",
    "\n",
    "| method | BLEU|\n",
    "| - | - |\n",
    "| InCoder | 16.05--18.27 |\n",
    "| RoBERTa (FT) | 18.14 |\n",
    "| CodeBERT (FT) | 19.06 |\n",
    "| PLBART (FT) | 19.30 |\n",
    "| CodeT5 (FT) | 20.36 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n",
    "Обычно данные для code summarization или code generation --- это пары: код и описание кода на естественном языке.\n",
    "\n",
    "- Java dataset: [Hu et al. 2018](https://xin-xia.github.io/publication/icpc182.pdf)\n",
    "- Python dataset: [Wei et al. 2018](https://arxiv.org/abs/1910.05923)\n",
    "- [CodeSearchNet](https://github.com/github/CodeSearchNet) [[Husain et al. 2019](https://github.com/github/CodeSearchNet)]: 2 миллиона пар (комментарий, код); языки: Python, Javascript, Ruby, Go, Java, PHP\n",
    "- CoCoNet: [Wang et al. 2021](https://arxiv.org/abs/2107.01933): 1.2 миллиона пар, Java\n",
    "\n",
    "Инструменты для сбора своего датасета:\n",
    "- [code-summarization-dataset](https://github.com/JetBrains-Research/code-summarization-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeXGLUE\n",
    "\n",
    "CodeSerachNet входит в бенчмарк [CodeXGLUE](https://arxiv.org/abs/2102.04664).\n",
    "CodeXGLUE (Microsoft) включает в себя [набор](https://github.com/microsoft/CodeXGLUE) задач анализа кода и платформу для оценки и сравнения моделей.\n",
    "CodeXGLUE расшифровывается как *General Language Understanding Evaluation* для *CODE*.\n",
    "Он включает в себя 14 наборов данных для 10 разнообразных задач анализа кода, охватывающих следующие сценарии:\n",
    "- код-код (обнаружение клонов, обнаружение дефектов и т.д.)\n",
    "- text-code (поиск в коде, генерация кода)\n",
    "- код-текст (суммаризация)\n",
    "- текст-текст (перевод документации)\n",
    "\n",
    "![](https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/tasks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU (bilingual evaluation understudy, [Papineni et al. 2002](https://aclanthology.org/P02-1040/)) --- алгоритм оценки качества текста, который был машинно переведен с одного естественного языка на другой.\n",
    "Качество считается соответствием между работой машины и человека: \"чем ближе машинный перевод к профессиональному человеческому переводу, тем он лучше\".\n",
    "BLEU была одной из первых метрик, заявивших о высокой корреляции с человеческими суждениями о качестве, и остается одной из самых популярных автоматических и недорогих метрик.\n",
    "\n",
    "Баллы рассчитываются для отдельных переведенных сегментов (как правило, предложений) путем сравнения их с набором эталонных переводов хорошего качества. Эти оценки затем усредняются по всему корпусу, чтобы получить оценку общего качества перевода.\n",
    "\n",
    "Результатов оценки BLEU всегда представляет собой число от 0 до 1. Это значение указывает, насколько текст-кандидат похож на справочные тексты (чем больше, тем лучше).\n",
    "\n",
    "Как вычислить BLEU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сначала Precision\n",
    "\n",
    "1. для каждой $n$-граммы из предложения-кандидата считаем, сколько раз она встречается в исходном предложении\n",
    "2. суммируем общее количество вхождений для каждой уникальной $n$-граммы из предложения-кандидата\n",
    "3. и делим на количество $n$-грамм в предложении-кандидате.\n",
    "\n",
    "*Пример 1*:\n",
    "- для простоты, $n = 1$, униграмма --- отдельное слово\n",
    "- исходное предложение: `the cat is on the mat`\n",
    "- предложение-кандидат: `the cat the cat on the mat`\n",
    "- все $n$-граммы предложения-кандидата: `the` `cat` `on` `mat`\n",
    "\n",
    "\n",
    "|кандидат     | the | cat | the | cat | on | the | mat |\n",
    "|-------------|-----|-----|-----|-----|----|-----|-----|\n",
    "|встречается? | 1   | 1   | 1   | 1   | 1  | 1   | 1   |\n",
    "\n",
    "Суммируем:\n",
    "\n",
    "|             | the | cat | on | mat |\n",
    "|-------------|-----|-----|----|-----|\n",
    "|сколько раз? |  3  |  2  |  1 |  1  |\n",
    "\n",
    "Таким образом, общее количество вхождения равно для каждой уникальной $n$-граммы из предложения-кандидата $3+2+1+1=7$. При этом, количестов $n$-грамм в предложении-кандидате тоже равно $7$. Таким образом, precision равно $\\frac{7}{7} = 1$.\n",
    "\n",
    "\n",
    "*Пример 2*:\n",
    "- исходное предложение: `the cat is on the mat`\n",
    "- предложение-кандидат: `the cat this cat on the mat`\n",
    "- все $n$-граммы предложения-кандидата: `the` `cat` `this` `on` `mat`\n",
    "\n",
    "\n",
    "|кандидат     | the | cat | this | cat | on | the | mat |\n",
    "|-------------|-----|-----|------|-----|----|-----|-----|\n",
    "|встречается? | 1   | 1   | 0    | 1   | 1  | 1   | 1   |\n",
    "\n",
    "Суммируем:\n",
    "\n",
    "|             | the | cat | this | on | mat |\n",
    "|-------------|-----|-----|------|----|-----|\n",
    "|сколько раз? |  2  |  2  |  0   |  1 |  1  |\n",
    "\n",
    "Общее количество вхождения равно для каждой уникальной $n$-граммы из предложения-кандидата $2+2+0+1+1=6$. При этом, количестов $n$-грамм в предложении-кандидате тоже равно $7$. Таким образом, precision равно $\\frac{6}{7}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Потом Modified precision\n",
    "\n",
    "1. для каждой $n$-граммы из предложения-кандидата определям, встречается ли она в исходном предложении\n",
    "2. суммируем общее количество вхождений для каждой уникальной $n$-граммы из предложения-кандидата\n",
    "3. считаем минимум между полученным значением и количеством вхождений в исходное предложение\n",
    "4. и делим на количество $n$-грамм в предложении-кандидате.\n",
    "\n",
    "*Пример*:\n",
    "- исходное предложение: `the cat is on the mat`\n",
    "- предложение-кандидат: `the cat the cat on the mat`\n",
    "\n",
    "\n",
    "|кандидат     | the | cat | the | cat | on | the | mat |\n",
    "|-------------|-----|-----|-----|-----|----|-----|-----|\n",
    "|встречается? |True |True |True |True |True|True |True |\n",
    "\n",
    "Суммируем:\n",
    "\n",
    "|             | the | cat | on | mat |\n",
    "|-------------|-----|-----|----|-----|\n",
    "|сколько раз? |  3  |  2  |  1 |  1  |\n",
    "\n",
    "Минимизируем:\n",
    "\n",
    "|             | the | cat | on | mat |\n",
    "|-------------|-----|-----|----|-----|\n",
    "|сколько раз? |  2=min(3,2)  |  1=min(2,1)  |  1 |  1  |\n",
    "\n",
    "\n",
    "Общее количество вхождений для уникальных $n$-грамм в исходном предложении равно $5=2+1+1+1$, и общее количество униграмм в предложении-кандидате равно $7$. Таким образом, modified precision равно $\\frac{5}{7}$.\n",
    "\n",
    "Теперь для каждой пары: (предложение-кандидат и исходное предложение) мы можем считать modified precision. Если исходных предложений несколько (например, несколько правильных переводов), то имеем свой modified precision (обозначается через $p_n)$ для каждой пары."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь [BLEU](https://en.wikipedia.org/wiki/BLEU)\n",
    "\n",
    "$$BLEU = BP⋅\\exp \\left(\\sum_{n=1}^N w_n \\ln p_n \\right)$$\n",
    "где\n",
    "- $N$ --- верхняя граница для $n$ (например, $\\infty$)\n",
    "- $p_n$ --- это modified precision для $n$-грамм,\n",
    "- $w_n$ --- фиксированный набор весовых коэффициентов таких, что $\\sum_{n=1}^N w_n = 1$, $w_n \\geq 0$,\n",
    "- $BP$ --- штраф за краткость (штрафуем короткие машинные переводы).\n",
    "\n",
    "Пусть\n",
    "- $c$ --- длина предложения-кандидата (или сумма длин, если их несколько),\n",
    "а\n",
    "- $r$ --- сумма длин наилучших совпадений каждого предложения-кандидата с исходными предложениями (если у нас одно предложение-кандидат, что имеем длину наиболее близкого по длине исходного предложения).\n",
    "\n",
    "Тогда\n",
    "\n",
    "$BP = \\left\\{\n",
    "  \\begin{array}{ c l }\n",
    "    1 & \\quad \\textrm{если } c > r \\\\\n",
    "    \\exp(1−\\frac{r}{c}) & \\quad \\textrm{иначе}\n",
    "  \\end{array}\n",
    "\\right.$\n",
    "\n",
    "\n",
    "> Пример:\n",
    "> Здесь наилучшей длиной совпадения является ближайшая к предложениям-кандидатам длина исходного предложения.\n",
    "> Например, если есть три исходных предложения длины $12$, $14$ и $17$ слов,\n",
    "> а предложение-кандидат содержит $13$ слов,\n",
    "> то теоретически наилучшей длиной совпадения может быть либо $12$, либо $14$,\n",
    "> но мы выбираем более короткую, которая равна $12$ ($r = 12$).\n",
    "\n",
    "Обычно BLEU оценивается на корпусе, где есть много предложений-кандидатов, переведенных из разных исходных текстов, и каждое из них имеет исхожных предложений.\n",
    "\n",
    "\n",
    "Нетрудно обнаружить, что BLEU всегда имеет значение от $0$ до $1$.\n",
    "Действительно, $BP$, $w_n$ и $p_n$ всегда от $0$ до $1$, и\n",
    "  \n",
    "$$\\exp \\left(\\sum_{n=1}^N w_n \\ln p_n \\right)=\\prod_{n=1}^N \\exp\\left(w_n \\ln p_n \\right)=\\prod_{n=1}^N\\left[\\exp(\\ln p_n)\\right]^{w_n}= \\prod_{n=1}^N p_n^{w_n} \\in [0, 1]$$\n",
    "\n",
    "Обычно, для BLEU используют $N=4$ и $w_n=\\frac{1}{N}$.\n",
    "\n",
    "\n",
    "> Пример: Несложно вычислить modified precision для остальных $n$-грамм.\n",
    "> Итак,\n",
    "> \n",
    "> $$p_1=\\frac{5}{7}, p_2=\\frac{4}{6}, p_3=\\frac{2}{5}, p_4=\\frac{1}{4}$$\n",
    "> $$w_1=w_2=w_3=w_4=\\frac{1}{4}$$\n",
    "> \n",
    "> Поскольку в корпусе имеется только один набор переводов, имеем $c=7$ и $r=7$.\n",
    "> \n",
    "> $$BP=1$$\n",
    "> \n",
    "> Мы подставляем эти значения к уравнению BLEU, и получаем\n",
    "> \n",
    "> $$BLEU=0.467$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "\n",
    "score = nltk.translate.bleu_score.corpus_bleu(references, candidates, weights=(0.25,0.25,0.25,0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR\n",
    "\n",
    "[METEOR](https://en.wikipedia.org/wiki/METEOR) (Metric for Evaluation of Translation with Explicit ORdering, [Banerjee, Lavie 2005](https://aclanthology.org/W05-0909/)) — метрика для оценки качества машинного перевода. Метрика базируется на использовании $n$-грамм и ориентирована на использование статистической и точной оценки исходного текста. В отличие от метрики BLEU, данная метрика использует функции сопоставления синонимов вместе с точным соответствием слов. Метрика была разработана, чтобы решить проблемы, которые были найдены в более популярной метрике BLEU, а также создать хорошую корреляцию с оценкой экспертов на уровне словосочетаний или предложений. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE-L\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (Recall-Oriented Understudy for Gisting Evaluation, [Lin 2004](https://aclanthology.org/W04-1013/)) --- набор показателей и программный пакет, используемый для оценки программного обеспечения автоматического суммирования и машинного перевода при обработке естественного языка. Метрики сравнивают автоматически созданную сводку или перевод со справочной информацией или набором ссылок (созданным человеком) сводкой или переводом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIDEr\n",
    "\n",
    "[CIDEr](https://arxiv.org/abs/1411.5726) (Consensus-based Image Description Evaluation, [Vedantam et al. 2015](https://arxiv.org/abs/1411.5726))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CodeBLEU (не для суммаризации)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU разработан для NLP и не учитывает отличия языков программирования от естественных языков.\n",
    "- небольшое количество ключевых слов в языках программирования; при этом ключевые слова более важные, чем остальные\n",
    "- код имее древовидную структуру\n",
    "- код понимается однозначно\n",
    "\n",
    "Поэтому был предложен [CodeBLEU](https://arxiv.org/abs/2009.10297) (Microsoft, 2020) --- использует $n$-граммы, AST и потоки данных (data-flow).\n",
    "\n",
    "![](./res/06_codebleu_formula.png)\n",
    "\n",
    "$$CodeBLEU = \\alpha \\cdot BLEU + \\beta \\cdot BLEU_{weight} + \\gamma \\cdot Match_{ast} + \\delta \\cdot Match_{df}$$\n",
    "\n",
    "- $\\alpha = \\beta = \\gamma = \\delta = 0.25$\n",
    "- $BLEU$ --- стандартный $BLEU$\n",
    "- $BLEU_{weight}$ --- взвешанный $BLEU$, учитывает важность отдельных слов (ключевых слов): значение $p_n$ учитывает веса $n$-граммы. Например, в исходной работе $N = 1$, $w_1 = 1$, ключевые слова имеют вес в $5$ раз больше\n",
    "- $Match_{ast}$ --- мера похожести AST-деревьев. Из деревьев выкидываются листья, поскольку в них хранятся имена функций, значения переменных. Сравнивается количество поддеревьев, построенных библиотекой `tree-sitter`. $Match_{ast} = \\frac{Count_{clip}(T_{cand})}{Count(T_{ref})}$,\n",
    " где $Count(T_{ref})$ --- общее количество поддеревьев у референсного кода, $Count_{clip}(T_{cand})$ --- количество поддеревьев в коде-кандидате, которые соответствуют (в некотором смысле) референсному коду.\n",
    "- $Match_{df}$ --- строится граф потока данных (аналогично [GraphCodeBERT](https://arxiv.org/abs/2009.08366)): узлы --- переменные, дуги графа --- откуда возникла каждая переменная. Оцениваем похожесть этих графов (отношение количества переменных). \n",
    "\n",
    "![](./res/06_codebleu_dataflow.png)\n",
    "\n",
    "\n",
    "> Примеры\n",
    "> ![](./res/06_codebleu_example_1.png)\n",
    "> ![](./res/06_codebleu_example_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Упражнение\n",
    "\n",
    "На основе существующих открытых обученных моделей (CodeBERT, InCoder) собрать решение для суммаризации кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Полезные ссылки\n",
    "\n",
    "- [Code summarization](https://github.com/saltudelft/ml4se#code-summarization)\n",
    "- [Recommendations for Datasets for Source Code Summarization](https://arxiv.org/abs/1904.02660)\n",
    "- [Ahmad et al - A Transformer-based Approach for Source Code Summarization 2021](https://arxiv.org/abs/2005.00653)\n",
    "- [Wang et al - CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network 2021 (Microsoft)](https://arxiv.org/abs/2107.01933)\n",
    "- [Shi et al - On the Evaluation of Neural Code Summarization 2021 (Microsoft)](https://arxiv.org/abs/2107.07112)\n",
    "- [Rauf et al - Meta Learning for Code Summarization 2022](https://arxiv.org/abs/2201.08310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
