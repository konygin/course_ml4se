{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. VULNERABILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] введение\n",
    "2. [x] VulBERTa\n",
    "3. [x] упражнение\n",
    "4. [x] ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение\n",
    "\n",
    "*Уязвимость (vulnerability)* --- недостаток в системе, который может нарушить eё целостность. Уязвимость можно рассматривать в качесте одного из типов дефектов\n",
    "\n",
    "### Как возникают?\n",
    "\n",
    "Уязвимость может быть результатом:\n",
    "- ошибок программирования\n",
    "- недостатков, допущенных при проектировании системы\n",
    "- ненадежных паролей\n",
    "- вирусов\n",
    "- других вредоносных программ, скриптовых и SQL-инъекций.\n",
    "\n",
    "### Чем опасны?\n",
    "\n",
    "Обычно уязвимость позволяет атакующему \"обмануть\" приложение:\n",
    "- выполнить непредусмотренные создателем действия или\n",
    "- заставить приложение совершить действие, на которое у того не должно быть прав.\n",
    "\n",
    "### Примеры?\n",
    "\n",
    "```\n",
    "var express = require('express')\n",
    "\n",
    "var app = express()\n",
    "const Sequelize = require('sequelize');\n",
    "const sequelize = new Sequelize('database', 'username', 'password', {\n",
    "  dialect: 'sqlite',\n",
    "  storage: 'data/juiceshop.sqlite'\n",
    "});\n",
    "\n",
    "app.post('/login', function (req, res) {\n",
    "    sequelize.query('SELECT * FROM Products WHERE name LIKE ' +  req.body.username);\n",
    "  })\n",
    "```\n",
    "\n",
    "Ещё примеры: [Vulnerable-Code-Snippets](https://github.com/snoopysecurity/Vulnerable-Code-Snippets)\n",
    "\n",
    "\n",
    "### Как обнаружить?\n",
    "\n",
    "Для выявления уязвимостей проводятся *пентесты* (тестирование на проникновение).\n",
    "\n",
    "Появляются подходы с применением машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VulBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Статья\n",
    "\n",
    "[Hanif Maffeis - VulBERTa: Simplified Source Code Pre-Training for Vulnerability Detection 2022](https://arxiv.org/abs/2205.12424) --- использование архитектуры Трансформер для обнаружения уязвимостей в C/C++ коде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Подход\n",
    "\n",
    "Используется модель RoBERTa с задачей MLM.\n",
    "На выходе токены коды преобразуются в эмбеддинги длины 768.\n",
    "Далее эмбеддинги используются для в качестве вход для нейронной сети, которая выполняет классификацию.\n",
    "\n",
    "Общая схема:\n",
    "\n",
    "![](./res/11_vulberta_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Токенизация\n",
    "\n",
    "Общая схема:\n",
    "![](./res/11_vulberta_tokenization.png)\n",
    "\n",
    "### Парсер\n",
    "\n",
    "- На входе: код.\n",
    "- На выходе: промежуточные токены.\n",
    "\n",
    "Как работает парсер?\n",
    "0. работаем на уровне функий --- разбиваем код на отдельные функции\n",
    "1. убираем комментарии из каждой функции с помощью регулярных выражений\n",
    "2. разбиваем на промежуточные токены (учитываем синтаксическую структуру кода) с помощью библиотеки [Clang](https://clang.llvm.org/)\n",
    "\n",
    "![](./res/11_vulberta_parser.png)\n",
    "\n",
    "\n",
    "### Токенизатор\n",
    "\n",
    "- На входе: промежуточные токены.\n",
    "- На выходе: токены (идентификаторы).\n",
    "\n",
    "Токенизатор основан на [BPE](http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html) (byte pair encoding). Кроме того, в словарь токенов добавляем дополнительные токены --- ключевые слова из языка программирования, пунктуацию, названия некоторых функций. Таким образом, мы хотим обеспечить, чтобы ключевые слова не дробились и информация о исходном коде лучше сохранялась.\n",
    "Вместе со специальными BPE-токенами мы имеем следующие 451 предопределённый токен:\n",
    "\n",
    "![](./res/11_vulberta_tokens.png)\n",
    "\n",
    "Остальные токены определяются в результате алгоритма BPE.\n",
    "\n",
    "После того, как определены токены, происходит кодирование токенов --- получаем тензоры длины 512 (1024 для файнтюнинга), использую токен `<pad>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Pre-training\n",
    "\n",
    "Предобучение --- это обучение модили [RoBERTa-base](https://huggingface.co/roberta-base) на задаче MLM (masked language modelling).\n",
    "Отличия BERT и RoBERTa:\n",
    "- в BERT маскирование выполняется только один раз во время подготовки данных. Например, каждое предложение маскируется 10 различными способами. Во время обучения модель увидит только эти 10 вариантов каждого предложения\n",
    "- в RoBERTa маскировка выполняется во время обучения. Следовательно, каждый раз, когда предложение включается в batch, оно маскируется, и, следовательно, количество потенциально различных замаскированных версий каждого предложения неограничено\n",
    "\n",
    "\n",
    "Для обучения нужны данные. Здесь использовалось два датасета GitHub и Draper.\n",
    "\n",
    "### GitHub \n",
    "\n",
    "- 1101075 функций на языках C/C++\n",
    "- 1060 репозитория\n",
    "- для сбора данных использовались: [GitHub API](https://docs.github.com/en/rest) и [PyGitHub](https://github.com/PyGithub/PyGithub)\n",
    "- репозитории выбирались на основе количества звёзд\n",
    "- для извлечения функций исползовалась библиотека [Joern](https://joern.io/)\n",
    "\n",
    "### Draper\n",
    "\n",
    "- 1274366 функций на языках C/C++\n",
    "- есть разметка, выполненная с помощью инструментов статического анализа (потенциальные уязвимости)\n",
    "- датасет [опубликован](https://osf.io/d45bw)\n",
    "\n",
    "### Обучение\n",
    "\n",
    "- Google Compute Engine (GCP) VMs\n",
    "- 48 vCPUs\n",
    "- 240GB RAM\n",
    "- 2 NVIDIA Tesla A100 40GB GPUs\n",
    "- 72--96 часов на эксперимент\n",
    "- 500000 шагов (обновлений градиента)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Fine-tuning\n",
    "\n",
    "Здесь реализовано два подхода: VulBERTa-MLP и VulBERTa-CNN.\n",
    "\n",
    "![](./res/11_vulberta_finetuning.png)\n",
    "\n",
    "### VulBERTa-MLP\n",
    "\n",
    "Эмбеддинги из предобученного RoBERTa отправляются в MLP, где происходит классификация на два класса: содержит ли код уязвимость или нет.\n",
    "- полносвязный слой (размер 768)\n",
    "- выходной слой (размер 2)\n",
    "- бинарная классификация\n",
    "\n",
    "![](./res/11_vulberta_mlp.png)\n",
    "\n",
    "Обучаем только MLP, несколько эпох.\n",
    "\n",
    "### VulBERTa-CNN\n",
    "\n",
    "В этом подходе используется Text Convolutional Neural Network ([TextCNN](https://github.com/delldu/TextCNN)).\n",
    "\n",
    "![](./res/11_vulberta_textcnn.jpg)\n",
    "\n",
    "- одномерная свёрточная сеть\n",
    "- слой max-pooling\n",
    "- выходы конкатенируются и вытягиваются\n",
    "- полносвязный слой (размер 256)\n",
    "- полносвязный слой (размер 128)\n",
    "- выходной слой для классификации\n",
    "\n",
    "![](./res/11_vulberta_cnn.png)\n",
    "\n",
    "Обучение происходит быстрее, чем для VulBERT-MLP.\n",
    "\n",
    "### Данные для файнтюнинга\n",
    "\n",
    "1. [Vuldeepecker](https://github.com/CGCL-codes/VulDeePecker)\n",
    "2. [Draper](https://osf.io/d45bw)\n",
    "3. [REVEAL](https://arxiv.org/abs/2009.07235)\n",
    "4. [muVuldeepecker](https://github.com/muVulDeePecker/muVulDeePecker)\n",
    "5. [Devign](https://arxiv.org/abs/1909.03496)\n",
    "6. [D2A](https://github.com/IBM/D2A)\n",
    "\n",
    "### Обучение\n",
    "\n",
    "- a machine with 48 cores Intel Xeon Silver CPU\n",
    "- 292GB RAM\n",
    "- 2 NVIDIA GTX TITAN Xp GPU (12GB of video memory)\n",
    "- 10 эпох\n",
    "- 5--10 часов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Evaluation\n",
    "\n",
    "![](./res/11_vulberta_results.png)\n",
    "\n",
    "![](./res/11_vulberta_benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Реализация\n",
    "\n",
    "github: https://github.com/ICL-ml4csec/VulBERTa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Упражнение\n",
    "\n",
    "Использовать VulBERTa для поиска уязвимостей в любом из репозиториев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ссылки\n",
    "\n",
    "- [What is a Vulnerability? Definition + Examples](https://www.upguard.com/blog/vulnerability)\n",
    "- https://ulearn.me/Course/Hackerdom/Temy_kursa_2bc58982-19aa-49ef-bae2-8aaa5d13b967\n",
    "- Software Vulnerability Detection Using Deep Neural Networks: A Survey (2020), Proceedings of the IEEE, Lin, Guanjun, et al.\n",
    "- https://github.com/saltudelft/ml4se#bugvulnerability-detection\n",
    "- I. Rosenberg, G. Sicard, and E. David, “End-to-end deep neural networks and transfer learning for automatic analysis of nation-state malware,” Entropy, vol. 20, no. 5, p. 390, 2018.\n",
    "- L. Chen, S. Sultana, and R. Sahita, “Henet: A deep learning approach on intel processor trace for effective exploit detection,” in 2018 IEEE security and privacy workshops (SPW), 2018, pp. 109--115.\n",
    "- Zhang et al - COMBO Pre-training representations of binary code using contrastive learning 2022\n",
    "- [Hanif Maffeis - VulBERTa: Simplified Source Code Pre-Training for Vulnerability Detection](https://arxiv.org/abs/2205.12424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
