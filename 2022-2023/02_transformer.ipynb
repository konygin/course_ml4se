{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02.  ТРАНСФОРМЕР\n",
    "\n",
    "1. [x]  seq2seq\n",
    "2. [x]  Transformer\n",
    "3. [x]  общая схема\n",
    "4. [x]  энкодер\n",
    "5. [x]  декодер\n",
    "6. [x]  attention\n",
    "7. [x]  FFN\n",
    "8. [x]  embeddings and softmax\n",
    "9. [x]  positional encodings\n",
    "10. [x] упражнение\n",
    "11. [x] ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq --- это семейство подходов для задач преобразования последовательностей (например, NLP, ASR, ML4SE).\n",
    "\n",
    "На текущий момент, самые распространённые подходы --- это подходы на основе Трансформера (энкодер-декодер, декодер или энкодер).\n",
    "Здесь мы будем рассматривать архитектуру полного Трансформера (энкодер-декодер).\n",
    "\n",
    "Энкодер-декодерная модель состоит из:\n",
    "- энкодера (encoder) и\n",
    "- декодера (decoder).\n",
    "\n",
    "*Энкодер* отображает входную последовательность представлений символов $(x_1, ..., x_n)$ в последовательность непрерывных представлений $z = (z_1, ..., z_n)$.\n",
    "\n",
    "*Декодер* отображает $z$ в выходную последовательность $(y_1, ..., y_m)$ символов по одному элементу за раз.\n",
    "\n",
    "На каждом шаге модель является авторегрессивной, получая ранее сгенерированные символы в качестве дополнительных входных данных при создании следующего.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*GvN7ekDa8rFPAZIc_xUmIw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f23f4539950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Transformer\n",
    "\n",
    "Ранее использовались энкодер-декодерные архитектуры на основе [RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html). В 2017 был предложена архитектура Трансформер (Google Brain).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*d41DR9IWuuF_y-k8Iff4HA.png)\n",
    "\n",
    "[Vaswani et al - Attention Is All You Need 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Общая схема\n",
    "\n",
    "Трансформер состоит из энкодера и декодера, каждый из которых состоит из слоёв.\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Энкодер\n",
    "\n",
    "Энкодер состоит из $N$ (например,  $N = 6$) одинаковых последовательных слоёв.\n",
    "Каждый слой имеет два подслоя:\n",
    "- multi-head self-attention mechanism, и\n",
    "- position-wise fully connected feed-forward network.\n",
    "\n",
    "Кроме этого используется *residual connection* вокруг каждого из двух подслоев,\n",
    "за которым следует [layer normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "![](https://files.ai-pool.com/a/214fce0794479b4acc9b32fd4f6d0216.png)\n",
    "![](https://files.ai-pool.com/a/1aad7d26ad7a918d3ae54adde77c1a5d.png)\n",
    "\n",
    "Более формально, выход каждого подслоя --- это `LayerNorm(x + Sublayer(x))`, где `Sublayer(x)` --- это функция, реализованная самим подслоём.\n",
    "\n",
    "![](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure_W640.jpg)\n",
    "\n",
    "Все подслои в модели и выходные эмбеддинги имеют одинаковую размерность $d_{model}$ (например, $d_{model} = 512$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Декодер\n",
    "\n",
    "\n",
    "Декодер также состоит из $N$ последовательных одинаковых слоёв (например, $N = 6$).\n",
    "Кроме аналогичных двух подслоёв добавляется третий подслой,\n",
    "- который реализует multi-head attention над выходами с энкодера.\n",
    "\n",
    "Также как и в энкодере, в декодере испольузуются residual connections вокруг каждого из подслоёв с последующей layer normalization.\n",
    "\n",
    "![](https://i.stack.imgur.com/kOs4Z.png)\n",
    "\n",
    "Важно, что подслой self-attention в декодере модифицирован:\n",
    "- добавлены маски для предотвращения посещения последующих позиций.\n",
    "Такое маскирование в сочетании с тем фактом, что выходные эмбеддинги смещены на одну позицию, гарантирует, что прогнозы для позиции $i$ могут зависеть только от известных выходов в позициях, меньших $i$.\n",
    "\n",
    "![](https://peterbloem.nl/files/transformers/masked-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Attention\n",
    "\n",
    "Attention можно описать как функцию, отображающую *запрос (query)* и множество пар *ключ-значение (key-value)* в некоторый выход.\n",
    "\n",
    "Запрос, ключи, значения и выход являются векторами.\n",
    "Вектор вычисляется как взвешенная сумма значений, где вес, присвоенный каждому значению, вычисляется по паре \"запрос-ключ\".\n",
    "\n",
    "![](https://i.stack.imgur.com/SG66z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "На входе мы имеем:\n",
    "- запросы и ключи ключей размерности $d_k$ и\n",
    "- значений размерности $d_v$.\n",
    "\n",
    "Далее вычисляем скалярные произведения запроса со всеми ключами, делим каждое такое произведение на $\\sqrt{d_k}$.\n",
    "\n",
    "Затем применяем softmax для получения весов, с которыми будем суммировать значения.\n",
    "\n",
    "![](https://wiki.aidungeon.io/images/b/bb/Scaled_Dot-Product_Attention.png)\n",
    "\n",
    "На практике вычисления происходят в матричной форме (одновременно для набора запросов, упакованных вместе в матрицу $Q$). Ключи и значения также упаковываются вместе в матрицы $K$ и $V$.\n",
    "\n",
    "Таким образом,\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Деление на $\\sqrt{d_k}$ позволяет уменьшить значение больших скалярных произведений.\n",
    "(Если они большие, то функция softmax в таком случае имеет очень маленькие градиенты. Это плохо.)\n",
    "\n",
    "\n",
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention позволяет запустить несколько attention-механизмов, каждый из которых может обладать своими полезными особенностями.\n",
    "Таким образом собирается информация из различных представлений.\n",
    "\n",
    "![](https://i.ytimg.com/vi/mMa2PmYJlCo/mqdefault.jpg)\n",
    "\n",
    "Поэтому с помощью линейных преобразований (projections) получаем несколько ($h$) троек запрос-ключ-значение (размерности, соответственно, $d_k$, $d_k$, $d_v$).\n",
    "\n",
    "Затем для каждой из этих троек параллельно запускается attention-механизм, что даёт выходные значения размерности $d_v$.\n",
    "Далее полученные выходные значения конкатенируются и снова проецируются.\n",
    "\n",
    "![](https://repository-images.githubusercontent.com/283979760/0f00ed80-d368-11ea-979d-78033d0a1cee)\n",
    "\n",
    "Более формально:\n",
    "\n",
    "$$MultiHead(Q, K, V ) = concat(head_1, ..., head_h)W^O$$\n",
    "\n",
    "где $head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)$ и используются следующие матрицы проецирования\n",
    "\n",
    "$W^Q_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^K_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^V_i \\in R^{d_{model} \\times d_v}$,\n",
    "\n",
    "$W^O_i \\in R^{hd_v \\times d_{model}}$.\n",
    "\n",
    "Размерности могут быть, например, следующими $h = 8$, $d_k = d_v = \\frac{d_{model}}{h} = 64$.\n",
    "\n",
    "Трансформер использует multi-head attention тремя различными способами:\n",
    "\n",
    "1. В слоях \"энкодер-декодер attention\" запросы поступают с предыдущего уровня декодера, а ключи и значения поступают с выходных данных энкодера. Это позволяет каждой позиции в декодере посетить все позиции входной последовательности.\n",
    "\n",
    "2. Энкодер содержит self-attention слои.\n",
    "Здесь все ключи, значения и запросы поступают из одного и того же места --- из вывода предыдущего слоя энкодера. Каждая позиция в энкодере может посетить все позиции предыдущего слоя энкодера.\n",
    "\n",
    "3. Декодер содержит self-attention слои.\n",
    "Здесь допускается для каждой позиции декодера посетить все предыдущие позиции (слева) до текущей позии включительно.\n",
    "Такое ограничение возникает, поскольку необходимо сохранить авторегрессионное свойство декодера (по предыдущим токенам предсказывать следующий).\n",
    "Ограничение реализуются внутри scaled dot-product attention максированием (полагая $-\\infty$) всех значений на входе softmax-а, которые соответствуют недопустимым посещениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Position-wise Feed-Forward Networks\n",
    "\n",
    "Каждый из слоёв энкодера и декодера содержит FFN. Этот подслой одинаково применяется к каждой позиции отдельно.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
    "Он состоит из двух линейных преобразований с активацией [ReLU](https://arxiv.org/abs/1803.08375) между ними.\n",
    "\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Хотя линейные преобразования одинаковы для разных позиций,\n",
    "  они используют разные параметры от слоя к слою.\n",
    "\n",
    "Внутренний слой имеет большую размерностью.\n",
    "Например, размерность входа и выхода $d_{model} = 512$, размерность внутреннего слоя $d_{ff} = 2048$.\n",
    "\n",
    "В частности мы имеем, что в Трансформере токен в каждой позиции проходит свой собственный путь в энкодере.\n",
    "Есть зависимости на уровне self-attention слоёв, но нет зависимостей на уровне FFN.\n",
    "Это позволяет проходить FFN-подслои параллельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Embeddings и softmax\n",
    "\n",
    "Для преобразования входных/выходных токенов в вектора (размерности $d_{model}$) используются обучаемые эмбеддинги.\n",
    "Для преобразования выходов декодеров в предсказанные вероятности следующего токена используется обучаемое линейное преобразование и softmax.\n",
    "\n",
    "Чуть более подробно.  На выходе декодера мы получаем вектор.\n",
    "Этот вектор проходит через линейный слой (fully connected neural network), за которым следует softmax.\n",
    "\n",
    "Линейный слой проецирует выход декодера в гораздо больший вектор (logits-вектор).\n",
    "Размер этого вектор --- это размер словаря токенов.\n",
    "\n",
    "Затем слой softmax превращает координаты в вероятности.\n",
    "Выбирается наиболее вероятный токен.\n",
    "\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Positional Encoding\n",
    "\n",
    "Для того, чтобы использовать информацию о порядке токенов в последовательности, в Трансформере применяются positional encoding. Positional encoding --- это вектор, в котором закодирован номер.\n",
    "Он имеет ту же длину, что и эмбеддинги ($d_{model}$). И  прибавляется к эмбеддингу на входе.\n",
    "\n",
    "Например, можно использовать такие функции:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "где $pos$ --- это индекс позиции, а $i$  --- индекс координаты.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Собираем всё вместе\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражение\n",
    "\n",
    "Реализовать Трансформер с нуля на PyTorch. \n",
    "Быть готовыми ответить на вопросы по коду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезные ссылки и источники изображений\n",
    "\n",
    "- [Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Robertson - Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "- [Alammar - Mechanics of Seq2seq Models With Attention](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Alammar - The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Doshi - Transformers Explained Visually 1](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)\n",
    "- [Doshi - Transformers Explained Visually 2](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)\n",
    "- [Doshi - Transformers Explained Visually 3](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "- [Doshi - Transformers Explained Visually 4](https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)\n",
    "- [Bloem - Transformers from scratch](https://peterbloem.nl/blog/transformers)\n",
    "- [Harvard - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Karpathy - minGPT](https://github.com/karpathy/minGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
