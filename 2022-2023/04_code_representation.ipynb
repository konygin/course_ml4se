{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. CODE REPRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] введение\n",
    "2. [x] CodeBERT\n",
    "5. [x] упражнение\n",
    "6. [x] ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение\n",
    "\n",
    "Гипотеза (например, [Hindle et al - On the naturalness of software 2016](https://dl.acm.org/doi/abs/10.1145/2902362)):\n",
    ">\n",
    "> *Язык программирования (PL) похож на естественный язык (NL).*\n",
    "\n",
    "Если так, то можем использовать эффективные подходы из NLP, учитывая специфику PL.\n",
    "\n",
    "Encoder-only:\n",
    "- CuBERT (Kanade et al, 2019)\n",
    "- CodeBERT (Feng et al, 2020)\n",
    "- GraphCodeBERT (Guo et al, 2020)\n",
    "- SynCoBERT (Wang et al, 2022)\n",
    "\n",
    "Decoder-only:\n",
    "- GPT-C (Svyatkovskiy et al, 2020)\n",
    "- CodeGPT (Lu et al, 2021)\n",
    "\n",
    "Encoder-decoder:\n",
    "- PLBART (Ahmad et al, 2021)\n",
    "- CodeT5 (Wang et al, 2021)\n",
    "- TreeBERT (Jiang et al, 2021)\n",
    "- UniXcoder (Guo et al, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CodeBERT\n",
    "\n",
    "[CodeBERT](https://arxiv.org/abs/2002.08155v4) --- бимодальная предварительно обученная модель для языков программирования и естественного языка. CodeBERT на выходе выдаёт эмбеддинги общего назначения. Эмбеддинги могут использоваться в таких задач, как поиск по коду, суммаризация кода и т. д. Модель разработана в Microsoft в 2020 году.\n",
    "\n",
    "![](./res/04_codebert_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура\n",
    "\n",
    "В основе модели CodeBERT лежит [BERT](https://arxiv.org/abs/1810.04805)(Google, 2018). Точнее, [RoBERTa](https://arxiv.org/abs/1907.11692) (Meta, 2019, отличие в процедуре формирования масок).\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*p4LFBwyHtCw_Qq9paDampA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "print(nl_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "print(code_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "\n",
    "CodeBERT --- это бимодальная модель. Это значит, что на вход могут подаваться пары вида (NL, PL).\n",
    "При этом пара преобразуется в последовательность токенов.\n",
    "Для этого, каждый элемент пары: NL- и PL-часть, с помощью токенизатора ([Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding), BPE) превращается в последовательность токенов (сегмент).\n",
    "\n",
    "На этапе предобучения модели на вход подаётся конкатенация двух сегментов со специальным токеном-разделителем:\n",
    "\n",
    "$$[\\texttt{CLS}], w_1, w_2, \\ldots, w_n, [\\texttt{SEP}], c_1, c_2, \\ldots, c_m, [\\texttt{EOS}].$$\n",
    "\n",
    "- первый сегмент: текст естественного языка\n",
    "- второй сегмент: исходный код\n",
    "\n",
    "$[\\texttt{CLS}]$ --- специальный токен для получение агрегированного эмбеддинга (может быть использован, например, при классификации)\n",
    "\n",
    "На выходе имеем:\n",
    "- контекстное векторное представление каждого токена (для естественного языка и кода);\n",
    "- представление для токена $[\\texttt{CLS}]$.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Обучение модели CodeBERT происходит как на *бимодальных* данных (параллельные пары текст и кода), так и на *унимодальных* данных.\n",
    "\n",
    "- из репозиториев Github\n",
    "- бимодальная пара --- это отдельная функция и соответствующая документация (примерно, 2 млн)\n",
    "- унимодальные данные --- это функция без парной документации (примерно, 6 млн)\n",
    "- шесть языков программирования: Python, Java, JavaScript, PHP, Ruby и Go.\n",
    "\n",
    "### Обучение\n",
    "\n",
    "Предобучение происходит на двух задачах:\n",
    "- MLM (masked language modeling), BERT, Devlin et al. (2018): восстановление токенов, бимодальные данные\n",
    "- RTD (replaced token detection), ELECTRA, Clark et al. (2020): обнаружение замены токенов, унимодальные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача MLM**\n",
    "\n",
    "![](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png)\n",
    "\n",
    "Пусть $x = \\{w, c\\}$ --- входная NL-PL пара, где $w$ --- последовательность NL-токенов, а $c$ --- последовательность PL-токенов.\n",
    "\n",
    "Сначала выбираем случайный набор позиций для NL и PL, которые необходимо замаскировать --- $m_w$ и $m_c$ соответственно. Затем заменяем выбранные позиции специальным токеном $[\\texttt{MASK}]$. Как и в Devlin et al. (2018), маскируется 15% токенов из $x$ (если выбран токен, то с вероятностью 0.8 он заменяется на $[\\texttt{MASK}]$, с вероятностью 0.1 заменяется на случайный токен, с вероятностью 0.1 остаётся прежним).\n",
    "\n",
    "- для $i \\in \\{1, \\ldots, |w|\\}$ имеем $m^w_i ∼ \\texttt{unif}\\{1, |w|\\}$ --- какие NL-токены хотим маскировать\n",
    "- для $i \\in \\{1, \\ldots, |c|\\}$ имеем $w^c_m ∼ \\texttt{unif}\\{1, |c|\\}$ --- какие PL-токены хотим маскировать\n",
    "- $w^{masked} = \\texttt{REPLACE}(w, m^w, [\\texttt{MASK}])$\n",
    "- $c^{masked} = \\texttt{REPLACE}(c, m^c, [\\texttt{MASK}])$\n",
    "- $x = [w;c]$ --- конкатенируем\n",
    "\n",
    "В MLM задача состоит в том, чтобы предсказать токены, которые были маскированы. Для этого используется следующая функция потерь:\n",
    "\n",
    "$$L_{MLM}(\\theta) = \\sum_{i \\in m^w \\cup m^c} -\\log p (x_i~|~w^{masked}, c^{masked}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE = \"\"\"\n",
    "if x is not <mask>:\n",
    "    x += 1\"\n",
    "\"\"\"\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "outputs = fill_mask(CODE)\n",
    "pp.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача RTD**\n",
    "\n",
    "Изначально, задача RTD (replaced token detection) была использована для предобучния моделей естественного языка [ELECTRA](https://arxiv.org/abs/2003.10555). Для модели CodeBERT эта задача была адаптирована для кода.\n",
    "Используются как *бимодальные*, так и *унимодальные* данные. В частности, здесь есть два генератора данных:\n",
    "- NL-генератор $p^{G_w}$ и\n",
    "- PL-генератор $p^{G_c}$.\n",
    "\n",
    "Оба генератора нужны для создания правдоподобных альтернатив выбранных токенов.\n",
    "\n",
    "![](./res/04_rtd_electra.png)\n",
    "\n",
    "Генератор --- произвольая языковая модель, генерирующая распределение на токенах. Обычно используют небольшую  модель, обучающуюся совместно с генератором.\n",
    "\n",
    "Более формально:\n",
    "- для $i \\in m^w$ имеем $\\hat{w}_i ∼ p^{G_w} (w_i~|~w_{masked})$ --- какие токены и на какие хотим заменить\n",
    "- для $i \\in m^c$ имеем $\\hat{c}_i ∼ p^{G_c} (c_i~|~c_{masked})$ --- аналогично для кода\n",
    "- $w^{corrupt} = \\texttt{REPLACE}(w, m^w , \\hat{w})$\n",
    "- $c^{corrupt} = \\texttt{REPLACE}(c, m^c , \\hat{c})$\n",
    "- $x^{corrupt} = [w^{corrupt};c^{corrupt}]$ --- конкатенируем \n",
    "\n",
    "Задача дискриминатора определить, является ли токен оригинальным или нет, т.е. бинарная классификация.\n",
    "![](./res/04_rtd_codebert.png)\n",
    "\n",
    "Заметим, что генератор применяется к каждой позиции во входных данных и отличается от GAN тем,\n",
    " что, если генератор выдает правильный токен, то метка этого токена \"original\", а не \"replaced\".\n",
    "\n",
    "\n",
    "Функция потерь для задачи RTD:\n",
    "\n",
    "$$L_{RTD}(\\theta) = \\sum_{i=1}^{|w|+|c|}(\\delta(i) \\log p^{D} (x^{corrupt}, i) + (1-\\delta(i))(1-\\log p^{D}(x^{corrupt}, i))),$$\n",
    "\n",
    "где\n",
    "- $\\theta$ --- параметры,\n",
    "- $p^{D}$ --- дискриминатор, предсказывающий вероятность того, что $i$-й токен является оригинальным,\n",
    "- $\\delta(i)$ --- индикаторная функция: $\\delta(i) = 1$, если $x^{corrupt}_i = x_i$; иначе, $\\delta(i) = 0.$\n",
    "\n",
    "Существует множество различных способов реализации генераторов. Для CodeBERT используются две модели языка (NL, PL) на основе  $n$-грамм (Jurafsky, 2000) с двунаправленными контекстами. Модели были обучены на соответствующих унимодальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача MLM+RTD**\n",
    "\n",
    "Итоговая задача выглядит так:\n",
    "$$ \\min_{\\theta}( L_{MLM}(\\theta) + L_{RTD}(\\theta) )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализация\n",
    "\n",
    "- https://github.com/microsoft/CodeBERT\n",
    "- https://huggingface.co/microsoft/graphcodebert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Упражнение\n",
    "\n",
    "Исследовать возможность использовать CodeBERT для задачи clone detection (поиск клонов).\n",
    "Например, можно использовать датасет [BigCloneBench](https://github.com/clonebench/BigCloneBench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Kanade et al - CuBERT: Learning and Evaluating Contextual Embedding of Source Code](https://arxiv.org/abs/2001.00059)\n",
    "- [Feng et al - CodeBERT: A Pre-trained Model for Programming and Natural Languages 2020](https://arxiv.org/abs/2002.08155)\n",
    "- [Karampatsis Sutton - SCELMo: Source Code Embeddings from Language Models 2020](https://arxiv.org/abs/2004.13214)\n",
    "- [Jain et al - Contrastive Code Representation Learning 2020](https://arxiv.org/abs/2007.04973)\n",
    "- [Guo et al - GraphCodeBERT: Pre-training Code Representations with Data Flow 2020](https://arxiv.org/abs/2009.08366)\n",
    "- [Chirkova Torshin - An Empirical Study of Transformers for Source Code 2020](https://arxiv.org/abs/2010.07987)\n",
    "- [Gu et al - Multimodal representation for neural code search 2022](https://arxiv.org/abs/2107.00992)\n",
    "- [Guo et al - UniXcoder: Unified Cross-Modal Pre-training for Code Representation](https://arxiv.org/abs/2203.03850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
