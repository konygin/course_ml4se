{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. SEQ2SEQ\n",
    "\n",
    "1. Seq2seq models\n",
    "2. Tokenization: text\n",
    "3. Tokenization: code\n",
    "4. Exercise\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seq2seq models\n",
    "\n",
    "The paper [Sutskever et al - Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) introduced a universal architectural approach for solving sequence transformation problems.\n",
    "\n",
    "- English to French translation\n",
    "- two recurrent neural networks work together to transform one sequence to another\n",
    "- an encoder network condenses an input sequence into a vector\n",
    "- a decoder network eats that vector to produce an output sequence\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "Consider the sentence *\"Je ne suis pas le chat noir\"* --- *\"I am not the black cat\"*. Most of the words in the input sentence have a direct translation in the output sentence, but are in slightly *different* orders, e.g. chat noir and black cat. Because of the *ne/pas* construction there is also one more word in the input sentence. It would be difficult to produce a correct translation directly from the sequence of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the _meaning_ of the input sequence into a single vector --- a single point in some $N$ dimensional space of sentences.\n",
    "\n",
    "More details:\n",
    "> [Robertson - NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- takes input\n",
    "- capture essential information\n",
    "- creates hidden states\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- takes hidden states\n",
    "- generates the output sequence\n",
    "- operates in an autoregressive manner, producing one element of the output sequence at a time. At each step, it considers the previously generated elements, hidden states, and the input to make predictions for the next element in the output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" style=\"width:800px; height:auto;\"/>\n",
    "\n",
    "More:\n",
    "- [Alammar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Voita - Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [pytorch.org - Language Modeling with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#language-modeling-with-nn-transformer-and-torchtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization: text\n",
    "\n",
    "To use the seq2seq architecture for text, the text must be represented as a sequence of numerical data.\n",
    "To do this, the text needs to be split into tokens (pieces of text), and then the tokens must be converted into numbers.\n",
    "\n",
    "Tokenization:\n",
    "- Word-level tokenization\n",
    "- Char-level tokenization\n",
    "- Subword tokenization\n",
    "    - [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE)\n",
    "    - [Sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization $\\rightarrow$ Vocabulary $\\rightarrow$ Embeddings\n",
    "\n",
    "![](./res/03_vocabulary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Convert': 0, 'text': 1, 'into': 17, 'a': 8, 'sequence': 4, 'of': 11, 'tokens,': 13, 'create': 7, 'numerical': 9, 'representation': 10, 'the': 12, 'and': 14, 'assemble': 15, 'them': 16, 'tensors': 18}\n"
     ]
    }
   ],
   "source": [
    "text = 'Convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors'\n",
    "words = text.split(' ')\n",
    "tokens = {v: k for k, v in enumerate(words)}\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks?\n",
    "\n",
    "- Huge vocabulary\n",
    "- Out-Of-Vocabulary (OOV) problem: new words which are encountered at testing (UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0, 'o': 116, 'n': 114, 'v': 3, 'e': 113, 'r': 117, 't': 112, ' ': 111, 'x': 10, 'i': 107, 'a': 93, 's': 118, 'q': 22, 'u': 50, 'c': 55, 'f': 75, 'k': 83, ',': 87, 'm': 105, 'l': 99, 'p': 61, 'h': 103, 'd': 91, 'b': 98}\n"
     ]
    }
   ],
   "source": [
    "tokens = {v: k for k, v in enumerate(text)}\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- It is simple to implement.\n",
    "- It helps solve the Out-Of-Vocabulary problem.\n",
    "\n",
    "Cons:\n",
    "- It makes it much harder for the model to learn meaningful input representations. This leads to a loss in performance, as the model cannot distinguish between fundamental differences (e.g., \"a\" vs. \"apple\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subword tokenization\n",
    "\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492062561/files/assets/anlp_0401.png\" style=\"width:800px; height:auto;\"/>\n",
    "\n",
    "- Increase the amount of information per token.\n",
    "- Decrease the total number of tokens (vocabulary size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenizations:\n",
    "\n",
    "- Byte pair encoding (BPE). See [R. Sennrich et al. --- Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)\n",
    "- WordPiece. See [M. Schuster and K. Nakajima --- Japanese and Korean Voice Search, 2012](https://research.google/pubs/japanese-and-korean-voice-search/)\n",
    "- SentencePiece. See [T. Kudo and J. Richardson --- SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing, 2018](https://arxiv.org/abs/1808.06226)\n",
    "\n",
    "Reference:\n",
    "- https://www.oreilly.com/library/view/applied-natural-language/9781492062561/ch04.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of tokenization: [tiktokenizer.vercel.app](https://tiktokenizer.vercel.app/)\n",
    "\n",
    "GPT-4o, English\n",
    "\n",
    "<img src=\"res/03_to_be_or_not_to_be_en.png\" style=\"width:800px; height:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o, Russian\n",
    "\n",
    "<img src=\"res/03_to_be_or_not_to_be_ru.png\" style=\"width:800px; height:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3.5, Russian\n",
    "\n",
    "<img src=\"res/03_to_be_or_not_to_be_ru_3.5.png\" style=\"width:800px; height:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Original BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "\n",
    "Suppose the data to be encoded is\n",
    "\n",
    "> aaabdaaabac\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\".\n",
    "Now there is the following data and replacement table:\n",
    "\n",
    "> ZabdZabac\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n",
    "\n",
    "> ZYdZYac\n",
    "> \n",
    "> Y=ab\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n",
    "\n",
    "> XdXac\n",
    "> \n",
    "> X=ZY\n",
    "> \n",
    "> Y=ab\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\n",
    "\n",
    "To decompress the data, simply perform the replacements in the reverse order. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Implementation\n",
    "\n",
    "Source: https://gist.github.com/bigsnarfdude/8e99709d5c3d9d58b3831221fcbdaf68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print('length:', len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print('length:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (240, 159) into a new token 257\n",
      "merging (226, 128) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (97, 110) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (257, 133) into a new token 263\n",
      "merging (257, 135) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (239, 189) into a new token 266\n",
      "merging (258, 140) into a new token 267\n",
      "merging (267, 264) into a new token 268\n",
      "merging (101, 114) into a new token 269\n",
      "merging (111, 114) into a new token 270\n",
      "merging (116, 32) into a new token 271\n",
      "merging (259, 103) into a new token 272\n",
      "merging (115, 116) into a new token 273\n",
      "merging (261, 100) into a new token 274\n",
      "merging (32, 262) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 616\n",
      "ids length: 451\n",
      "compression ratio: 1.37X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forced splits (pre-tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 tokenizer\n",
    "\n",
    "<img src=\"./res/03_gpt2.png\" style=\"width:800px; height:auto;\"/>\n",
    "\n",
    "GPT-4 tokenizer\n",
    "\n",
    "<img src=\"./res/03_cl100kbase.png\" style=\"width:800px; height:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spe—Åial tokens\n",
    "\n",
    "Special tokens are additional tokens added during the tokenization process to serve specific purposes in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode [code points](https://en.wikipedia.org/wiki/Code_point) directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the [code points](https://en.wikipedia.org/wiki/Code_point) and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: [Andrej Karpathy - Tokenization :(](https://youtu.be/zduSFxRajkE?si=f40nDHXAPYIKWGlS)\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open('toy.txt', 'w', encoding='utf-8') as f:\n",
    "  f.write('SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input='toy.txt',\n",
    "  input_format='text',\n",
    "  # output spec\n",
    "  model_prefix='tok400', # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type='bpe',\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name='identity', # turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integers in GPT2\n",
    "\n",
    "![](https://www.beren.io/assets/figures/number_tokenization_weirdness_gpt2.png)\n",
    "\n",
    "- each row here represents 100 integers\n",
    "- if a square is colored yellow it means a unique token is assigned to that integer\n",
    "- if it is blue then the integer is coded by a composite set of tokens\n",
    "\n",
    "\n",
    "For instance, the number: 2249 is tokenized as ‚Äò2‚Äô, and ‚Äò249‚Äô (1-3). The number 2250 is tokenized as ‚Äò22‚Äô and ‚Äò50‚Äô (2-2) and the number ‚Äò2251‚Äô is tokenized as ‚Äò225‚Äô and ‚Äò1‚Äô (3-1).\n",
    "\n",
    "If we repeat our analysis but color in the different categories for how 4 digit numbers are tokenized ‚Äì i.e. as unique, 1-3 length tokens, 2-2 tokens or 3-1 tokens, we get the following result.\n",
    "\n",
    "![](https://www.beren.io/assets/figures/gpt2_number_composition.png)\n",
    "\n",
    "[Source: [Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huggingface\n",
    "\n",
    "- [Tokenizers.Quicktour](https://huggingface.co/docs/tokenizers/en/quicktour)\n",
    "- [Building a tokenizer, block by block](https://huggingface.co/learn/nlp-course/en/chapter6/8)\n",
    "- [Summary of the tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "\n",
    "encoded_input = tokenizer('Do not meddle in the affairs of wizards, for they are subtle and quick to anger.')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary with three important items:\n",
    "- `input_ids` are the indices corresponding to each token in the sentence.\n",
    "- `attention_mask` indicates whether a token should be attended to or not.\n",
    "- `token_type_ids` identifies which sequence a token belongs to when there is more than one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama\n",
    "\n",
    "- https://arxiv.org/abs/2302.13971\n",
    "- BPE algorithm (Sennrich et al., 2015), using the implementation from SentencePiece (Kudo and Richardson, 2018)\n",
    "- split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama 2\n",
    "\n",
    "- https://arxiv.org/abs/2307.09288\n",
    "- the same tokenizer as Llama 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization: code\n",
    "\n",
    "- https://pypi.org/project/code-tokenize/\n",
    "- [CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code](https://arxiv.org/abs/2308.00683)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InCoder\n",
    "\n",
    "- https://arxiv.org/abs/2204.05999\n",
    "- a byte-level BPE tokenizer Sennrich et al. (2016); Radford et al. (2019)\n",
    "- allow tokens to extend across whitespace (excluding newline characters) so that common code idioms (e.g., import numpy as np) are represented as single tokens in the vocabulary.\n",
    "\n",
    "This substantially improves the tokenizer‚Äôs efficiency reducing the total number of tokens required to encode our training corpus by 45% relative to the byte-level BPE tokenizer and vocabulary of GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VulBERTa\n",
    "\n",
    "- https://arxiv.org/abs/2205.12424\n",
    "- remove comments from the source code of each function using several regular expressions\n",
    "- parse the source code\n",
    "- the tokens produced by parser are further processed by the BPE algorithm, modified to take\n",
    "into account the pre-defined tokens\n",
    "- vocabulary size  is 50000\n",
    "- pre-defined 451 tokens are tokens that explicitly included in the vocabulary\n",
    "\n",
    "![](./res/03_vulberta_pre-defined_tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "\n",
    "- https://arxiv.org/abs/2308.12950\n",
    "- data is tokenized via BPE (Sennrich et al. (2016)), employing the same tokenizer as Llama and Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Star Coder\n",
    "\n",
    "- https://arxiv.org/abs/2305.06161\n",
    "- Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens, including the sentinel tokens\n",
    "- the pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeek Coder\n",
    "\n",
    "- https://arxiv.org/abs/2401.14196\n",
    "- HuggingFace Tokenizer library to train BPE tokenizers\n",
    "- vocabulary size is 32,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Star Coder 2\n",
    "\n",
    "- https://arxiv.org/abs/2402.19173\n",
    "- follow the procedure of StarCoderBase and train a byte-level Byte-Pair-Encoding tokenizer\n",
    "- vocabulary size is 49,152 tokens, including the sentinel tokens\n",
    "- The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems caused by tokenization\n",
    "\n",
    "- problems related to individual symbols (count the number of symbols, write in reverse)\n",
    "- arithmetic problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exercise\n",
    "\n",
    "Let's assume that we have a snippet of Python code (3.11) as a string as input.\n",
    "You need to:\n",
    "1. define the rules for splitting the string into substrings for tokenization\n",
    "2. implement these rules in Python  \n",
    "3. write a program that tokenizes (using SentencePiece or BPE according to these rules)\n",
    "\n",
    "For example, indents and keywords should become separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "\n",
    "- [Andrej Karpathy - Tokenization :(](https://youtu.be/zduSFxRajkE?si=f40nDHXAPYIKWGlS)\n",
    "- [Andrej Karpathy - Tokenization :( Colab](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L)\n",
    "- [Andrej Karpathy - minbpe](https://github.com/karpathy/minbpe)\n",
    "- [HF playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)\n",
    "- [Exploring BERT's Vocabulary](https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)\n",
    "- [A Programmer's Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
    "- [Huggingface: Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "- [Patel Arasanipalai - Applied Natural Language Processing in the Enterprise](https://www.oreilly.com/library/view/applied-natural-language/9781492062561/ch04.html)\n",
    "- [Willison - Understanding GPT tokenizers](https://simonwillison.net/2023/Jun/8/gpt-tokenizers/)\n",
    "- [Millidge - Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "- [gpt-tokenizer](https://github.com/niieani/gpt-tokenizer)\n",
    "- [Free GPT Tokenizer](https://koala.sh/tools/free-gpt-tokenizer)\n",
    "- [Grudzie≈Ñ - GPT Tokens Explained: what they are and how they work](https://www.quickchat.ai/post/tokens-entropy-question)\n",
    "- [Rumbelow - SolidGoldMagikarp (plus, prompt generation)](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)\n",
    "- [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
