{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. SEQ2SEQ\n",
    "\n",
    "1. Seq2seq models\n",
    "2. Tokenization: text\n",
    "3. Tokenization: code\n",
    "4. Exercise\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seq2seq models\n",
    "\n",
    "Paper: [Sutskever et al - Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "- English to French translation\n",
    "- two recurrent neural networks work together to transform one sequence to another\n",
    "- an encoder network condenses an input sequence into a vector\n",
    "- a decoder network eats that vector to produce an output sequence\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "Consider the sentence *\"Je ne suis pas le chat noir\"* --- *\"I am not the black cat\"*. Most of the words in the input sentence have a direct translation in the output sentence, but are in slightly *different* orders, e.g. chat noir and black cat. Because of the *ne/pas* construction there is also one more word in the input sentence. It would be difficult to produce a correct translation directly from the sequence of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the \"meaning\" of the input sequence into a single vector --- a single point in some N dimensional space of sentences.\n",
    "\n",
    "More details: [Robertson - NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- takes input\n",
    "- capture essential information\n",
    "- creates hidden states\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- takes hidden states\n",
    "- generates the output sequence\n",
    "- operates in an autoregressive manner, producing one element of the output sequence at a time. At each step, it considers the previously generated elements, hidden states, and the input to make predictions for the next element in the output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "\n",
    "![Alammar - The Illustrated Transformer](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "\n",
    "More:\n",
    "- [Alammar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Voita - Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [pytorch.org - Language Modeling with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#language-modeling-with-nn-transformer-and-torchtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization: text\n",
    "\n",
    "- Word-level tokenization\n",
    "- Char-level tokenization\n",
    "- Subword tokenization\n",
    "    - [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE)\n",
    "    - [Sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization $\\rightarrow$ Vocabulary $\\rightarrow$ Embeddings\n",
    "\n",
    "![](./res/03_vocabulary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors'\n",
    "words = text.split(' ')\n",
    "tokens = {v: k for k, v in enumerate(words)}\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks?\n",
    "\n",
    "- huge vocabulary\n",
    "- Out Of Vocabulary (OOV): new words which are encountered at testing (UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {v: k for k, v in enumerate(text)}\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simple\n",
    "- solve the OOV problem\n",
    "- makes it much harder for the model to learn meaningful input representations: \"a\" vs \"apple\" --- loss of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subword tokenization\n",
    "\n",
    "![](https://www.oreilly.com/api/v2/epubs/9781492062561/files/assets/anlp_0401.png)\n",
    "\n",
    "- Increase the amount of information per token.\n",
    "- Decrease the total number of tokens (vocabulary size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenizations:\n",
    "\n",
    "- Byte pair encoding (BPE). See R. Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units,” arXiv, 2015, https://arxiv.org/abs/1508.07909\n",
    "- WordPiece. See M. Schuster and K. Nakajima, “Japanese and Korean Voice Search,” International Conference on Acoustics, Speech and Signal Processing, IEEE (2012), https://research.google/pubs/japanese-and-korean-voice-search/\n",
    "- SentencePiece. See T. Kudo and J. Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing,” arXiv, 2018, https://arxiv.org/abs/1808.06226\n",
    "\n",
    "Reference:\n",
    "- https://www.oreilly.com/library/view/applied-natural-language/9781492062561/ch04.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of tokenization: [tiktokenizer.vercel.app](https://tiktokenizer.vercel.app/)\n",
    "\n",
    "GPT-4o, English\n",
    "![](res/03_to_be_or_not_to_be_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o, Russian\n",
    "![](res/03_to_be_or_not_to_be_ru.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3.5, Russian\n",
    "![](res/03_to_be_or_not_to_be_ru_3.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE --- Byte pair encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "code= \"\"\"\\\n",
    "def bubbleSort(arr):\n",
    "    n = len(arr)\n",
    "    # optimize code, so if the array is already sorted, it doesn't need\n",
    "    # to go through the entire process\n",
    "    # Traverse through all array elements\n",
    "    for i in range(n-1):\n",
    "        swapped = False\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j + 1]:\n",
    "                swapped = True\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "\n",
    "        if not swapped:\n",
    "            return\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(code.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Original BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "\n",
    "Suppose the data to be encoded is\n",
    "\n",
    "> aaabdaaabac\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\".\n",
    "Now there is the following data and replacement table:\n",
    "\n",
    "> ZabdZabac\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n",
    "\n",
    "> ZYdZYac\n",
    "> \n",
    "> Y=ab\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n",
    "\n",
    "> XdXac\n",
    "> \n",
    "> X=ZY\n",
    "> \n",
    "> Y=ab\n",
    "> \n",
    "> Z=aa\n",
    "\n",
    "This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\n",
    "\n",
    "To decompress the data, simply perform the replacements in the reverse order. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Source: https://gist.github.com/bigsnarfdude/8e99709d5c3d9d58b3831221fcbdaf68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print('length:', len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print('length:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forced splits (pre-tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 tokenizer\n",
    "![](./res/03_gpt2.png)\n",
    "\n",
    "GPT-4 tokenizer\n",
    "![](./res/03_cl100kbase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special symbols\n",
    "\n",
    "Special tokens are additional tokens added during the tokenization process to serve specific purposes in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the [code points](https://en.wikipedia.org/wiki/Code_point) and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: [Andrej Karpathy - Tokenization :(](https://youtu.be/zduSFxRajkE?si=f40nDHXAPYIKWGlS)\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open('toy.txt', 'w', encoding='utf-8') as f:\n",
    "  f.write('SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input='toy.txt',\n",
    "  input_format='text',\n",
    "  # output spec\n",
    "  model_prefix='tok400', # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type='bpe',\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name='identity', # turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integers in GPT2\n",
    "\n",
    "![](https://www.beren.io/assets/figures/number_tokenization_weirdness_gpt2.png)\n",
    "\n",
    "- each row here represents 100 integers\n",
    "- if a square is colored yellow it means a unique token is assigned to that integer\n",
    "- if it is blue then the integer is coded by a composite set of tokens\n",
    "\n",
    "![](https://www.beren.io/assets/figures/gpt2_number_composition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huggingface\n",
    "\n",
    "- [Tokenizers.Quicktour](https://huggingface.co/docs/tokenizers/en/quicktour)\n",
    "- [Building a tokenizer, block by block](https://huggingface.co/learn/nlp-course/en/chapter6/8)\n",
    "- [Summary of the tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "\n",
    "encoded_input = tokenizer('Do not meddle in the affairs of wizards, for they are subtle and quick to anger.')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary with three important items:\n",
    "- input_ids are the indices corresponding to each token in the sentence.\n",
    "- attention_mask indicates whether a token should be attended to or not.\n",
    "- token_type_ids identifies which sequence a token belongs to when there is more than one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama\n",
    "\n",
    "- https://arxiv.org/abs/2302.13971\n",
    "- BPE algorithm (Sennrich et al., 2015), using the implementation from SentencePiece (Kudo and Richardson, 2018)\n",
    "- split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama 2\n",
    "\n",
    "- https://arxiv.org/abs/2307.09288\n",
    "- the same tokenizer as Llama 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization: code\n",
    "\n",
    "- https://pypi.org/project/code-tokenize/\n",
    "- [CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code](https://arxiv.org/abs/2308.00683)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InCoder\n",
    "\n",
    "- https://arxiv.org/abs/2204.05999\n",
    "- a byte-level BPE tokenizer Sennrich et al. (2016); Radford et al. (2019)\n",
    "- allow tokens to extend across whitespace (excluding newline characters) so that common code idioms (e.g., import numpy as np) are represented as single tokens in the vocabulary.\n",
    "\n",
    "This substantially improves the tokenizer’s efficiency reducing the total number of tokens required to encode our training corpus by 45% relative to the byte-level BPE tokenizer and vocabulary of GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VulBERTa\n",
    "\n",
    "- https://arxiv.org/abs/2205.12424\n",
    "- remove comments from the source code of each function using several regular expressions\n",
    "- parse the source code\n",
    "- the tokens produced by parser are further processed by the BPE algorithm, modified to take\n",
    "into account the pre-defined tokens\n",
    "- vocabulary size  is 50000\n",
    "- pre-defined 451 tokens are tokens that explicitly included in the vocabulary\n",
    "\n",
    "![](./res/03_vulberta_pre-defined_tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "\n",
    "- https://arxiv.org/abs/2308.12950\n",
    "- data is tokenized via BPE (Sennrich et al. (2016)), employing the same tokenizer as Llama and Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Star Coder\n",
    "\n",
    "- https://arxiv.org/abs/2305.06161\n",
    "- Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens, including the sentinel tokens\n",
    "- the pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeek Coder\n",
    "\n",
    "- https://arxiv.org/abs/2401.14196\n",
    "- HuggingFace Tokenizer library to train BPE tokenizers\n",
    "- vocabulary size is 32,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Star Coder 2\n",
    "\n",
    "- https://arxiv.org/abs/2402.19173\n",
    "- follow the procedure of StarCoderBase and train a byte-level Byte-Pair-Encoding tokenizer\n",
    "- vocabulary size is 49,152 tokens, including the sentinel tokens\n",
    "- The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems caused by tokenization:\n",
    "\n",
    "- problems related to individual symbols (count the number of symbols, write in reverse)\n",
    "- arithmetic problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exercise\n",
    "\n",
    "Let's assume that we have a snippet of Python code (3.11) as a string as input.\n",
    "You need to:\n",
    "1. define the rules for splitting the string into substrings for tokenization\n",
    "2. implement these rules in Python\n",
    "3. write a program that tokenizes (Sentencepiece or BPE according to these rules)\n",
    "\n",
    "For example, indents and keywords should become separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "\n",
    "- [Andrej Karpathy - Tokenization :(](https://youtu.be/zduSFxRajkE?si=f40nDHXAPYIKWGlS)\n",
    "- [Andrej Karpathy - Tokenization :( Colab](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L)\n",
    "- [Andrej Karpathy - minbpe](https://github.com/karpathy/minbpe)\n",
    "- [HF playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)\n",
    "- [Exploring BERT's Vocabulary](https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)\n",
    "- [A Programmer's Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
    "- [Huggingface: Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "- [Patel Arasanipalai - Applied Natural Language Processing in the Enterprise](https://www.oreilly.com/library/view/applied-natural-language/9781492062561/ch04.html)\n",
    "- [Willison - Understanding GPT tokenizers](https://simonwillison.net/2023/Jun/8/gpt-tokenizers/)\n",
    "- [Millidge - Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "- [gpt-tokenizer](https://github.com/niieani/gpt-tokenizer)\n",
    "- [Free GPT Tokenizer](https://koala.sh/tools/free-gpt-tokenizer)\n",
    "- [Grudzień - GPT Tokens Explained: what they are and how they work](https://www.quickchat.ai/post/tokens-entropy-question)\n",
    "- [Rumbelow - SolidGoldMagikarp (plus, prompt generation)](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)\n",
    "- [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
