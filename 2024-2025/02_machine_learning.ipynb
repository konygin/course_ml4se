{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. MACHINE LEARNING\n",
    "\n",
    "1. Introduction\n",
    "2. Gradient descent\n",
    "3. Neural networks\n",
    "4. Backpropagation\n",
    "5. Exercise\n",
    "6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to machine learning\n",
    "\n",
    "- $X$ is a set of objects\n",
    "- $Y$ is a set of answers\n",
    "- $y^∗: X \\rightarrow Y$ is an objective function, the values of which $y_i = y^∗(x_i)$ are known only on a finite subset of objects $\\{ x_1, ..., x_l\\} \\subseteq X$\n",
    " \n",
    "Set of pairs $X_l = \\{(x_i, y_i) | i \\in \\{1, ..., l\\} \\}$ is called *the training set*.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "It is necessary to restore the function $y^∗$ from the set $X_l$.\n",
    "\n",
    "That is, it is necessary to construct a function $y: X \\rightarrow Y$ that would approximate the objective function $y^∗(x)$, not only on the objects of the training set, but also on the entire set $X$.\n",
    "\n",
    "![](./res/02_learning_scheme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "How to construct a function $y: X \\rightarrow Y$?\n",
    "\n",
    "1. Let's define an auxiliary loss function --- this is a non-negative function $L(y, x)$, characterizing the error value of the function $y$ on the object $x$. If $L(y, x) = 0$, then the answer $y$ is called *correct*.\n",
    "2. Depending on the value of the loss function, we will correct the function $y$.\n",
    "\n",
    "The correction assumes:\n",
    "1. there is some set $A$ of functions\n",
    "2. the functions from $A$ are parameterized: $A = \\{f_a | a \\in D\\}$, $D$ is a set of admissible parameters\n",
    "3. there is some rule that selects $f_a$ depending on the value of the loss function. Ideally, we want to select such $f_a \\in A$ that $L(f_a, x)$ takes minimal values for all $a \\in D.$\n",
    "\n",
    "Example: Linear regression, two-parameter family of functions (lines $y = kx + b$).\n",
    "\n",
    "![](./res/02_linear_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search for the function $y$ can be represented as a descent along the relief of the loss function:\n",
    "\n",
    "![https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png](https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png)\n",
    "\n",
    "Thus, we have an *optimization problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How will we know that the training was successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. Value of the loss function.\n",
    "2. Testing on hold-out dataset\n",
    "\n",
    "![](./res/02_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization methods\n",
    "\n",
    "What optimization methods do you know?\n",
    "\n",
    "- differentiable functions\n",
    "- continuous functions\n",
    "- discontinuous functions\n",
    "\n",
    "If we compare optimization of differentiable and discontinuous functions, which is easier and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient descent\n",
    "\n",
    "It is usually assumed that functions are differentiable. Then *gradient descent* can be used.\n",
    "\n",
    "$$\\frac{df}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) -f (x)}{h}$$\n",
    "\n",
    "![](./res/02_gradient_descent_1.jpeg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any disadvantages to gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./res/02_gradient_descent_2.jpeg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural networks\n",
    "\n",
    "![](./res/02_neural_network_human_cortex.png)\n",
    "\n",
    "[Cortex neurons (Ramón y Cajal)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single neuron model\n",
    "\n",
    "![](https://www.researchgate.net/publication/374679637/figure/fig2/AS:11431281206265945@1700629912758/Neural-network-architecture.png)\n",
    "\n",
    "or\n",
    "\n",
    "![](./res/02_neuron_math.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Neural network\n",
    "\n",
    "![](./res/02_neural_network_math.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The universality of neural networks\n",
    "\n",
    "Nonlinearity\n",
    "\n",
    "![](./res/02_nonlinearity.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Cybenko's theorem\n",
    "\n",
    "![](./res/02_cybenko_1.png)\n",
    "![](./res/02_cybenko_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### PyTorch\n",
    "\n",
    "[AlexNet](https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), scheme:\n",
    "\n",
    "![](./res/02_alexnet.png)\n",
    "\n",
    "AlexNet, code in PyTorch:\n",
    "\n",
    "![](./res/02_alexnet_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backpropagation\n",
    "\n",
    "#### How to optimize\n",
    "\n",
    "Suppose we want to optimize a loss function using gradient descent.\n",
    "\n",
    "1. we have a loss function $L$ that we want to minimize\n",
    "2. we have model parameters that we can change\n",
    "3. if we take the parameter $w$, then $\\frac{dL}{dw}$ contains information about what change in the value of the loss function $L$ will result from a small change in the parameter $w$\n",
    "\n",
    "Suppose $y = g(x)$ and $z = f (g(x)) = f (y)$.\n",
    "Then $$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "Let's take a slightly more complex situation: $x = f(w)$, $y = f(x)$, $z = f(y)$.\n",
    "Then\n",
    "$$\\frac{dz}{dw} = \\frac{dz}{dy} \\frac{dy}{dx} \\frac{dx}{dw} =$$\n",
    "$$= f'(y) f'(x) f'(w) =$$\n",
    "$$= f'(f(f(w))) f'(f(w)) f'(w).$$\n",
    "\n",
    "The expression $f(w)$ occurs more than once. We can:\n",
    "1. calculate, save and use\n",
    "2. calculate each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "How can this problem be solved? --- Backpropagation algorithm.\n",
    "\n",
    "The method was first described in 1974 by A. I. Galushkin, and also independently and simultaneously by Paul J. Werbos\n",
    "\n",
    "![](./res/02_learning_process.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [karpathy/micrograd](https://github.com/karpathy/micrograd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!apt-get install -y graphviz\n",
    "!pip install graphviz\n",
    "!pip install micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{data %.2f | grad %.2f}\" % (n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"770pt\" height=\"128pt\"\n",
       " viewBox=\"0.00 0.00 770.00 128.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 124)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-124 766,-124 766,4 -4,4\"/>\n",
       "<!-- 140303276188832 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140303276188832</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-83.5 0,-119.5 170,-119.5 170,-83.5 0,-83.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"42\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"84,-83.5 84,-119.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 2.00</text>\n",
       "</g>\n",
       "<!-- 140303276194544* -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140303276194544*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"233\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"233\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140303276188832&#45;&gt;140303276194544* -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140303276188832&#45;&gt;140303276194544*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.08,-85.37C179.44,-83.58 188.55,-81.83 196.78,-80.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-83.68 206.68,-78.36 196.2,-76.8 197.52,-83.68\"/>\n",
       "</g>\n",
       "<!-- 140303276194544 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140303276194544</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-55.5 296,-91.5 466,-91.5 466,-55.5 296,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"338\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"380,-55.5 380,-91.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140303276190656+ -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140303276190656+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"529\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"529\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 140303276194544&#45;&gt;140303276190656+ -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140303276194544&#45;&gt;140303276190656+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466.08,-57.37C475.44,-55.58 484.55,-53.83 492.78,-52.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"493.52,-55.68 502.68,-50.36 492.2,-48.8 493.52,-55.68\"/>\n",
       "</g>\n",
       "<!-- 140303276194544*&#45;&gt;140303276194544 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140303276194544*&#45;&gt;140303276194544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M260.1,-73.5C267.69,-73.5 276.45,-73.5 285.72,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"285.93,-77 295.93,-73.5 285.93,-70 285.93,-77\"/>\n",
       "</g>\n",
       "<!-- 140303276194736 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140303276194736</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-28.5 0,-64.5 170,-64.5 170,-28.5 0,-28.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"42\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"84,-28.5 84,-64.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140303276194736&#45;&gt;140303276194544* -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140303276194736&#45;&gt;140303276194544*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.08,-62.05C179.44,-63.78 188.55,-65.47 196.78,-66.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.21,-70.44 206.68,-68.82 197.48,-63.56 196.21,-70.44\"/>\n",
       "</g>\n",
       "<!-- 140303276190656 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140303276190656</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"592,-27.5 592,-63.5 762,-63.5 762,-27.5 592,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"634\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 3.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"676,-27.5 676,-63.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"719\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140303276190656+&#45;&gt;140303276190656 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140303276190656+&#45;&gt;140303276190656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M556.1,-45.5C563.69,-45.5 572.45,-45.5 581.72,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"581.93,-49 591.93,-45.5 581.93,-42 581.93,-49\"/>\n",
       "</g>\n",
       "<!-- 140303276181488 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140303276181488</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-0.5 296,-36.5 466,-36.5 466,-0.5 296,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"338\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"380,-0.5 380,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140303276181488&#45;&gt;140303276190656+ -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140303276181488&#45;&gt;140303276190656+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466.08,-34.05C475.44,-35.78 484.55,-37.47 492.78,-38.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"492.21,-42.44 502.68,-40.82 493.48,-35.56 492.21,-42.44\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9ae6f02b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Value(1.0); x.label = 'x'\n",
    "#y = (x * 2 + 1).relu()\n",
    "y = x * 2 + 1\n",
    "y.backward()\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py\n",
    "\n",
    "![](./res/02_micrograd_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "![](./res/02_computation_graph_pytorch_1.jpg)\n",
    "\n",
    "`loss.backward()`\n",
    "\n",
    "![](./res/02_computation_graph_pytorch_2.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercise\n",
    "\n",
    "Implement the operation:\n",
    "\n",
    "$\\tanh(x)$ --- hyperbolic tangent\n",
    "\n",
    "as a method of the [Value](https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/engine.py#L2) class. For testing, you need to build a computational graph, run backpropagation and draw it ([example](https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb)).\n",
    "\n",
    "Example for the addition operation: https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/engine.py#L13.\n",
    "Additional [explanations](https://www.youtube.com/watch?v=VMj-3S1tku0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. References\n",
    "\n",
    "1. [Goodfellow et al - Deep learning](https://www.deeplearningbook.org/)\n",
    "2. [Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools](https://www.manning.com/books/deep-learning-with-pytorch)\n",
    "3. [Karpathy - Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/tree/master)\n",
    "4. [Zhang et al - Dive into Deep Learning](https://d2l.ai/)\n",
    "5. [Karpathy - The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "6. [Clarkson - LLM from scratch: Automatic Differentiation](https://bclarkson-code.com/posts/llm-from-scratch-scalar-autograd/post.html)\n",
    "7. [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "8. [Ramón y Cajal and the Case for Drawing in Science](https://www.scientificamerican.com/blog/sa-visual/ramon-y-cajal-and-the-case-for-drawing-in-science2/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
