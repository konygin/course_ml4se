{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intro\n",
    "2. Prompting\n",
    "3. PEFT\n",
    "4. Alignment\n",
    "5. LLMs for Code\n",
    "6. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers ([lena-voita.github.io](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)):\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCALING LAWS\n",
    "\n",
    "[Kaplan et al - Scaling Laws for Neural Language Models 2020](https://arxiv.org/abs/2001.08361):\n",
    "\n",
    "Model performance depends most strongly on scale, which consists of three factors: the amount of compute used for training, the size of the dataset, and the number of model parameters (excluding embeddings). Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width.\n",
    "\n",
    "![](res/05_scaling_laws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LARGE MODELS\n",
    "\n",
    "![](./res/07_model_size_in_tokens.png)\n",
    "\n",
    "\n",
    "[Source: [scale.com/guides/large-language-models](https://scale.com/guides/large-language-models#model-size-and-performance)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONTEXT SIZE\n",
    "\n",
    "![](./res/07_gpt-4_context_length.png)\n",
    "\n",
    "[Source: [lifearchitect.ai/gpt-4](https://lifearchitect.ai/gpt-4/)]\n",
    "\n",
    "Claude 3.5 --- 200K context tokens\n",
    "- about 150K words\n",
    "- hundreds of pages of text\n",
    "- a couple of books (The Great Gatsby about 72K tokens)\n",
    "- text that would take about 10 hours to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HARD TASKS FOR LLMS\n",
    "\n",
    "There are examples of complex problems that some LLMs solve well.\n",
    "\n",
    "![](res/07_hard_task_gpt4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are examples of simple problems that LLMs do poorly.\n",
    "\n",
    "![](./res/07_llm_hard_gpt4o_calculator.png)\n",
    "\n",
    "![](./res/07_llm_hard_gpt4o_alice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MORE\n",
    "- [Dziri et al - Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/abs/2305.18654)\n",
    "- [Bubeck et al - Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)\n",
    "- [Nezhurina et al - Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models](https://arxiv.org/abs/2406.02061)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IN-CONTEXT LEARNING\n",
    "\n",
    "In-context learning (ICL) is a technique where task demonstrations are integrated into the prompt in a natural language format.\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)\n",
    "\n",
    "- 0-shot\n",
    "- 1-shot\n",
    "- few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSTRUCTIONS\n",
    "\n",
    "- LM just predicts the next token given the previous tokens\n",
    "- One core capability of Large Language Models (LLMs) is to follow natural language instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAIN-OF-THOUGHT\n",
    "\n",
    "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MORE\n",
    "\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)\n",
    "- [Microsoft: The power of prompting](https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/)\n",
    "- [Yandex: Тетрадь с чит-промптами](https://ya.ru/project/cheat-prompts/index)\n",
    "- [Antropic: Prompt engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TAXONOMY\n",
    "\n",
    "![](res/07_peft_taxonomy.png)\n",
    "\n",
    "[Source: [Lialin et al 2023](https://arxiv.org/abs/2303.15647)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "\n",
    "![](./res/07_lora.png)\n",
    "\n",
    "[Source: [sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MORE\n",
    "\n",
    "- [Lialin et al - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "- [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [Practical tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Alignment\n",
    "\n",
    "![](res/07_alignment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLHF\n",
    "\n",
    "![](2022-2023/res_nlp/rlhf_overview.png)\n",
    "\n",
    "More:\n",
    "- [Stiennon et al - Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### MORE\n",
    "\n",
    "- [Zhou et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206): SFT on carefully selected examples (1000), without using RL\n",
    "- [Rafailov et al - Direct Preference Optimization](https://arxiv.org/abs/2305.18290)\n",
    "- [Constitutional AI](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)\n",
    "- [Zhout et al - LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)\n",
    "- [Rafailov et al - Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)\n",
    "- https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx\n",
    "- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "- https://github.com/opendilab/awesome-RLHF\n",
    "- https://rail.eecs.berkeley.edu/deeprlcourse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMs for Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**model**     | **team** |**year** | **context** | **terms**   | **reference**|\n",
    "|--------------|----------|---------|-------------|-------------|--------------|\n",
    "| GPT-4        | OpenAI   | 2023    | 32K         | [pricing](https://openai.com/pricing)|[gpt-4](https://openai.com/gpt-4) |\n",
    "| codellama-7b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf) |\n",
    "| codellama-13b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf) |\n",
    "| codellama-34b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |\n",
    "| codellama-70b-instruct-hf| meta | 2023 | 16K     |[terms](https://github.com/facebookresearch/llama/blob/main/LICENSE)|[codellama/CodeLlama-70b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf) |\n",
    "| deepseek-coder-33b-instruct | deepseek | 2024 | 16K | [terms](https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL) | [deepseek-ai/deepseek-coder-33b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct)|\n",
    "| codegemma-2b | google   | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-2b](https://huggingface.co/google/codegemma-2b)|\n",
    "| codegemma-7b | google   | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-7b](https://huggingface.co/google/codegemma-7b)|\n",
    "| codegemma-7b-it| google | 2024    | 8K          |[terms](https://ai.google.dev/gemma/terms)|[google/codegemma-7b-it](https://huggingface.co/google/codegemma-7b-it)|\n",
    "|              |          |         |                          |                    |             |              |\n",
    "|              |          |         |                          |                    |             |              |\n",
    "\n",
    "#### MORE\n",
    "\n",
    "- https://github.com/huybery/Awesome-Code-LLM\n",
    "- [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)\n",
    "- [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)\n",
    "- [CodeGemma: Open Code Models Based on Gemma](https://goo.gle/codegemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercise\n",
    "\n",
    "It is necessary to conduct a mini-study on the applicability of LLMs for the task of clone detection.\n",
    "\n",
    "Possible plan:\n",
    "1. choose any open model (codellama, codegemma etc.)\n",
    "2. choose any dataset for clones, or part of it, or come up with a small number of examples yourself\n",
    "3. select a prompt\n",
    "4. get the model's responses\n",
    "5. transform the model's responses into labels (in any way: classification, regular expressions, manually...)\n",
    "6. calculate metrics, make conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
