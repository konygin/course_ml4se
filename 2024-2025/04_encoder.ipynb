{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. ENCODER\n",
    "\n",
    "1. Seq2seq\n",
    "2. Transformer\n",
    "3. Encoder\n",
    "4. Attention\n",
    "5. FFN\n",
    "6. Embeddings and Softmax\n",
    "7. Positional Encodings\n",
    "8. Exercise\n",
    "9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq is a family of approaches for sequence transformation problems (e.g. NLP, ASR, ML4SE).\n",
    "\n",
    "Currently, the most common approaches are Transformer-based approaches (encoder-decoder, decoder, or encoder).\n",
    "\n",
    "An encoder-decoder model consists of:\n",
    "- an encoder and\n",
    "- a decoder.\n",
    "\n",
    "The *encoder* maps an input sequence $(x_1, ..., x_n)$ to a sequence of continuous representations $z = (z_1, ..., z_n)$.\n",
    "\n",
    "The *decoder* maps $z$ to an output sequence $(y_1, ..., y_m)$ one element at a time.\n",
    "\n",
    "At each step, the model is *autoregressive*, receiving previously generated elements (tokens) as additional input when generating the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2155d4f3d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, encoder-decoder architectures based on [RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) were used. In 2017, the Transformer architecture was proposed in the work [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Google Brain).\n",
    "\n",
    "![paper](res/04_attention_is_all_you_need.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer consists of an encoder and a decoder, each of which consists of layers [lena-voita.github.io]:\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Jay Allamar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Brandon Rohrer - Transformers from Scratch](https://e2eml.school/transformers.html)\n",
    "- [Peter Bloem - Transformers from Scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of $N$ (e.g.,  $N = 6$) identical consecutive layers.\n",
    "Each layer has two sublayers:\n",
    "- multi-head self-attention mechanism, and\n",
    "- position-wise fully connected feed-forward network.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/Transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequence is tokenized. Each token corresponds to a vector (embedding).\n",
    "\n",
    "![](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the architecture uses a *residual connection* for each of the two sublayers, followed by [layer normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "More formally, the output of each sublayer is `LayerNorm(x + Sublayer(x))`, where `Sublayer(x)` is a function implemented by the sublayer itself.\n",
    "\n",
    "![](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure_W640.jpg)\n",
    "\n",
    "All sublayers in the model and output embeddings have the same dimension $d_{model}$ (e.g. $d_{model} = 512$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Ulf Mertens - The Transformer encoder](https://github.com/mertensu/transformer-tutorial/blob/master/transformer_encoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention can be described as a function mapping a *query* and a set of *key-value* pairs to some output.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
    "\n",
    "The query, keys, values, and output are vectors.\n",
    "The output vector is computed as a weighted sum of values, where the weight assigned to each value is computed over the query-key pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the calculations are done in matrix form (simultaneously for a set of queries, packed together into a matrix $Q$). The keys and values ​​are also packed together into matrices $K$ and $V$.\n",
    "\n",
    "Thus,\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ allows us to reduce the value of large scalar products.\n",
    "(If they are large, then the softmax function has very small gradients. This is bad.)\n",
    "\n",
    "In matrix notation:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention allows you to run several attention mechanisms, each of which can have its own useful features.\n",
    "This way, information is collected from different views.\n",
    "\n",
    "![](https://i.ytimg.com/vi/mMa2PmYJlCo/mqdefault.jpg)\n",
    "\n",
    "Therefore, using linear transformations (projections), we obtain several ($h$) query-key-value triplets (dimensions, respectively, $d_k$, $d_k$, $d_v$).\n",
    "\n",
    "Then, for each of these triplets, an attention mechanism is launched in parallel, which yields output values ​​of dimension $d_v$.\n",
    "Then the obtained output values ​​are concatenated and projected again.\n",
    "\n",
    "![](https://repository-images.githubusercontent.com/283979760/0f00ed80-d368-11ea-979d-78033d0a1cee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally:\n",
    "\n",
    "$$MultiHead(Q, K, V ) = concat(head_1, ..., head_h)W^O$$\n",
    "\n",
    "where $head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)$ and the following projection matrices are used\n",
    "\n",
    "$W^Q_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^K_i \\in R^{d_{model} \\times d_k}$,\n",
    "\n",
    "$W^V_i \\in R^{d_{model} \\times d_v}$,\n",
    "\n",
    "$W^O_i \\in R^{hd_v \\times d_{model}}$.\n",
    "\n",
    "The dimensions can be, for example, $h = 8$, $d_k = d_v = \\frac{d_{model}}{h} = 64$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer uses multi-head attention in three different ways:\n",
    "\n",
    "1. In the encoder-decoder attention layers, the *queries* come from the previous decoder layer, and the *keys* and *values* come from the encoder output. This allows each position in the decoder to visit all positions in the input sequence.\n",
    "\n",
    "2. The encoder contains self-attention layers.\n",
    "Here, all *keys*, *values*, and *queries* come from the same place --- from the output of the previous encoder layer. Each position in the encoder can visit all positions in the previous encoder layer.\n",
    "\n",
    "3. The decoder contains self-attention layers.\n",
    "Here, each position in the decoder is allowed to visit all previous positions (from the left) up to and including the current position.\n",
    "This restriction arises because it is necessary to preserve the autoregressive property of the decoder (predict the next token based on the previous tokens).\n",
    "The constraint is implemented inside scaled dot-product attention by maximizing (assuming $-\\infty$) all values ​​at the softmax input that correspond to invalid visits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the encoder and decoder layers contains a FFN. This sublayer is applied equally to each position separately.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
    "It consists of two linear transforms with a [ReLU](https://arxiv.org/abs/1803.08375) activation between them.\n",
    "\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Although the linear transforms are the same for different positions,\n",
    "they use different parameters from layer to layer.\n",
    "\n",
    "The inner layer has a higher dimensionality.\n",
    "For example, the input and output dimensionality is $d_{model} = 512$, the inner layer dimensionality is $d_{ff} = 2048$.\n",
    "\n",
    "In particular, we have that in the Transformer, the token in each position goes its own path in the encoder.\n",
    "There are dependencies at the level of self-attention layers, but no dependencies at the FFN level.\n",
    "This allows FFN sublayers to be passed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Embeddings and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform input/output tokens into vectors (of dimension $d_{model}$), learnable embeddings are used.\n",
    "\n",
    "To transform outputs into predicted probabilities of the next token, learnable linear transformation and softmax are used.\n",
    "\n",
    "In a bit more detail, the output vector is passed through a linear layer (a fully connected neural network), followed by softmax.\n",
    "\n",
    "The linear layer projects the output into a much larger vector (a logits vector).\n",
    "The size of this vector is the size of the token dictionary.\n",
    "\n",
    "Then the softmax layer turns the coordinates into probabilities.\n",
    "The most probable token is selected.\n",
    "\n",
    "![](./res/04_transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the information about the order of tokens in the sequence, the Transformer uses positional encoding. Positional encoding is a vector that encodes the token's ordinal number. It has the same length as the embeddings ($d_{model}$), and is added to the embedding at the input.\n",
    "\n",
    "For example, you can use the following functions:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "where $pos$ is the position index, and $i$ is the coordinate index.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options:\n",
    "- [learnable](https://aclanthology.org/2022.findings-aacl.42.pdf)\n",
    "- [relative](https://arxiv.org/abs/1803.02155)\n",
    "- [RoPE](https://arxiv.org/abs/2104.09864)\n",
    "\n",
    "#### More\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [LongRoPE](https://arxiv.org/abs/2402.13753)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Batches and Paddings\n",
    "\n",
    "![](./res/04_padding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Encoder from scratch in PyTorch. Be prepared to answer questions about the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. References\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [nn.labml.ai - Paper Implementations](https://nn.labml.ai/)\n",
    "- [Post LayerNorm - Pre LayerNorm](https://arxiv.org/abs/2002.04745)\n",
    "- [LayerNorm - RMSNorm](https://arxiv.org/abs/1910.07467)\n",
    "- [Attention modifications](https://arxiv.org/abs/2305.13245)\n",
    "- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)\n",
    "- [pytorch - NLP from Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
