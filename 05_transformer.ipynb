{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.  TRANSFORMER\n",
    "\n",
    "1. Intro\n",
    "2. Decoder\n",
    "3. Transformer\n",
    "4. Learning\n",
    "5. Tasks\n",
    "6. LLMs\n",
    "7. Exercise\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq --- это семейство подходов для задач преобразования последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcc8d4a83d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2017 в работе [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762) была предложена архитектура Трансформер (Google Brain).\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Jay Allamar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Brandon Rohrer - Transformers from Scratch](https://e2eml.school/transformers.html)\n",
    "- [Peter Bloem - Transformers from Scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decoder (GPT)\n",
    "\n",
    "Общая схема работы декодера:\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/gpt2-self-attention-example-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подслой self-attention в декодере модифицирован:\n",
    "- добавлены маски для предотвращения посещения последующих позиций.\n",
    "\n",
    "![](res/05_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Self-Attention\n",
    "\n",
    "Сравнение self-attention и masked self-attention:\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое маскирование гарантирует, что прогнозы для позиции $i$ могут зависеть только от известных выходов в позициях, меньших $i$. Используется треугольная маска:\n",
    "\n",
    "![](https://peterbloem.nl/files/transformers/masked-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation\n",
    "\n",
    "Для преобразования выходов декодеров в предсказанные вероятности следующего токена используется обучаемое линейное преобразование и softmax.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)\n",
    "\n",
    "На выходе декодера мы получаем вектор.\n",
    "Этот вектор проходит через линейный слой (fully connected neural network), за которым следует softmax.\n",
    "\n",
    "Линейный слой проецирует выход декодера в гораздо больший вектор (logits-вектор).\n",
    "Размер этого вектор --- это размер словаря токенов.\n",
    "\n",
    "Затем слой softmax превращает координаты в вероятности.\n",
    "Выбирается наиболее вероятный токен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Словарь и logits\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/gpt2-output-scores-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregression\n",
    "\n",
    "![](https://habrastorage.org/getpro/habr/upload_files/80e/243/698/80e24369887bf050a35ece72a3e161b5.gif)\n",
    "\n",
    "[comment]: <> (http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy search\n",
    "\n",
    "![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam search\n",
    "\n",
    "![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More:\n",
    "\n",
    "- [nn.labml.ai - GPT](https://nn.labml.ai/transformers/gpt/index.html)\n",
    "- [Jay Alammar - The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)\n",
    "\n",
    "Декодер трансформера состоит из $N$ последовательных одинаковых слоёв (например, $N = 6$).\n",
    "Кроме аналогичных двух подслоёв добавляется третий подслой,\n",
    "- который реализует multi-head attention над выходами с энкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Развёрнутая схема\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа декодера\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа энкодера\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры:\n",
    "- GPT\n",
    "- PaLM\n",
    "\n",
    "#### More:\n",
    "- [Transformer Encoder and Decoder Models](https://nn.labml.ai/transformers/models.html)\n",
    "- [Patrick von Platen - How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- обучать с нуля (*from scratch*)\n",
    "- использовать модели, которые были обучены на других (но связанных с текущей) задачах (*transfer learning*)\n",
    "\n",
    "Если обучать модель полностью с нуля, то потребуется много рамеченных данных. Где взять?\n",
    "\n",
    "Идея:\n",
    "- давайте найдём подходящую промежуточную задачу со свойствами:\n",
    "    - связана с исходной задачей\n",
    "    - легко найти для неё много размеченных данных\n",
    "- обучимся на ней (*pretraining*)\n",
    "- дообучим модель (*finetuning*) на нашей задачи (*downstream*-задача), используя меньший датасет\n",
    "\n",
    "| обучение | тип задачи        | назначение          | таргеты        | разметка        |\n",
    "|----------|-------------------|---------------------|----------------|-----------------|\n",
    "| pretrain | pretrain-задача   | моделирование языка | синтетические  | self-supervised |\n",
    "| finetune | downstream-задача | полезная задача     | реальные       | supervised      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "Обучение модели с нуля промежуточной задаче. Особенности:\n",
    "- требуется много данных\n",
    "- дорого\n",
    "\n",
    "Выбор промежуточной задачи:\n",
    "- связь с конечной задачей, например, если NLP, то в качестве такой задачи подойдёт *задача моделирования языка*\n",
    "- наличие большого количества размеченных данных.\n",
    "\n",
    "Разметка с помощью экспертов:\n",
    "- долго,\n",
    "- дорого.\n",
    "\n",
    "Поэтому на этом этапе используется автоматическая разметка (self-supervised), т.е. разметка, которую можно получить без участия человека:\n",
    "- быстро,\n",
    "- дёшево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal language modeling (CLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg)\n",
    "\n",
    "- слева направо\n",
    "- авторегрессионно\n",
    "- GPT (decoder)\n",
    "- Prefix language modeling (PrefixLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked language modeling (MLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg)\n",
    "\n",
    "- BERT (encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span corruption\n",
    "\n",
    "![](https://user-images.githubusercontent.com/6536835/116129345-4b306700-a6ca-11eb-9acd-a14aa2b8d115.png)\n",
    "\n",
    "- T5 (encoder-decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UL2\n",
    "\n",
    "- R-denoising (regular denoising): standard span corruption objective --- short spans & low corruption\n",
    "- S-denoising (sequential denoising): strictly follows sequence order --- sequential denoising / prefix language modeling\n",
    "- X-denoising (extreme denoising): extreme span lengths and corruption rates\n",
    "\n",
    "![](res/05_denoising.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Tay et al - UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning (SFT)\n",
    "\n",
    "\n",
    "### Датасеты / бенчмарки\n",
    "\n",
    "- SQuAD ---  Stanford Question Answering Dataset\n",
    "- CoQA --- Conversational Question Answering\n",
    "- (Super) GLUE --- General Language Understanding Evaluation benchmark\n",
    "    - CoLA --- Corpus of Linguistic Acceptability\n",
    "    - SST --- Stanford Sentiment Treebank\n",
    "    - MRPC --- Microsoft Research Paraphrase Corpus\n",
    "    - QQP --- Quora Question Pairs\n",
    "    - MultiNLI --- Multi-Genre Natural Language Inference Corpus\n",
    "    - RTE --- Recognizing Textual Entailment\n",
    "    - etc\n",
    "\n",
    "### Метрики\n",
    "\n",
    "- Exact match (EM)\n",
    "- F1 score\n",
    "- Perplexity\n",
    "- BLEU\n",
    "\n",
    "Подробнее:\n",
    "- [Resources and Benchmarks for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tasks\n",
    "\n",
    "Natural language processing (обработка естественного языка) --- различные задачи, связанные с обработкой текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis\n",
    "\n",
    "Анализ тональности текста, выявление эмоциональной окраски текста.\n",
    "\n",
    "![](res/05_sentiment_analysis.png)\n",
    "\n",
    "Например:\n",
    "- [Pascual - Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification\n",
    "\n",
    "Классификация текстов или документов.\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)\n",
    "\n",
    "Например:\n",
    "- [developers.google.com - Text Classification](https://developers.google.com/machine-learning/guides/text-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition (NER)\n",
    "\n",
    "Идентификация и классификации сущностей (таких как люди, места и организации...) в тексте.\n",
    "\n",
    "![](https://www.shaip.com/wp-content/uploads/2022/02/Blog_Named-Entity-Recognition-%E2%80%93-The-Concept-Types-Applications.jpg.webp)\n",
    "\n",
    "Например:\n",
    "- [bert-base-NER ](https://huggingface.co/dslim/bert-base-NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech tagging (POS tagging, POST)\n",
    "\n",
    "Определение, какой частью речи является каждое слово в предложении.\n",
    "\n",
    "![](https://slideplayer.com/slide/6426089/22/images/2/Part-of-Speech+Tagging.jpg)\n",
    "\n",
    "Например:\n",
    "- [English Part-of-Speech Tagging in Flair](https://huggingface.co/flair/pos-english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question answering (QA)\n",
    "\n",
    "Генерация ответов на вопросы. Вариации:\n",
    "- запрос содержит ответ на вопрос (например, документ + вопрос к документу)\n",
    "- запрос не содержит ответа на вопрос (необходимо использовать информацию, извлечённую на этапе обучения)\n",
    "\n",
    "![](https://www.nlplanet.org/course-practical-nlp/_images/question_answering_11.png)\n",
    "![](https://www.nlplanet.org/course-practical-nlp/_images/question_answering_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text summarization\n",
    "\n",
    "Генерация резюме текста (абстрактивная суммаризация и экстрактивная суммаризация).\n",
    "\n",
    "![](https://assets-global.website-files.com/62ab5c229babcf02f79fbd7d/63bd8a1f20da862484184fdb_blog%20extractive%20-p-1600.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Summarization](https://huggingface.co/docs/transformers/tasks/summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Mathematical) reasoning\n",
    "\n",
    "Построение рассуждений, решение математических задач\n",
    "\n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpHov9BD2yiBEDrAWZUQxDYWRIuofmpbWVJaJPDPrE-2BbT3B_15-R4n22yNnDVs_8Vkea-Y-ykOHaB6mCKwkLYkBDBoS1r8NX2u4KsCpNC53GAM_8seK6L_90CJCmhC4ML9SSVY03lErXDQd6Pp-ysGsANdvNcqur7lMARO7h4RtDtf6Y7UlNYuEjjQ/s1999/image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLU vs NLG\n",
    "\n",
    "Обычно выделяют:\n",
    "- задачи генерации текста (NLG) --- происходит генерация ответа на естественном языке\n",
    "- задачи понимания текста (NLU) --- происходит генерация (или дополнение) ответа в рамках некоторого формата\n",
    "\n",
    "<img src=\"https://datasolut.com/wp-content/uploads/2021/05/NLP-100.jpg.webp\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели энкодеры\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/Encoder_block.png)\n",
    "\n",
    "Примеры:\n",
    "- BERT\n",
    "- RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели декодеры\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_decoder_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели энкодер-декодеры\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png)\n",
    "\n",
    "[comment]: <> (https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder_step_by_step.png)\n",
    "\n",
    "\n",
    "Примеры:\n",
    "- BART\n",
    "- T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение\n",
    "\n",
    "|                |   NLU      | NLG        |\n",
    "|----------------|------------|------------|\n",
    "| encoder-only   | optimal    | poor       |\n",
    "| decoder-only   | poor       | optimal    |\n",
    "| encoder-decoder| suboptimal | suboptimal |\n",
    "\n",
    "Например, в ML4SE:\n",
    "\n",
    "![](https://github.com/microsoft/CodeXGLUE/raw/main/baselines.jpg)\n",
    "\n",
    "[comment]: <> (https://github.com/microsoft/CodeXGLUE/raw/main/tasks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если необходимо отличное качество одновременно в NLU и NLG?\n",
    "![CodeT5+](res/05_codet5plus.png)\n",
    "\n",
    "Подробнее:\n",
    "- [Wang et al - CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling laws\n",
    "\n",
    "![](res/05_scaling_laws.png)\n",
    "\n",
    "### Large models\n",
    "\n",
    "![](https://cdn.builder.io/api/v1/image/assets%2Fe0438815ba51486bbb6a202747122d4b%2F894dade70d724952bfad3956c599865a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-context learning\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exercise\n",
    "\n",
    "Реализовать Decoder с нуля на PyTorch. Быть готовыми ответить на вопросы по коду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. References\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [nn.labml.ai - Paper Implementations](https://nn.labml.ai/)\n",
    "- [Andrej Karpathy - nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)\n",
    "- [pytorch - NLP from Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "- [nn.labml.ai - GPT](https://nn.labml.ai/transformers/gpt/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
