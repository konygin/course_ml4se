{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Â TRANSFORMER\n",
    "\n",
    "1. Intro\n",
    "2. Decoder\n",
    "3. Transformer\n",
    "4. Learning\n",
    "5. Tasks\n",
    "6. LLMs\n",
    "7. Exercise\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq is a family of approaches for sequence transformation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"None\"\n",
       "            src=\"http://jalammar.github.io/images/seq2seq_3.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9657ebea10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Source: http://jalammar.github.io\n",
    "IFrame(src='http://jalammar.github.io/images/seq2seq_3.mp4', width=800, height=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2017, the Transformer (Google Brain) architecture was proposed in the paper [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)\n",
    "\n",
    "[Source: [lena-voita.github.io](https://lena-voita.github.io/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More\n",
    "- [Vaswani et al - Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Jay Allamar - The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Huang et al - The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
    "- [Brandon Rohrer - Transformers from Scratch](https://e2eml.school/transformers.html)\n",
    "- [Peter Bloem - Transformers from Scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decoder (GPT)\n",
    "\n",
    "Overview of decoder:\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/gpt2-self-attention-example-2.png)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-attention sublayer in the decoder has been modified:\n",
    "- Masks have been added to prevent visiting subsequent positions.\n",
    "\n",
    "![](res/05_decoder.png)\n",
    "\n",
    "[Source: [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Self-Attention\n",
    "\n",
    "Comparison of self-attention and masked self-attention:\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This masking ensures that predictions for position $i$ can only depend on known outputs at positions smaller than $i$. A triangular mask is used:\n",
    "\n",
    "![](https://peterbloem.nl/files/transformers/masked-attention.svg)\n",
    "\n",
    "[Source: [peterbloem.nl](http://peterbloem.nl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation\n",
    "\n",
    "A learnable linear transformation and softmax are used to transform the decoder outputs into predicted probabilities of the next token.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]\n",
    "\n",
    "The output of the decoder is a vector.\n",
    "This vector is passed through a linear layer (a fully connected neural network), followed by softmax.\n",
    "\n",
    "The linear layer projects the decoder output into a much larger vector (a logits vector).\n",
    "The size of this vector is the size of the vocabulory.\n",
    "\n",
    "The softmax layer then turns the coordinates into probabilities.\n",
    "The most probable token is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulory and logits\n",
    "\n",
    "![](http://jalammar.github.io/images/gpt2/gpt2-output-scores-2.png)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregression\n",
    "\n",
    "![](https://habrastorage.org/getpro/habr/upload_files/80e/243/698/80e24369887bf050a35ece72a3e161b5.gif)\n",
    "\n",
    "[comment]: <> (http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy search\n",
    "\n",
    "![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam search\n",
    "\n",
    "![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More:\n",
    "\n",
    "- [nn.labml.ai - GPT](https://nn.labml.ai/transformers/gpt/index.html)\n",
    "- [Jay Alammar - The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)\n",
    "\n",
    "[Source: [lena-voita.github.io](https://lena-voita.github.io/)]\n",
    "\n",
    "The transformer decoder consists of $N$ consecutive identical layers (for example, $N = 6$).\n",
    "The third sublayer is added, which implements multi-head attention over the outputs from the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "\n",
    "[Source: [jalammar.github.io](http://jalammar.github.io)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- GPT\n",
    "- PaLM\n",
    "\n",
    "#### More:\n",
    "- [Transformer Encoder and Decoder Models](https://nn.labml.ai/transformers/models.html)\n",
    "- [Patrick von Platen - How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train from scratch\n",
    "- Transfer learning: use models that were trained on other (but related to the current) tasks\n",
    "\n",
    "If you train a model completely from scratch, you will need a lot of labeled data. Where to get it?\n",
    "\n",
    "Idea:\n",
    "- let's find a suitable intermediate task with the following properties:\n",
    "    - related to the original task\n",
    "    - easy to find a lot of labeled data for it\n",
    "- *pretrain*\n",
    "- *fine-tune* the model on *downstream*-task, using a smaller dataset\n",
    "\n",
    "| training | task type | purpose | targets | labelling |\n",
    "|----------|-------------------|---------------------|----------------|-----------------|\n",
    "| pretrain | pretrain task | language modeling | synthetic | self-supervised |\n",
    "| finetune | downstream task | useful task | real | supervised |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "Training a model from scratch for an intermediate task. Features:\n",
    "- requires a lot of data;\n",
    "- expensive.\n",
    "\n",
    "Choosing an intermediate task:\n",
    "- related to the original task (for example, *language modeling* for NLP-task)\n",
    "- availability of a large amount of labeled data.\n",
    "\n",
    "Labeling with the help of experts:\n",
    "- takes a lot of time,\n",
    "- expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal language modeling (CLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg)\n",
    "\n",
    "- left to right\n",
    "- autoregressive\n",
    "- GPT (decoder)\n",
    "- Prefix language modeling (PrefixLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked language modeling (MLM)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg)\n",
    "\n",
    "[Source: [huggingface.co](https://huggingface.co/datasets/huggingface-course)]\n",
    "\n",
    "- BERT (encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span corruption\n",
    "\n",
    "![](https://user-images.githubusercontent.com/6536835/116129345-4b306700-a6ca-11eb-9acd-a14aa2b8d115.png)\n",
    "\n",
    "[Source: [Raffel et al. 2019](https://arxiv.org/abs/1910.10683)]\n",
    "\n",
    "- T5 (encoder-decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UL2\n",
    "\n",
    "[Tay et al - UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)\n",
    "\n",
    "- R-denoising (regular denoising): standard span corruption objective --- short spans & low corruption\n",
    "- S-denoising (sequential denoising): strictly follows sequence order --- sequential denoising / prefix language modeling\n",
    "- X-denoising (extreme denoising): extreme span lengths and corruption rates\n",
    "\n",
    "![](res/05_denoising.png)\n",
    "\n",
    "[Source: [Tay et al. 2022](https://arxiv.org/abs/2205.05131)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning (SFT)\n",
    "\n",
    "\n",
    "### Datasets and benchmarks\n",
    "\n",
    "- SQuAD ---  Stanford Question Answering Dataset\n",
    "- CoQA --- Conversational Question Answering\n",
    "- (Super) GLUE --- General Language Understanding Evaluation benchmark\n",
    "    - CoLA --- Corpus of Linguistic Acceptability\n",
    "    - SST --- Stanford Sentiment Treebank\n",
    "    - MRPC --- Microsoft Research Paraphrase Corpus\n",
    "    - QQP --- Quora Question Pairs\n",
    "    - MultiNLI --- Multi-Genre Natural Language Inference Corpus\n",
    "    - RTE --- Recognizing Textual Entailment\n",
    "    - etc\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- Exact match (EM)\n",
    "- F1 score\n",
    "- Perplexity\n",
    "- BLEU\n",
    "\n",
    "More:\n",
    "- [Resources and Benchmarks for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tasks\n",
    "\n",
    "Natural language processing --- various tasks related to text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis\n",
    "\n",
    "Analysis of the tonality of the text, identification of the emotional coloring of the text.\n",
    "\n",
    "![](res/05_sentiment_analysis.png)\n",
    "\n",
    "[Source: [Pascual - Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification\n",
    "\n",
    "Classification of texts or documents.\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)\n",
    "\n",
    "[Source: [developers.google.com - Text Classification](https://developers.google.com/machine-learning/guides/text-classification)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition (NER)\n",
    "\n",
    "Identification and classification of entities (such as people, places, and organizations...) in text.\n",
    "\n",
    "![](https://www.shaip.com/wp-content/uploads/2022/02/Blog_Named-Entity-Recognition-%E2%80%93-The-Concept-Types-Applications.jpg.webp)\n",
    "\n",
    "[Source: [www.shaip.com](https://www.shaip.com/blog/named-entity-recognition-and-its-types/)]\n",
    "\n",
    "More:\n",
    "- [bert-base-NER ](https://huggingface.co/dslim/bert-base-NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech tagging (POS tagging, POST)\n",
    "\n",
    "Determining what part of speech each word in a sentence is.\n",
    "\n",
    "![](https://camo.githubusercontent.com/085652a97d120fc9bed34fb5007aae43b99c87e5ba33571680bbe70c021bf886/68747470733a2f2f64333377756272666b69306c36382e636c6f756466726f6e742e6e65742f643563626334623065313463323066383737333636623639623931373136343961666531316664612f64393661382f6173736574732f696d616765732f62696772616d2d686d6d2f706f732d7469746c652e6a7067)\n",
    "\n",
    "[Source: [github.com/dwayne99/NLP-Basics](https://github.com/dwayne99/NLP-Basics)]\n",
    "\n",
    "More:\n",
    "- [English Part-of-Speech Tagging in Flair](https://huggingface.co/flair/pos-english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question answering (QA)\n",
    "\n",
    "Generating answers to questions:\n",
    "- *Extractive*: the query contains the answer to the question (e.g. document + question to the document)\n",
    "- *Generative:* the query does not contain the answer to the question (it is necessary to use the information extracted during the training stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text summarization\n",
    "\n",
    "Text summary generation (abstractive summarization and extractive summarization)\n",
    "\n",
    "\n",
    "![](https://assets-global.website-files.com/62ab5c229babcf02f79fbd7d/63bd8a1f20da862484184fdb_blog%20extractive%20-p-1600.png)\n",
    "\n",
    "[Source: [medium.com](https://medium.com/@abstractive-health/extractive-vs-abstractive-summarization-in-healthcare-bfe7424eb586)]\n",
    "\n",
    "More:\n",
    "\n",
    "- [Summarization](https://huggingface.co/docs/transformers/tasks/summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Mathematical) reasoning\n",
    "\n",
    "Building reasoning, solving mathematical problems.\n",
    "\n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpHov9BD2yiBEDrAWZUQxDYWRIuofmpbWVJaJPDPrE-2BbT3B_15-R4n22yNnDVs_8Vkea-Y-ykOHaB6mCKwkLYkBDBoS1r8NX2u4KsCpNC53GAM_8seK6L_90CJCmhC4ML9SSVY03lErXDQd6Pp-ysGsANdvNcqur7lMARO7h4RtDtf6Y7UlNYuEjjQ/s1999/image5.png)\n",
    "\n",
    "[Source: [research.google](https://research.google/blog/minerva-solving-quantitative-reasoning-problems-with-language-models/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLU vs NLG\n",
    "\n",
    "Usually, there are:\n",
    "- Natural Language Generation (NLG) tasks --- a response is generated in natural language\n",
    "- Natural Language Understanding (NLU) tasks --- a response is generated (or supplemented) within a certain format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/Encoder_block.png)\n",
    "\n",
    "Examples:\n",
    "- BERT\n",
    "- RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_decoder_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder\n",
    "\n",
    "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png)\n",
    "\n",
    "[comment]: <> (https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder_step_by_step.png)\n",
    "\n",
    "\n",
    "Examples:\n",
    "- BART\n",
    "- T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder vs. Decoder. vs Encoder-Decoder\n",
    "\n",
    "|                |   NLU      | NLG        |\n",
    "|----------------|------------|------------|\n",
    "| encoder-only   | optimal    | poor       |\n",
    "| decoder-only   | poor       | optimal    |\n",
    "| encoder-decoder| suboptimal | suboptimal |\n",
    "\n",
    "In ML4SE:\n",
    "\n",
    "![](https://github.com/microsoft/CodeXGLUE/raw/main/baselines.jpg)\n",
    "\n",
    "[comment]: <> (https://github.com/microsoft/CodeXGLUE/raw/main/tasks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you need excellent quality in both NLU and NLG?\n",
    "![CodeT5+](res/05_codet5plus.png)\n",
    "\n",
    "More:\n",
    "- [Wang et al - CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling laws\n",
    "\n",
    "![](res/05_scaling_laws.png)\n",
    "\n",
    "### Large models\n",
    "\n",
    "![](https://img.plasmic.app/img-optimizer/v1/img?src=9b78df93c795c1101319b6a1ec6911ae.png&f=webp&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-context learning\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More:\n",
    "\n",
    "- [Kaplan et al - Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361v1)\n",
    "- [Wei et al - Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)\n",
    "- [Wei et al - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exercise\n",
    "\n",
    "Using LLM implement Decoder from scratch in PyTorch. Be prepared to answer questions about the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. References\n",
    "\n",
    "- [Lilian Weng - The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [nn.labml.ai - Paper Implementations](https://nn.labml.ai/)\n",
    "- [Andrej Karpathy - nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)\n",
    "- [pytorch - NLP from Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "- [nn.labml.ai - GPT](https://nn.labml.ai/transformers/gpt/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
