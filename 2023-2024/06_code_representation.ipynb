{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. CODE REPRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CodeBERT\n",
    "2. GraphCodeBERT\n",
    "3. UniXCoder\n",
    "4. Exercise\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CodeBERT\n",
    "\n",
    "[CodeBERT](https://arxiv.org/abs/2002.08155v4) --- бимодальная модель для языков программирования и естественного языка. CodeBERT на выходе выдаёт эмбеддинги общего назначения. Эмбеддинги могут использоваться в таких задач, как поиск по коду, суммаризация кода и т. д. Модель разработана в Microsoft в 2020 году.\n",
    "\n",
    "![](./res/06_codebert_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе модели CodeBERT лежит [BERT](https://arxiv.org/abs/1810.04805)(Google, 2018). Точнее, [RoBERTa](https://arxiv.org/abs/1907.11692) (Meta, 2019, отличие в процедуре формирования масок)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeBERT на Huggingface"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "print(nl_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['return', 'Ġmaximum', 'Ġvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "print(code_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['def', 'Ġmax', '(', 'a', ',', 'b', '):', 'Ġif', 'Ġa', '>', 'b', ':', 'Ġreturn', 'Ġa', 'Ġelse', 'Ġreturn', 'Ġb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['<s>', 'return', 'Ġmaximum', 'Ġvalue', '</s>', 'def', 'Ġmax', '(', 'a', ',', 'b', '):', 'Ġif', 'Ġa', '>', 'b', ':', 'Ġreturn', 'Ġa', 'Ġelse', 'Ġreturn', 'Ġb', '</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[0, 30921, 4532, 923, 2, 9232, 19220, 1640, 102, 6, 428, 3256, 114, 10, 15698, 428, 35, 671, 10, 1493, 671, 741, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.Size([1, 23, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor([[[-0.1423,  0.3766,  0.0443,  ..., -0.2513, -0.3099,  0.3183],\n",
    "         [-0.5739,  0.1333,  0.2314,  ..., -0.1240, -0.1219,  0.2033],\n",
    "         [-0.1579,  0.1335,  0.0291,  ...,  0.2340, -0.8801,  0.6216],\n",
    "         ...,\n",
    "         [-0.4042,  0.2284,  0.5241,  ..., -0.2046, -0.2419,  0.7031],\n",
    "         [-0.3894,  0.4603,  0.4797,  ..., -0.3335, -0.6048,  0.4730],\n",
    "         [-0.1433,  0.3785,  0.0450,  ..., -0.2527, -0.3121,  0.3207]]],\n",
    "       grad_fn=<NativeLayerNormBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-training\n",
    "\n",
    "CodeBERT --- это бимодальная модель. Это значит, что на вход могут подаваться пары вида (NL, PL).\n",
    "При этом пара преобразуется в последовательность токенов.\n",
    "Для этого, каждый элемент пары: NL- и PL-часть, с помощью токенизатора ([Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding), BPE) превращается в последовательность токенов (сегмент).\n",
    "\n",
    "На этапе предобучения модели на вход подаётся конкатенация двух сегментов со специальным токеном-разделителем:\n",
    "\n",
    "$$[\\texttt{CLS}], w_1, w_2, \\ldots, w_n, [\\texttt{SEP}], c_1, c_2, \\ldots, c_m, [\\texttt{EOS}].$$\n",
    "\n",
    "- первый сегмент: текст естественного языка\n",
    "- второй сегмент: исходный код\n",
    "\n",
    "Предобучение происходит на двух задачах:\n",
    "- MLM (masked language modeling), BERT, Devlin et al. (2018): восстановление токенов, бимодальные данные\n",
    "- RTD (replaced token detection), ELECTRA, Clark et al. (2020): обнаружение замены токенов, унимодальные данные\n",
    "\n",
    "$[\\texttt{CLS}]$ --- специальный токен для получение агрегированного эмбеддинга (может быть использован, например, при классификации)\n",
    "\n",
    "На выходе имеем:\n",
    "- контекстное векторное представление каждого токена (для естественного языка и кода);\n",
    "- представление для токена $[\\texttt{CLS}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данные для pretrain\n",
    "\n",
    "Обучение модели CodeBERT происходит как на *бимодальных* данных (параллельные пары текст и кода), так и на *унимодальных* данных.\n",
    "\n",
    "- репозитории Github\n",
    "- бимодальная пара --- это функция и соответствующая документация (примерно, 2 млн)\n",
    "- унимодальные данные --- это функция без парной документации (примерно, 6 млн)\n",
    "- 6 языков программирования: Python, Java, JavaScript, PHP, Ruby и Go.\n",
    "\n",
    "![пример бимодальной пары](res/06_codebert_bimodal.png)\n",
    "\n",
    "NL-часть --- только текстовая часть (выделено красным) документации (пунктирный контур)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача MLM\n",
    "\n",
    "Пусть $x = \\{w, c\\}$ --- входная NL-PL пара, где $w$ --- последовательность NL-токенов, а $c$ --- последовательность PL-токенов.\n",
    "\n",
    "Сначала выбираем случайный набор позиций для NL и PL, которые необходимо замаскировать --- $m_w$ и $m_c$ соответственно. Затем заменяем выбранные позиции специальным токеном $[\\texttt{MASK}]$. Как и в [Devlin et al. (2018)](https://arxiv.org/abs/1810.04805), маскируется 15% токенов из $x$ --- если выбран токен, то с вероятностью 0.8 он заменяется на $[\\texttt{MASK}]$, с вероятностью 0.1 заменяется на случайный токен, с вероятностью 0.1 остаётся прежним).\n",
    "\n",
    "- для $i \\in \\{1, \\ldots, |w|\\}$ имеем $m^w_i ∼ \\texttt{unif}\\{1, |w|\\}$ --- какие NL-токены хотим маскировать\n",
    "- для $i \\in \\{1, \\ldots, |c|\\}$ имеем $m^c_m ∼ \\texttt{unif}\\{1, |c|\\}$ --- какие PL-токены хотим маскировать\n",
    "- $w^{masked} = \\texttt{REPLACE}(w, m^w, [\\texttt{MASK}])$\n",
    "- $c^{masked} = \\texttt{REPLACE}(c, m^c, [\\texttt{MASK}])$\n",
    "- $x = [w;c]$ --- конкатенируем\n",
    "\n",
    "В MLM задача состоит в том, чтобы предсказать токены, которые были маскированы. Для этого используется следующая функция потерь:\n",
    "\n",
    "$$L_{MLM}(\\theta) = \\sum_{i \\in m^w \\cup m^c} -\\log p (x_i~|~w^{masked}, c^{masked}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "if x is not <mask>:\n",
    "    x += 1\"\n",
    "\"\"\"\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "outputs = fill_mask(code)\n",
    "pp.pprint(outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ { 'score': 0.8179319500923157,\n",
    "    'sequence': '\\nif x is not None:\\n    x += 1\"\\n',\n",
    "    'token': 9291,\n",
    "    'token_str': ' None'},\n",
    "  { 'score': 0.04150787368416786,\n",
    "    'sequence': '\\nif x is not 0:\\n    x += 1\"\\n',\n",
    "    'token': 321,\n",
    "    'token_str': ' 0'},\n",
    "  { 'score': 0.009036713279783726,\n",
    "    'sequence': '\\nif x is not zero:\\n    x += 1\"\\n',\n",
    "    'token': 4276,\n",
    "    'token_str': ' zero'},\n",
    "  { 'score': 0.00813678465783596,\n",
    "    'sequence': '\\nif x is not null:\\n    x += 1\"\\n',\n",
    "    'token': 23796,\n",
    "    'token_str': ' null'},\n",
    "  { 'score': 0.008066722191870213,\n",
    "    'sequence': '\\nif x is not x:\\n    x += 1\"\\n',\n",
    "    'token': 3023,\n",
    "    'token_str': ' x'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача RTD\n",
    "\n",
    "Изначально, задача RTD (replaced token detection) была использована для предобучния моделей естественного языка [ELECTRA](https://arxiv.org/abs/2003.10555). Для модели CodeBERT эта задача была адаптирована для кода.\n",
    "Используются как *бимодальные*, так и *унимодальные* данные. В частности, здесь есть два генератора данных:\n",
    "- NL-генератор $p^{G_w}$ и\n",
    "- PL-генератор $p^{G_c}$.\n",
    "\n",
    "Оба генератора нужны для создания правдоподобных замен выбранных токенов.\n",
    "\n",
    "![](./res/06_rtd_electra.png)\n",
    "\n",
    "Генератор --- произвольная языковая модель, генерирующая распределение на токенах. Обычно используют небольшую  модель, обучающуюся совместно с генератором.\n",
    "\n",
    "Более формально:\n",
    "- для $i \\in m^w$ имеем $\\hat{w}_i ∼ p^{G_w} (w_i~|~w_{masked})$ --- какие токены и на какие хотим заменить\n",
    "- для $i \\in m^c$ имеем $\\hat{c}_i ∼ p^{G_c} (c_i~|~c_{masked})$ --- аналогично для кода\n",
    "- $w^{corrupt} = \\texttt{REPLACE}(w, m^w , \\hat{w})$\n",
    "- $c^{corrupt} = \\texttt{REPLACE}(c, m^c , \\hat{c})$\n",
    "- $x^{corrupt} = [w^{corrupt};c^{corrupt}]$ --- конкатенируем "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача дискриминатора определить, является ли токен оригинальным или нет, т.е. бинарная классификация.\n",
    "![](./res/06_codebert_rtd.png)\n",
    "\n",
    "Заметим, что дикриминатор применяется к каждой позиции во входных данных, кроме того, если генератор выдает правильный токен, то метка этого токена \"original\", а не \"replaced\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь для задачи RTD:\n",
    "\n",
    "$$L_{RTD}(\\theta) = \\sum_{i=1}^{|w|+|c|}(\\delta(i) \\log p^{D} (x^{corrupt}, i) + (1-\\delta(i))(1-\\log p^{D}(x^{corrupt}, i))),$$\n",
    "\n",
    "где\n",
    "- $\\theta$ --- параметры,\n",
    "- $p^{D}$ --- дискриминатор, предсказывающий вероятность того, что $i$-й токен является оригинальным,\n",
    "- $\\delta(i)$ --- индикаторная функция: $\\delta(i) = 1$, если $x^{corrupt}_i = x_i$; иначе, $\\delta(i) = 0.$\n",
    "\n",
    "Существует множество различных способов реализации генераторов. Для CodeBERT используются две модели языка (NL, PL) на основе  $n$-грамм (Jurafsky, 2000). Модели были обучены на соответствующих унимодальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача MLM+RTD\n",
    "\n",
    "Итоговая задача выглядит так:\n",
    "$$ \\min_{\\theta}( L_{MLM}(\\theta) + L_{RTD}(\\theta) )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация\n",
    "\n",
    "- https://github.com/microsoft/CodeBERT\n",
    "- https://huggingface.co/microsoft/graphcodebert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downstream tasks\n",
    "\n",
    "- natural language code search\n",
    "- code documentation generation\n",
    "- clone detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GraphCodeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-trained model for code that considers the **inherent structure** of code\n",
    "\n",
    "The model uses **data flow** in the pre-training stage.\n",
    "\n",
    "![](res/06_graphcodebert_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA FLOW\n",
    "\n",
    "Data flow is a graph that represents **dependency relation between variables**, in which nodes represent variables and edges represent where the value of each variable comes from.\n",
    "\n",
    "Unlike AST, data flow **is same under different abstract grammars** for the same source code.\n",
    "Such code structure provides crucial code semantic information for code understanding.\n",
    "\n",
    "\n",
    "> Example:\n",
    ">\n",
    "> ![overview](./res/06_graphcodebert_dataflow.png)\n",
    ">\n",
    "> There are four variables with same name (i.e. $x^3$, $x^7$, $x^9$ and $x^{11}$) but with different semantic. The graph in the figure shows dependency relation between these variables and supports $x^{11}$ to pay more attention to $x^7$ and $x^9$ instead of $x^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. source code $C = \\{c_1, c_2, ..., c_n \\}$\n",
    "2. parse the code into an abstract syntax tree (AST) by a standard compiler tool\n",
    "3. the AST includes syntax information of the code and terminals (leaves) are used to identify the variable sequence, denoted as $V = \\{v_1, v_2, ..., v_k\\}$\n",
    "4. take each variable as a node of the graph and an direct edge $e = < v_i, v_j >$ from $v_i$ to $v_j$ refers that the value of $j$-th variable comes from $i$-th variable\n",
    "5. denote the set of directed edges as $E = \\{e_1, e_2 , ..., e_l\\}$ and the graph $G(C) = (V, E)$ is data flow used to represent dependency relation between variables of the source code $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL ARCHITECTURE\n",
    "\n",
    "![overview](./res/06_graphcodebert_overview.png)\n",
    "\n",
    "- Text --- comment to source code, $W = \\{w_1, w_2, ..., w_m\\}$\n",
    "- Source Code $C = \\{c_1, c_2, ..., c_n\\}$\n",
    "- Variable Sequence $V = \\{v_1, v_2, ..., v_k\\}$, where $G(C) = (V, E)$ is the corresponding data flow with $E = \\{e_1, e_2, ..., e_l\\}$ is a set of direct edges that represent where the value of each variable comes from\n",
    "\n",
    "Input:\n",
    "- We concatenate the comment, source code and the set of variables as the sequence input $X = \\{[CLS], W, [SEP], C, [SEP], V \\}$, where $[CLS]$ is a special token in front of three segments and $[SEP]$ is a special symbol to split two kinds of data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphCodeBERT takes the sequence $X$ as the input and then converts the sequence into input vectors $H^0$. For each token, its input vector is constructed by summing the corresponding token and position embeddings. We use a special position embedding for all variables to indicate that they are nodes of data flow.\n",
    "\n",
    "The model applies $N$ transformer layers over the input vectors to produce contextual representations $H^n = transformer_n(H^{n−1})$, $n \\in [1, N]$. Each transformer layer contains an architecturally identical transformer that applies a multi-headed self-attention operation (Vaswani et al., 2017) followed by a feed forward layer over the input $H^{n-1}$ in the $n$-th layer.\n",
    "\n",
    "$$G^n = LN (MultiAttn (H^{n-1}) + H^{n-1})$$\n",
    "$$H^n = LN (FFN (G^n) + G^n)$$\n",
    "\n",
    "where $MultiAttn$ is a multi-headed self-attention mechanism, $FFN$ is a two layers feed forward network, and $LN$ represents a layer normalization operation. For the $n$-th transformer layer, the output $\\hat{G}^n$ of a multi-headed self-attention is computed via:\n",
    "\n",
    "$$Q_i = H^{n-1}W^Q_i$$\n",
    "$$K_i = H^{n-1}W^K_i$$\n",
    "$$V_i = H^{n-1}W^V_i$$\n",
    "$$head_i = softmax(\\frac{Q_iK^T_i}{\\sqrt{d_k}} + M)V_i ~~~~~ (*)$$\n",
    "$$\\hat{G}^n = [head_1 ; ...; head_u]W^O_n$$\n",
    "\n",
    "where the previous layer’s output $H^{n-1} \\in R^{|X|} \\times R^{d_h}$ is linearly projected to a triplet of queries, keys and values using model parameters $W^Q_i, W^K_i, W^V_i \\in R^{d_h} \\times R^{d_k}$, respectively. $u$ is the number of heads, $d_k$ is the dimension of a head, and $W^O_n \\in R^{d_h} \\times R^{d_h}$ is the model parameters.\n",
    "\n",
    "\n",
    "$M \\in R^{|X|} \\times R^{|X|}$ is a mask matrix, where $M_{ij}$ is $0$ if $i$-th token is allowed to attend $j$-th token otherwise $-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPH-GUIDED MASKED ATTENTION\n",
    "\n",
    "To incorporate the graph structure into Transformer, we define a graph-guided masked attention function to filter out irrelevant signals.\n",
    "\n",
    "The attention masking function could avoid the key $k_i$ attended by the query $q_j$ by adding the attention score $q^T_jk_i$ an **infinitely negative value** so that the attention weight becomes zero after using a softmax function.\n",
    "\n",
    "To represent dependency relation between variables, a node-query $q_{v_i}$ is allowed to attend to a node-key $k_{v_j}$ if there is a direct edge from the node $v_j$ to the node $v_i$ (i.e. $<v_j, v_i>, i \\in E$) or they are the same node (i.e. $i=j$). Otherwise, the attention is masked by adding an infinitely negative value into the attention score.\n",
    "\n",
    "To represent the relation between source code tokens and nodes of the data flow, we first define a set $E'$, where $\\frac{<v_i, c_j>}{<c_j, v_i>} \\in E'$ if the variable $v_i$ is identified from the source code token $c_j$. We then allow the node $q_{v_i}$ and code $k_{c_j}$ attend each other if and only if $\\frac{<v_i, c_j>}{<c_j, v_i>} \\in E'$.\n",
    "\n",
    "More formally, we use the following graph-guided masked attention matrix as the mask matrix $M$ in the equation ( * ):\n",
    "\n",
    "$M_{ij} = 0$ if $q_i \\in \\{[CLS], [SEP]\\}$ or $q_i, k_j \\in W \\cup C$ or $<q_i, k_j> \\in E \\cup E'$;\n",
    "\n",
    "$M_{ij} = - \\infty$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRE-TRAINING TASKS\n",
    "\n",
    "- masked language modeling\n",
    "- data flow edge prediction\n",
    "- variable-alignment across source code and data flow\n",
    "\n",
    "**Masked Language Modeling**\n",
    "\n",
    "- sample randomly 15% of the tokens from the source code and paired comment\n",
    "- replace them with a [MASK] token 80% of the time, with a random token 10% of the time, and leave them unchanged 10% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge Prediction**\n",
    "\n",
    "\n",
    "- randomly sample 20% of nodes $V_s$ in data flow, mask direct edges connecting these sampled nodes by add an infinitely negative value in the mask matrix, and then predict these masked edges $E_{mask}$\n",
    "\n",
    "Taking the variable $x^{11}$ in Figure for an example, we first mask edges $<x^7, x^{11}>$ and $<x^9, x^{11}>$ in the graph and then let the model to predict these edges.\n",
    "\n",
    "Formally, the pre-training objective of the task is calculated as Equation ( ** ), where $E_c = V_s \\times V \\cup V \\times V_s$ is a set of candidates for edge prediction, $\\delta(e_{ij} \\in E)$ is $1$ if $<v_i, v_j> \\in E$ otherwise $0$, and the probability $p_{e_{ij}}$ of existing an edge from $i$-th to $j$-th node **is calculated by dot product following a sigmoid function** using representations of two nodes from GraphCodeBERT. To balance positive-negative ratio of examples, we sample negative and positive samples with the same number for $E_c$.\n",
    "\n",
    "$$loss_{EdgePred} = - \\sum_{e_{ij}\\in E_c} [\\delta(e_{ij} \\in E_{mask})\\log(p_{e_{ij}}) + (1-\\delta(e_{ij} \\in E_{mask})) \\log(1-p_{e_{ij}})]~~~~~(**)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node Alignment**\n",
    "\n",
    "The motivation is to encourage the model to align variables and source code according to data flow.\n",
    "\n",
    "- predict edges between code tokens and nodes.\n",
    "\n",
    "Taking Figure for an example, we first mask edges between the variable $x^{11}$ in data flow and code tokens, and then predict which code token the variable $x^{11}$ in data flow is identified from. As we can see, the model could predict that the variable $x^{11}$ is identified form the variable $x$ in the expression “return $x$” according to data flow information (i.e. the value of $x^{11}$ comes from $x^7$ or $x^9$).\n",
    "\n",
    "![overview](./res/06_graphcodebert_node_task.png)\n",
    "\n",
    "Specially, we randomly sample 20% nodes $V'_s$ in the graph, mask edges between code tokens and sampled nodes, and then predict masked edges $E'_{mask}$. The pre-training objective of this task is similar to Equation ( ** ), where $E'_c = V'_s \\times C$ is a set of candidates for node alignment. Similarly, we also sample negative and positive samples with the same number for $E'_c$.\n",
    "\n",
    "$$loss_{NodeAlign} = - \\sum_{e_{ij}\\in E'_c} [\\delta(e_{ij} \\in E'_{mask})\\log(p_{e_{ij}}) + (1-\\delta(e_{ij} \\in E'_{mask})) \\log(1-p_{e_{ij}})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. UniXCoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/06_unixcoder_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/06_unixcoder_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exercise\n",
    "\n",
    "Исследовать возможность использовать CodeBERT или GraphCodeBERT для задачи clone detection (поиск клонов).\n",
    "Например, можно использовать датасет [BigCloneBench](https://github.com/clonebench/BigCloneBench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Kanade et al - CuBERT: Learning and Evaluating Contextual Embedding of Source Code](https://arxiv.org/abs/2001.00059)\n",
    "- [Feng et al - CodeBERT: A Pre-trained Model for Programming and Natural Languages 2020](https://arxiv.org/abs/2002.08155)\n",
    "- [Karampatsis Sutton - SCELMo: Source Code Embeddings from Language Models 2020](https://arxiv.org/abs/2004.13214)\n",
    "- [Jain et al - Contrastive Code Representation Learning 2020](https://arxiv.org/abs/2007.04973)\n",
    "- [Guo et al - GraphCodeBERT: Pre-training Code Representations with Data Flow 2020](https://arxiv.org/abs/2009.08366)\n",
    "- [Chirkova Torshin - An Empirical Study of Transformers for Source Code 2020](https://arxiv.org/abs/2010.07987)\n",
    "- [Gu et al - Multimodal representation for neural code search 2022](https://arxiv.org/abs/2107.00992)\n",
    "- [Wang et al - CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859)\n",
    "- [Guo et al - UniXcoder: Unified Cross-Modal Pre-training for Code Representation](https://arxiv.org/abs/2203.03850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
