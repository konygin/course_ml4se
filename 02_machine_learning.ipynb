{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. MACHINE LEARNING\n",
    "\n",
    "1. Introduction\n",
    "2. Gradient descent\n",
    "3. Neural networks\n",
    "4. Backpropagation\n",
    "5. Exercise\n",
    "6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to machine learning\n",
    "\n",
    "- $X$ is a set of objects\n",
    "- $Y$ is a set of answers\n",
    "- $y^∗: X \\rightarrow Y$ is an objective function. Its values $y_i = y^∗(x_i)$ are known only for a finite subset of objects $\\{ x_1, ..., x_l\\} \\subseteq X$\n",
    " \n",
    "The set of pairs $X_l = \\{(x_i, y_i) | i \\in \\{1, ..., l\\} \\}$ is called the *training set*.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "The task is to reconstruct the function $y^∗$ using the training set $X_l$.\n",
    "\n",
    "That is, we must construct a function $y: X \\rightarrow Y$ that approximates the objective function $y^∗(x)$ not only on the training set but also on the entire set $X$.\n",
    "\n",
    "![](./res/02_learning_scheme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "How can we construct the function $y: X \\rightarrow Y$?\n",
    "\n",
    "A common strategy is to define an optimization problem:\n",
    "1.  Define a _loss function_ $L(\\hat{y}, y)$, a non-negative function that measures the discrepancy between a predicted answer $\\hat{y}$ and the true answer $y$. If $L(\\hat{y}, y) = 0$, the prediction is _correct_.\n",
    "2.  The overall goal is to find a function that minimizes the cumulative loss across the entire training set.\n",
    "\n",
    "\n",
    "This is typically formalized as follows:\n",
    "1.  We choose a _hypothesis space_ $A$, which is a set of candidate functions $f: X \\rightarrow Y$.\n",
    "2.  The functions in $A$ are often parameterized: $A = \\{f(a) \\mid a \\in D\\}$, where $D$ is a set of admissible parameters.\n",
    "3.  We then select the specific function $f \\in A$ that minimizes the total loss on the training data $X_l$. Ideally, this function will also perform well (have a low loss) on the entire set $X$.\n",
    "\n",
    "Example: In linear regression, we use the hypothesis space of linear functions (e.g., lines $y = kx + b$, and a common loss function is the squared error.\n",
    "\n",
    "![](./res/02_linear_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of optimizing the function $y$ can be visualized as a descent across the landscape of the loss function.\n",
    "\n",
    "![https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png](https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png)\n",
    "\n",
    "Thus, we have an *optimization problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How will we know that the training was successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. _Loss Value_: Monitoring the value of the loss function on the training data, which quantifies the model's training error.\n",
    "2. _Hold-Out Validation_: Testing the model on a separate, unseen hold-out dataset (or validation set) to estimate its generalization error and check for overfitting.\n",
    "\n",
    "![](./res/02_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization methods\n",
    "\n",
    "What optimization methods do you know?\n",
    "\n",
    "- differentiable functions\n",
    "- continuous functions\n",
    "- discontinuous functions\n",
    "\n",
    "If we compare optimization of differentiable and discontinuous functions, which is easier and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Methods\n",
    "\n",
    "**What are the primary classes of optimization methods?**\n",
    "\n",
    "Optimization methods are often categorized by the properties of the function being optimized:\n",
    "\n",
    "1. _Differentiable Functions_: Functions where gradients can be computed.\n",
    "   - Methods: Gradient Descent, Conjugate Gradient, Quasi-Newton methods\n",
    "3. _Continuous but Non-Differentiable Functions_: Functions that are continuous but may have kinks or corners where the gradient is undefined.\n",
    "   - Methods: Subgradient methods, Nelder-Mead algorithm.\n",
    "4. _Discontinuous or Combinatorial Functions_: Functions defined over discrete spaces or with jumps.\n",
    "    - Methods: Genetic algorithms, simulated annealing, swarm optimization.\n",
    "\n",
    "**Comparison: Differentiable vs. Discontinuous Optimization**\n",
    "\n",
    "Optimizing differentiable functions is generally \n",
    "- much easier and\n",
    "- more efficient.\n",
    "\n",
    "The reason is the existence of the _gradient_, which provides a definitive local direction of steepest descent. This allows for powerful, guided, and fast-converging algorithms.\n",
    "\n",
    "In contrast, optimizing discontinuous functions often requires heuristic methods that explore the search space more randomly, without such a reliable guide, making them slower and less guaranteed to find a global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient descent\n",
    "\n",
    "For many machine learning problems, the function we aim to optimize is _differentiable_.\n",
    "This property allows us to use a powerful and intuitive optimization algorithm called _gradient descent_.\n",
    "\n",
    "The core idea is simple: to minimize a function, repeatedly take small steps in the direction of the steepest descent, which is the negative of its gradient. The gradient of a function $f(x)$, denoted $\\frac{df}{dx}$ for a single variable, is defined as the limit:\n",
    "\n",
    "$$\\frac{df}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) -f (x)}{h}$$\n",
    "\n",
    "![](./res/02_gradient_descent_1.jpeg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any disadvantages to gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./res/02_gradient_descent_2.jpeg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural networks\n",
    "\n",
    "![](./res/02_neural_network_human_cortex.png)\n",
    "\n",
    "[Cortex neurons (Ramón y Cajal)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single neuron model\n",
    "\n",
    "![](https://www.researchgate.net/publication/374679637/figure/fig2/AS:11431281206265945@1700629912758/Neural-network-architecture.png)\n",
    "\n",
    "or\n",
    "\n",
    "![](./res/02_neuron_math.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Neural network\n",
    "\n",
    "![](./res/02_neural_network_math.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The universality of neural networks\n",
    "\n",
    "Nonlinearity\n",
    "\n",
    "![](./res/02_nonlinearity.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Cybenko's theorem\n",
    "\n",
    "![](./res/02_cybenko_1.png)\n",
    "![](./res/02_cybenko_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### PyTorch\n",
    "\n",
    "[AlexNet](https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), scheme:\n",
    "\n",
    "![](./res/02_alexnet.png)\n",
    "\n",
    "AlexNet, code in PyTorch:\n",
    "\n",
    "![](./res/02_alexnet_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backpropagation\n",
    "\n",
    "#### How to optimize\n",
    "\n",
    "Suppose we want to optimize a loss function using gradient descent.\n",
    "\n",
    "1. We have a loss function $L$ that we want to minimize.\n",
    "2. We have model parameters that we can change.\n",
    "3. if we take the parameter $w$, then $\\frac{dL}{dw}$ contains information about how the value of the loss function $L$ will change for a small change in the parameter $w$.\n",
    "\n",
    "Suppose $y = g(x)$ and $z = f (g(x)) = f (y)$.\n",
    "Then $$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "Let's take a slightly more complex situation: $x = f(w)$, $y = f(x)$, $z = f(y)$.\n",
    "Then\n",
    "$$\\frac{dz}{dw} = \\frac{dz}{dy} \\frac{dy}{dx} \\frac{dx}{dw} =$$\n",
    "$$= f'(y) f'(x) f'(w) =$$\n",
    "$$= f'(f(f(w))) f'(f(w)) f'(w).$$\n",
    "\n",
    "The expression $f(w)$ is used multiple times. We have two options:\n",
    "1.  Calculate it once, save the result, and reuse it.\n",
    "2.  Recalculate it every time it is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "The solution to this problem of efficient computation is the _Backpropagation Algorithm_.\n",
    "\n",
    "The method was first described in 1974 by A. I. Galushkin, and also independently and simultaneously by Paul J. Werbos\n",
    "\n",
    "![](./res/02_learning_process.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [karpathy/micrograd](https://github.com/karpathy/micrograd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!apt-get install -y graphviz\n",
    "!pip install graphviz\n",
    "!pip install micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{data %.2f | grad %.2f}\" % (n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"770pt\" height=\"127pt\"\n",
       " viewBox=\"0.00 0.00 770.00 127.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 123)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-123 766,-123 766,4 -4,4\"/>\n",
       "<!-- 140445031775296 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140445031775296</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-82.5 296,-118.5 466,-118.5 466,-82.5 296,-82.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"338\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"380,-82.5 380,-118.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140445031775632+ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140445031775632+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"529\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"529\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 140445031775296&#45;&gt;140445031775632+ -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140445031775296&#45;&gt;140445031775632+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466.08,-84.37C475.44,-82.58 484.55,-80.83 492.78,-79.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"493.52,-82.68 502.68,-77.36 492.2,-75.8 493.52,-82.68\"/>\n",
       "</g>\n",
       "<!-- 140445031767376 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140445031767376</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-27.5 296,-63.5 466,-63.5 466,-27.5 296,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"338\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"380,-27.5 380,-63.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140445031767376&#45;&gt;140445031775632+ -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140445031767376&#45;&gt;140445031775632+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466.08,-61.05C475.44,-62.78 484.55,-64.47 492.78,-65.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"492.21,-69.44 502.68,-67.82 493.48,-62.56 492.21,-69.44\"/>\n",
       "</g>\n",
       "<!-- 140445031767376* -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140445031767376*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"233\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"233\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140445031767376*&#45;&gt;140445031767376 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140445031767376*&#45;&gt;140445031767376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M260.1,-45.5C267.69,-45.5 276.45,-45.5 285.72,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"285.93,-49 295.93,-45.5 285.93,-42 285.93,-49\"/>\n",
       "</g>\n",
       "<!-- 140445031775632 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140445031775632</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"592,-54.5 592,-90.5 762,-90.5 762,-54.5 592,-54.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"634\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 3.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"676,-54.5 676,-90.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"719\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140445031775632+&#45;&gt;140445031775632 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140445031775632+&#45;&gt;140445031775632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M556.1,-72.5C563.69,-72.5 572.45,-72.5 581.72,-72.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"581.93,-76 591.93,-72.5 581.93,-69 581.93,-76\"/>\n",
       "</g>\n",
       "<!-- 140445031767472 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140445031767472</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-55.5 0,-91.5 170,-91.5 170,-55.5 0,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"42\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"84,-55.5 84,-91.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.00</text>\n",
       "</g>\n",
       "<!-- 140445031767472&#45;&gt;140445031767376* -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140445031767472&#45;&gt;140445031767376*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.08,-57.37C179.44,-55.58 188.55,-53.83 196.78,-52.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-55.68 206.68,-50.36 196.2,-48.8 197.52,-55.68\"/>\n",
       "</g>\n",
       "<!-- 140445031776208 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140445031776208</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 170,-36.5 170,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"42\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.00</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"84,-0.5 84,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 2.00</text>\n",
       "</g>\n",
       "<!-- 140445031776208&#45;&gt;140445031767376* -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140445031776208&#45;&gt;140445031767376*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.08,-34.05C179.44,-35.78 188.55,-37.47 196.78,-38.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.21,-42.44 206.68,-40.82 197.48,-35.56 196.21,-42.44\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fbbe83a98d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Value(1.0); x.label = 'x'\n",
    "#y = (x * 2 + 1).relu()\n",
    "y = x * 2 + 1\n",
    "y.backward()\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py\n",
    "\n",
    "![](./res/02_micrograd_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "![](./res/02_computation_graph_pytorch_1.jpg)\n",
    "\n",
    "`loss.backward()`\n",
    "\n",
    "![](./res/02_computation_graph_pytorch_2.jpg)\n",
    "\n",
    "[Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercise\n",
    "\n",
    "Implement the operation hyperbolic tangent $\\tanh(x)$ as a method of the [Value](https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/engine.py#L2) class.\n",
    "\n",
    "To test it, you need to build a computational graph, run backpropagation, and visualize it ([example](https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb)).\n",
    "\n",
    "[Example](https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/engine.py#L13) for the addition operation.\n",
    "Additional [explanations](https://www.youtube.com/watch?v=VMj-3S1tku0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. References\n",
    "\n",
    "1. [Goodfellow et al - Deep learning](https://www.deeplearningbook.org/)\n",
    "2. [Stevens et al - Deep Learning with PyTorch: Build, train, and tune neural networks using Python tools](https://www.manning.com/books/deep-learning-with-pytorch)\n",
    "3. [Karpathy - Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/tree/master)\n",
    "4. [Zhang et al - Dive into Deep Learning](https://d2l.ai/)\n",
    "5. [Karpathy - The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "6. [Clarkson - LLM from scratch: Automatic Differentiation](https://bclarkson-code.com/posts/llm-from-scratch-scalar-autograd/post.html)\n",
    "7. [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "8. [Ramón y Cajal and the Case for Drawing in Science](https://www.scientificamerican.com/blog/sa-visual/ramon-y-cajal-and-the-case-for-drawing-in-science2/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
